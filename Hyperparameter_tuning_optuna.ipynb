{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e977441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3a53c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54ecb1a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import optuna\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the Pima Indian Diabetes dataset from sklearn\n",
    "# Note: Scikit-learn's built-in 'load_diabetes' is a regression dataset.\n",
    "# We will load the actual diabetes dataset from an external source\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Pima Indian Diabetes dataset (from UCI repository)\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "columns = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI',\n",
    "           'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(url, names=columns)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "618209cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregnancies                 0\n",
      "Glucose                     0\n",
      "BloodPressure               0\n",
      "SkinThickness               0\n",
      "Insulin                     0\n",
      "BMI                         0\n",
      "DiabetesPedigreeFunction    0\n",
      "Age                         0\n",
      "Outcome                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Replace zero values with NaN in columns where zero is not a valid value\n",
    "cols_with_missing_vals = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
    "df[cols_with_missing_vals] = df[cols_with_missing_vals].replace(0, np.nan)\n",
    "\n",
    "# Impute the missing values with the mean of the respective column\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# Check if there are any remaining missing values\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71871f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (537, 8)\n",
      "Test set shape: (231, 8)\n"
     ]
    }
   ],
   "source": [
    "# Split into features (X) and target (y)\n",
    "X = df.drop('Outcome', axis=1)\n",
    "y = df['Outcome']\n",
    "\n",
    "# Split data into training and test sets (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Optional: Scale the data for better model performance\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Check the shape of the data\n",
    "print(f'Training set shape: {X_train.shape}')\n",
    "print(f'Test set shape: {X_test.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f0b42ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Define the objective function\n",
    "def objective(trial):\n",
    "    # Suggest values for the hyperparameters\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 200)\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 20)\n",
    "\n",
    "    # Create the RandomForestClassifier with suggested hyperparameters\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Perform 3-fold cross-validation and calculate accuracy\n",
    "    score = cross_val_score(model, X_train, y_train, cv=3, scoring='accuracy').mean()\n",
    "\n",
    "    return score  # Return the accuracy score for Optuna to maximize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61d73581",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 21:56:48,670] A new study created in memory with name: no-name-24b699c4-9096-4fbd-a9ae-04297086bc85\n",
      "[I 2025-08-16 21:56:48,887] Trial 0 finished with value: 0.7560521415270017 and parameters: {'n_estimators': 60, 'max_depth': 4}. Best is trial 0 with value: 0.7560521415270017.\n",
      "[I 2025-08-16 21:56:49,175] Trial 1 finished with value: 0.7560521415270017 and parameters: {'n_estimators': 82, 'max_depth': 6}. Best is trial 0 with value: 0.7560521415270017.\n",
      "[I 2025-08-16 21:56:49,859] Trial 2 finished with value: 0.7709497206703911 and parameters: {'n_estimators': 173, 'max_depth': 13}. Best is trial 2 with value: 0.7709497206703911.\n",
      "[I 2025-08-16 21:56:50,119] Trial 3 finished with value: 0.7783985102420857 and parameters: {'n_estimators': 67, 'max_depth': 17}. Best is trial 3 with value: 0.7783985102420857.\n",
      "[I 2025-08-16 21:56:50,385] Trial 4 finished with value: 0.7746741154562384 and parameters: {'n_estimators': 71, 'max_depth': 16}. Best is trial 3 with value: 0.7783985102420857.\n",
      "[I 2025-08-16 21:56:50,830] Trial 5 finished with value: 0.7802607076350093 and parameters: {'n_estimators': 115, 'max_depth': 18}. Best is trial 5 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:56:51,475] Trial 6 finished with value: 0.7728119180633147 and parameters: {'n_estimators': 144, 'max_depth': 7}. Best is trial 5 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:56:52,175] Trial 7 finished with value: 0.7709497206703911 and parameters: {'n_estimators': 164, 'max_depth': 13}. Best is trial 5 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:56:52,452] Trial 8 finished with value: 0.7783985102420857 and parameters: {'n_estimators': 72, 'max_depth': 18}. Best is trial 5 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:56:53,213] Trial 9 finished with value: 0.7709497206703911 and parameters: {'n_estimators': 182, 'max_depth': 12}. Best is trial 5 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:56:53,666] Trial 10 finished with value: 0.7746741154562384 and parameters: {'n_estimators': 110, 'max_depth': 20}. Best is trial 5 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:56:54,152] Trial 11 finished with value: 0.7765363128491621 and parameters: {'n_estimators': 107, 'max_depth': 16}. Best is trial 5 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:56:54,664] Trial 12 finished with value: 0.7690875232774674 and parameters: {'n_estimators': 100, 'max_depth': 20}. Best is trial 5 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:56:55,371] Trial 13 finished with value: 0.7709497206703911 and parameters: {'n_estimators': 138, 'max_depth': 17}. Best is trial 5 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:56:55,656] Trial 14 finished with value: 0.7616387337057727 and parameters: {'n_estimators': 52, 'max_depth': 10}. Best is trial 5 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:56:56,087] Trial 15 finished with value: 0.7653631284916201 and parameters: {'n_estimators': 92, 'max_depth': 15}. Best is trial 5 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:56:56,600] Trial 16 finished with value: 0.7690875232774674 and parameters: {'n_estimators': 127, 'max_depth': 10}. Best is trial 5 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:56:57,190] Trial 17 finished with value: 0.7746741154562384 and parameters: {'n_estimators': 153, 'max_depth': 18}. Best is trial 5 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:56:58,022] Trial 18 finished with value: 0.7765363128491621 and parameters: {'n_estimators': 196, 'max_depth': 15}. Best is trial 5 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:56:58,551] Trial 19 finished with value: 0.7765363128491619 and parameters: {'n_estimators': 126, 'max_depth': 19}. Best is trial 5 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:56:59,077] Trial 20 finished with value: 0.7709497206703911 and parameters: {'n_estimators': 115, 'max_depth': 14}. Best is trial 5 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:56:59,365] Trial 21 finished with value: 0.7783985102420857 and parameters: {'n_estimators': 75, 'max_depth': 17}. Best is trial 5 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:56:59,804] Trial 22 finished with value: 0.7765363128491619 and parameters: {'n_estimators': 91, 'max_depth': 18}. Best is trial 5 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:57:00,071] Trial 23 finished with value: 0.7728119180633147 and parameters: {'n_estimators': 62, 'max_depth': 19}. Best is trial 5 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:57:00,277] Trial 24 finished with value: 0.7709497206703911 and parameters: {'n_estimators': 50, 'max_depth': 18}. Best is trial 5 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:57:00,628] Trial 25 finished with value: 0.7690875232774674 and parameters: {'n_estimators': 85, 'max_depth': 20}. Best is trial 5 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:57:00,971] Trial 26 finished with value: 0.7560521415270017 and parameters: {'n_estimators': 72, 'max_depth': 10}. Best is trial 5 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:57:01,390] Trial 27 finished with value: 0.7690875232774674 and parameters: {'n_estimators': 98, 'max_depth': 16}. Best is trial 5 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:57:01,637] Trial 28 finished with value: 0.7728119180633147 and parameters: {'n_estimators': 64, 'max_depth': 14}. Best is trial 5 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:57:01,928] Trial 29 finished with value: 0.7541899441340782 and parameters: {'n_estimators': 78, 'max_depth': 3}. Best is trial 5 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:57:02,207] Trial 30 finished with value: 0.7765363128491621 and parameters: {'n_estimators': 61, 'max_depth': 17}. Best is trial 5 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:57:02,520] Trial 31 finished with value: 0.7783985102420856 and parameters: {'n_estimators': 72, 'max_depth': 17}. Best is trial 5 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:57:02,861] Trial 32 finished with value: 0.7653631284916201 and parameters: {'n_estimators': 82, 'max_depth': 19}. Best is trial 5 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:57:03,330] Trial 33 finished with value: 0.7783985102420857 and parameters: {'n_estimators': 120, 'max_depth': 15}. Best is trial 5 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:57:03,645] Trial 34 finished with value: 0.7802607076350094 and parameters: {'n_estimators': 70, 'max_depth': 18}. Best is trial 34 with value: 0.7802607076350094.\n",
      "[I 2025-08-16 21:57:03,875] Trial 35 finished with value: 0.7690875232774674 and parameters: {'n_estimators': 56, 'max_depth': 18}. Best is trial 34 with value: 0.7802607076350094.\n",
      "[I 2025-08-16 21:57:04,134] Trial 36 finished with value: 0.7783985102420857 and parameters: {'n_estimators': 65, 'max_depth': 19}. Best is trial 34 with value: 0.7802607076350094.\n",
      "[I 2025-08-16 21:57:04,505] Trial 37 finished with value: 0.7653631284916201 and parameters: {'n_estimators': 89, 'max_depth': 8}. Best is trial 34 with value: 0.7802607076350094.\n",
      "[I 2025-08-16 21:57:04,879] Trial 38 finished with value: 0.7690875232774674 and parameters: {'n_estimators': 100, 'max_depth': 5}. Best is trial 34 with value: 0.7802607076350094.\n",
      "[I 2025-08-16 21:57:05,531] Trial 39 finished with value: 0.7709497206703911 and parameters: {'n_estimators': 141, 'max_depth': 16}. Best is trial 34 with value: 0.7802607076350094.\n",
      "[I 2025-08-16 21:57:06,174] Trial 40 finished with value: 0.7709497206703911 and parameters: {'n_estimators': 157, 'max_depth': 12}. Best is trial 34 with value: 0.7802607076350094.\n",
      "[I 2025-08-16 21:57:06,488] Trial 41 finished with value: 0.7746741154562384 and parameters: {'n_estimators': 77, 'max_depth': 17}. Best is trial 34 with value: 0.7802607076350094.\n",
      "[I 2025-08-16 21:57:06,774] Trial 42 finished with value: 0.7783985102420857 and parameters: {'n_estimators': 69, 'max_depth': 20}. Best is trial 34 with value: 0.7802607076350094.\n",
      "[I 2025-08-16 21:57:07,145] Trial 43 finished with value: 0.7635009310986964 and parameters: {'n_estimators': 76, 'max_depth': 14}. Best is trial 34 with value: 0.7802607076350094.\n",
      "[I 2025-08-16 21:57:07,372] Trial 44 finished with value: 0.7709497206703911 and parameters: {'n_estimators': 56, 'max_depth': 17}. Best is trial 34 with value: 0.7802607076350094.\n",
      "[I 2025-08-16 21:57:07,775] Trial 45 finished with value: 0.7783985102420856 and parameters: {'n_estimators': 106, 'max_depth': 18}. Best is trial 34 with value: 0.7802607076350094.\n",
      "[I 2025-08-16 21:57:08,108] Trial 46 finished with value: 0.7635009310986964 and parameters: {'n_estimators': 85, 'max_depth': 16}. Best is trial 34 with value: 0.7802607076350094.\n",
      "[I 2025-08-16 21:57:08,478] Trial 47 finished with value: 0.7672253258845437 and parameters: {'n_estimators': 96, 'max_depth': 13}. Best is trial 34 with value: 0.7802607076350094.\n",
      "[I 2025-08-16 21:57:08,743] Trial 48 finished with value: 0.7783985102420857 and parameters: {'n_estimators': 67, 'max_depth': 19}. Best is trial 34 with value: 0.7802607076350094.\n",
      "[I 2025-08-16 21:57:09,320] Trial 49 finished with value: 0.7765363128491621 and parameters: {'n_estimators': 134, 'max_depth': 16}. Best is trial 34 with value: 0.7802607076350094.\n"
     ]
    }
   ],
   "source": [
    "# Create a study object and optimize the objective function\n",
    "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())  # We aim to maximize accuracy\n",
    "study.optimize(objective, n_trials=50)  # Run 50 trials to find the best hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "284448e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial accuracy: 0.7802607076350094\n",
      "Best hyperparameters: {'n_estimators': 70, 'max_depth': 18}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print the best result\n",
    "print(f'Best trial accuracy: {study.best_trial.value}')\n",
    "print(f'Best hyperparameters: {study.best_trial.params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f407cbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy with best hyperparameters: 0.74\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train a RandomForestClassifier using the best hyperparameters from Optuna\n",
    "best_model = RandomForestClassifier(**study.best_trial.params, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print the test accuracy\n",
    "print(f'Test Accuracy with best hyperparameters: {test_accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d08d1ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sampler optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6353a7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Define the objective function\n",
    "def objective(trial):\n",
    "    # Suggest values for the hyperparameters\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 200)\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 20)\n",
    "\n",
    "    # Create the RandomForestClassifier with suggested hyperparameters\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Perform 3-fold cross-validation and calculate accuracy\n",
    "    score = cross_val_score(model, X_train, y_train, cv=3, scoring='accuracy').mean()\n",
    "\n",
    "    return score  # Return the accuracy score for Optuna to maximize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe028c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 21:57:50,966] A new study created in memory with name: no-name-777f0317-f8bf-4d48-b7d1-1e22ed1688fc\n",
      "[I 2025-08-16 21:57:52,497] Trial 0 finished with value: 0.7579143389199254 and parameters: {'n_estimators': 142, 'max_depth': 3}. Best is trial 0 with value: 0.7579143389199254.\n",
      "[I 2025-08-16 21:57:53,213] Trial 1 finished with value: 0.7709497206703911 and parameters: {'n_estimators': 58, 'max_depth': 13}. Best is trial 1 with value: 0.7709497206703911.\n",
      "[I 2025-08-16 21:57:54,727] Trial 2 finished with value: 0.7690875232774674 and parameters: {'n_estimators': 96, 'max_depth': 20}. Best is trial 1 with value: 0.7709497206703911.\n",
      "[I 2025-08-16 21:57:56,093] Trial 3 finished with value: 0.7765363128491621 and parameters: {'n_estimators': 62, 'max_depth': 7}. Best is trial 3 with value: 0.7765363128491621.\n",
      "[I 2025-08-16 21:57:58,388] Trial 4 finished with value: 0.7579143389199254 and parameters: {'n_estimators': 86, 'max_depth': 4}. Best is trial 3 with value: 0.7765363128491621.\n",
      "[I 2025-08-16 21:58:02,549] Trial 5 finished with value: 0.7728119180633147 and parameters: {'n_estimators': 144, 'max_depth': 15}. Best is trial 3 with value: 0.7765363128491621.\n",
      "[I 2025-08-16 21:58:06,920] Trial 6 finished with value: 0.7690875232774674 and parameters: {'n_estimators': 159, 'max_depth': 12}. Best is trial 3 with value: 0.7765363128491621.\n",
      "[I 2025-08-16 21:58:08,050] Trial 7 finished with value: 0.7746741154562384 and parameters: {'n_estimators': 50, 'max_depth': 5}. Best is trial 3 with value: 0.7765363128491621.\n",
      "[I 2025-08-16 21:58:09,963] Trial 8 finished with value: 0.7765363128491621 and parameters: {'n_estimators': 178, 'max_depth': 14}. Best is trial 3 with value: 0.7765363128491621.\n",
      "[I 2025-08-16 21:58:10,304] Trial 9 finished with value: 0.7709497206703911 and parameters: {'n_estimators': 77, 'max_depth': 16}. Best is trial 3 with value: 0.7765363128491621.\n",
      "[I 2025-08-16 21:58:11,003] Trial 10 finished with value: 0.7728119180633147 and parameters: {'n_estimators': 134, 'max_depth': 13}. Best is trial 3 with value: 0.7765363128491621.\n",
      "[I 2025-08-16 21:58:11,618] Trial 11 finished with value: 0.7635009310986964 and parameters: {'n_estimators': 128, 'max_depth': 11}. Best is trial 3 with value: 0.7765363128491621.\n",
      "[I 2025-08-16 21:58:12,228] Trial 12 finished with value: 0.7635009310986964 and parameters: {'n_estimators': 151, 'max_depth': 8}. Best is trial 3 with value: 0.7765363128491621.\n",
      "[I 2025-08-16 21:58:12,687] Trial 13 finished with value: 0.7672253258845437 and parameters: {'n_estimators': 124, 'max_depth': 5}. Best is trial 3 with value: 0.7765363128491621.\n",
      "[I 2025-08-16 21:58:12,929] Trial 14 finished with value: 0.7690875232774674 and parameters: {'n_estimators': 56, 'max_depth': 9}. Best is trial 3 with value: 0.7765363128491621.\n",
      "[I 2025-08-16 21:58:13,276] Trial 15 finished with value: 0.7690875232774674 and parameters: {'n_estimators': 85, 'max_depth': 19}. Best is trial 3 with value: 0.7765363128491621.\n",
      "[I 2025-08-16 21:58:13,597] Trial 16 finished with value: 0.7653631284916201 and parameters: {'n_estimators': 78, 'max_depth': 15}. Best is trial 3 with value: 0.7765363128491621.\n",
      "[I 2025-08-16 21:58:13,910] Trial 17 finished with value: 0.7635009310986964 and parameters: {'n_estimators': 81, 'max_depth': 10}. Best is trial 3 with value: 0.7765363128491621.\n",
      "[I 2025-08-16 21:58:14,138] Trial 18 finished with value: 0.7672253258845437 and parameters: {'n_estimators': 58, 'max_depth': 10}. Best is trial 3 with value: 0.7765363128491621.\n",
      "[I 2025-08-16 21:58:14,507] Trial 19 finished with value: 0.7653631284916201 and parameters: {'n_estimators': 88, 'max_depth': 20}. Best is trial 3 with value: 0.7765363128491621.\n",
      "[I 2025-08-16 21:58:15,178] Trial 20 finished with value: 0.7672253258845437 and parameters: {'n_estimators': 99, 'max_depth': 8}. Best is trial 3 with value: 0.7765363128491621.\n",
      "[I 2025-08-16 21:58:16,392] Trial 21 finished with value: 0.7802607076350093 and parameters: {'n_estimators': 54, 'max_depth': 7}. Best is trial 21 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:58:20,359] Trial 22 finished with value: 0.7783985102420857 and parameters: {'n_estimators': 122, 'max_depth': 15}. Best is trial 21 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:58:23,017] Trial 23 finished with value: 0.7690875232774674 and parameters: {'n_estimators': 134, 'max_depth': 5}. Best is trial 21 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:58:26,342] Trial 24 finished with value: 0.7802607076350093 and parameters: {'n_estimators': 116, 'max_depth': 19}. Best is trial 21 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:58:27,947] Trial 25 finished with value: 0.7653631284916201 and parameters: {'n_estimators': 60, 'max_depth': 12}. Best is trial 21 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:58:30,520] Trial 26 finished with value: 0.7616387337057727 and parameters: {'n_estimators': 114, 'max_depth': 11}. Best is trial 21 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:58:34,784] Trial 27 finished with value: 0.7709497206703911 and parameters: {'n_estimators': 158, 'max_depth': 13}. Best is trial 21 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:58:36,742] Trial 28 finished with value: 0.7616387337057727 and parameters: {'n_estimators': 90, 'max_depth': 10}. Best is trial 21 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:58:38,700] Trial 29 finished with value: 0.7672253258845437 and parameters: {'n_estimators': 69, 'max_depth': 11}. Best is trial 21 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:58:42,604] Trial 30 finished with value: 0.7672253258845437 and parameters: {'n_estimators': 112, 'max_depth': 6}. Best is trial 21 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:58:44,306] Trial 31 finished with value: 0.7616387337057727 and parameters: {'n_estimators': 70, 'max_depth': 12}. Best is trial 21 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:58:49,132] Trial 32 finished with value: 0.7709497206703911 and parameters: {'n_estimators': 163, 'max_depth': 14}. Best is trial 21 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:58:49,453] Trial 33 finished with value: 0.7746741154562384 and parameters: {'n_estimators': 76, 'max_depth': 20}. Best is trial 21 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:58:49,759] Trial 34 finished with value: 0.7616387337057727 and parameters: {'n_estimators': 78, 'max_depth': 9}. Best is trial 21 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:58:50,101] Trial 35 finished with value: 0.7709497206703911 and parameters: {'n_estimators': 88, 'max_depth': 13}. Best is trial 21 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:58:50,310] Trial 36 finished with value: 0.7690875232774674 and parameters: {'n_estimators': 53, 'max_depth': 9}. Best is trial 21 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:58:50,719] Trial 37 finished with value: 0.7560521415270017 and parameters: {'n_estimators': 97, 'max_depth': 9}. Best is trial 21 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:58:51,072] Trial 38 finished with value: 0.7635009310986964 and parameters: {'n_estimators': 81, 'max_depth': 10}. Best is trial 21 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:58:51,774] Trial 39 finished with value: 0.7728119180633147 and parameters: {'n_estimators': 165, 'max_depth': 17}. Best is trial 21 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:58:52,162] Trial 40 finished with value: 0.7653631284916201 and parameters: {'n_estimators': 92, 'max_depth': 15}. Best is trial 21 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:58:52,459] Trial 41 finished with value: 0.7783985102420857 and parameters: {'n_estimators': 75, 'max_depth': 18}. Best is trial 21 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:58:52,759] Trial 42 finished with value: 0.7709497206703911 and parameters: {'n_estimators': 77, 'max_depth': 16}. Best is trial 21 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:58:53,358] Trial 43 finished with value: 0.7728119180633147 and parameters: {'n_estimators': 139, 'max_depth': 16}. Best is trial 21 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:58:54,032] Trial 44 finished with value: 0.7709497206703911 and parameters: {'n_estimators': 167, 'max_depth': 18}. Best is trial 21 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:58:54,329] Trial 45 finished with value: 0.7728119180633147 and parameters: {'n_estimators': 76, 'max_depth': 13}. Best is trial 21 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:58:54,635] Trial 46 finished with value: 0.7635009310986964 and parameters: {'n_estimators': 93, 'max_depth': 4}. Best is trial 21 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:58:54,954] Trial 47 finished with value: 0.7765363128491621 and parameters: {'n_estimators': 87, 'max_depth': 20}. Best is trial 21 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:58:55,169] Trial 48 finished with value: 0.7597765363128491 and parameters: {'n_estimators': 62, 'max_depth': 4}. Best is trial 21 with value: 0.7802607076350093.\n",
      "[I 2025-08-16 21:58:55,916] Trial 49 finished with value: 0.7709497206703911 and parameters: {'n_estimators': 189, 'max_depth': 15}. Best is trial 21 with value: 0.7802607076350093.\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.RandomSampler())  # We aim to maximize accuracy\n",
    "study.optimize(objective, n_trials=50)  # Run 50 trials to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5d8719b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial accuracy: 0.7802607076350093\n",
      "Best hyperparameters: {'n_estimators': 54, 'max_depth': 7}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print the best result\n",
    "print(f'Best trial accuracy: {study.best_trial.value}')\n",
    "print(f'Best hyperparameters: {study.best_trial.params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7499662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy with best hyperparameters: 0.74\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train a RandomForestClassifier using the best hyperparameters from Optuna\n",
    "best_model = RandomForestClassifier(**study.best_trial.params, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print the test accuracy\n",
    "print(f'Test Accuracy with best hyperparameters: {test_accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ecf91c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'max_depth': [5, 10, 15, 20]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "232e3c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 21:58:56,031] A new study created in memory with name: no-name-db9c806c-1954-4d9f-997c-31f29fb1871a\n",
      "[I 2025-08-16 21:58:56,361] Trial 0 finished with value: 0.7690875232774674 and parameters: {'n_estimators': 100, 'max_depth': 5}. Best is trial 0 with value: 0.7690875232774674.\n",
      "[I 2025-08-16 21:58:56,951] Trial 1 finished with value: 0.7672253258845437 and parameters: {'n_estimators': 150, 'max_depth': 10}. Best is trial 0 with value: 0.7690875232774674.\n",
      "[I 2025-08-16 21:58:57,148] Trial 2 finished with value: 0.7728119180633147 and parameters: {'n_estimators': 50, 'max_depth': 15}. Best is trial 2 with value: 0.7728119180633147.\n",
      "[I 2025-08-16 21:58:57,561] Trial 3 finished with value: 0.7653631284916201 and parameters: {'n_estimators': 100, 'max_depth': 15}. Best is trial 2 with value: 0.7728119180633147.\n",
      "[I 2025-08-16 21:58:57,932] Trial 4 finished with value: 0.7690875232774674 and parameters: {'n_estimators': 100, 'max_depth': 20}. Best is trial 2 with value: 0.7728119180633147.\n",
      "[I 2025-08-16 21:58:58,147] Trial 5 finished with value: 0.7579143389199254 and parameters: {'n_estimators': 50, 'max_depth': 10}. Best is trial 2 with value: 0.7728119180633147.\n",
      "[I 2025-08-16 21:58:58,655] Trial 6 finished with value: 0.7653631284916201 and parameters: {'n_estimators': 150, 'max_depth': 5}. Best is trial 2 with value: 0.7728119180633147.\n",
      "[I 2025-08-16 21:58:59,256] Trial 7 finished with value: 0.7728119180633147 and parameters: {'n_estimators': 150, 'max_depth': 20}. Best is trial 2 with value: 0.7728119180633147.\n",
      "[I 2025-08-16 21:58:59,808] Trial 8 finished with value: 0.7728119180633147 and parameters: {'n_estimators': 150, 'max_depth': 15}. Best is trial 2 with value: 0.7728119180633147.\n",
      "[I 2025-08-16 21:59:00,597] Trial 9 finished with value: 0.7709497206703911 and parameters: {'n_estimators': 200, 'max_depth': 10}. Best is trial 2 with value: 0.7728119180633147.\n",
      "[I 2025-08-16 21:59:01,378] Trial 10 finished with value: 0.7728119180633147 and parameters: {'n_estimators': 200, 'max_depth': 20}. Best is trial 2 with value: 0.7728119180633147.\n",
      "[I 2025-08-16 21:59:02,189] Trial 11 finished with value: 0.7728119180633147 and parameters: {'n_estimators': 200, 'max_depth': 15}. Best is trial 2 with value: 0.7728119180633147.\n",
      "[I 2025-08-16 21:59:02,869] Trial 12 finished with value: 0.7690875232774674 and parameters: {'n_estimators': 200, 'max_depth': 5}. Best is trial 2 with value: 0.7728119180633147.\n",
      "[I 2025-08-16 21:59:03,059] Trial 13 finished with value: 0.7746741154562384 and parameters: {'n_estimators': 50, 'max_depth': 5}. Best is trial 13 with value: 0.7746741154562384.\n",
      "[I 2025-08-16 21:59:03,447] Trial 14 finished with value: 0.7616387337057727 and parameters: {'n_estimators': 100, 'max_depth': 10}. Best is trial 13 with value: 0.7746741154562384.\n",
      "[I 2025-08-16 21:59:03,640] Trial 15 finished with value: 0.7709497206703911 and parameters: {'n_estimators': 50, 'max_depth': 20}. Best is trial 13 with value: 0.7746741154562384.\n"
     ]
    }
   ],
   "source": [
    "# Create a study and optimize it using GridSampler\n",
    "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.GridSampler(search_space))\n",
    "study.optimize(objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cfed089d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial accuracy: 0.7746741154562384\n",
      "Best hyperparameters: {'n_estimators': 50, 'max_depth': 5}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print the best result\n",
    "print(f'Best trial accuracy: {study.best_trial.value}')\n",
    "print(f'Best hyperparameters: {study.best_trial.params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aec70643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy with best hyperparameters: 0.74\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train a RandomForestClassifier using the best hyperparameters from Optuna\n",
    "best_model = RandomForestClassifier(**study.best_trial.params, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print the test accuracy\n",
    "print(f'Test Accuracy with best hyperparameters: {test_accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cdde1ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optuna visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1319014f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For visualizations\n",
    "from optuna.visualization import plot_optimization_history, plot_parallel_coordinate, plot_slice, plot_contour, plot_param_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76f3f03b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Optuna\\env\\Lib\\site-packages\\optuna\\visualization\\_plotly_imports.py:7\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m try_import() \u001b[38;5;28;01mas\u001b[39;00m _imports:\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m plotly_version\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'plotly'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 2. Parallel Coordinates Plot\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mplot_parallel_coordinate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m)\u001b[49m.show()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Optuna\\env\\Lib\\site-packages\\optuna\\visualization\\_parallel_coordinate.py:83\u001b[39m, in \u001b[36mplot_parallel_coordinate\u001b[39m\u001b[34m(study, params, target, target_name)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplot_parallel_coordinate\u001b[39m(\n\u001b[32m     51\u001b[39m     study: Study,\n\u001b[32m     52\u001b[39m     params: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     55\u001b[39m     target_name: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mObjective Value\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     56\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mgo.Figure\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     57\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Plot the high-dimensional parameter relationships in a study.\u001b[39;00m\n\u001b[32m     58\u001b[39m \n\u001b[32m     59\u001b[39m \u001b[33;03m    Note that, if a parameter contains missing values, a trial with missing values is not plotted.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m \u001b[33;03m        of :class:`~optuna.study.Study` is ``minimize``.\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     \u001b[43m_imports\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m     info = _get_parallel_coordinate_info(study, params, target, target_name)\n\u001b[32m     85\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _get_parallel_coordinate_plot(info)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Optuna\\env\\Lib\\site-packages\\optuna\\_imports.py:94\u001b[39m, in \u001b[36m_DeferredImportExceptionContextManager.check\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._deferred \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     93\u001b[39m     exc_value, message = \u001b[38;5;28mself\u001b[39m._deferred\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc_value\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'."
     ]
    }
   ],
   "source": [
    "# 2. Parallel Coordinates Plot\n",
    "plot_parallel_coordinate(study).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b48e5e97",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Optuna\\env\\Lib\\site-packages\\optuna\\visualization\\_plotly_imports.py:7\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m try_import() \u001b[38;5;28;01mas\u001b[39;00m _imports:\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m plotly_version\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'plotly'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 3. Slice Plot\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mplot_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m)\u001b[49m.show()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Optuna\\env\\Lib\\site-packages\\optuna\\visualization\\_slice.py:172\u001b[39m, in \u001b[36mplot_slice\u001b[39m\u001b[34m(study, params, target, target_name)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplot_slice\u001b[39m(\n\u001b[32m    144\u001b[39m     study: Study,\n\u001b[32m    145\u001b[39m     params: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    148\u001b[39m     target_name: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mObjective Value\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    149\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mgo.Figure\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    150\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Plot the parameter relationship as slice plot in a study.\u001b[39;00m\n\u001b[32m    151\u001b[39m \n\u001b[32m    152\u001b[39m \u001b[33;03m    Note that, if a parameter contains missing values, a trial with missing values is not plotted.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    169\u001b[39m \u001b[33;03m        A :class:`plotly.graph_objects.Figure` object.\u001b[39;00m\n\u001b[32m    170\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m     \u001b[43m_imports\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _get_slice_plot(_get_slice_plot_info(study, params, target, target_name))\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Optuna\\env\\Lib\\site-packages\\optuna\\_imports.py:94\u001b[39m, in \u001b[36m_DeferredImportExceptionContextManager.check\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._deferred \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     93\u001b[39m     exc_value, message = \u001b[38;5;28mself\u001b[39m._deferred\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc_value\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'."
     ]
    }
   ],
   "source": [
    "# 3. Slice Plot\n",
    "plot_slice(study).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e533516",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Optuna\\env\\Lib\\site-packages\\optuna\\visualization\\_plotly_imports.py:7\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m try_import() \u001b[38;5;28;01mas\u001b[39;00m _imports:\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m plotly_version\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'plotly'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 4. Contour Plot\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mplot_contour\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m)\u001b[49m.show()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Optuna\\env\\Lib\\site-packages\\optuna\\visualization\\_contour.py:99\u001b[39m, in \u001b[36mplot_contour\u001b[39m\u001b[34m(study, params, target, target_name)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplot_contour\u001b[39m(\n\u001b[32m     67\u001b[39m     study: Study,\n\u001b[32m     68\u001b[39m     params: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     71\u001b[39m     target_name: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mObjective Value\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     72\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mgo.Figure\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     73\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Plot the parameter relationship as contour plot in a study.\u001b[39;00m\n\u001b[32m     74\u001b[39m \n\u001b[32m     75\u001b[39m \u001b[33;03m    Note that, if a parameter contains missing values, a trial with missing values is not plotted.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     96\u001b[39m \u001b[33;03m        of :class:`~optuna.study.Study` is ``minimize``.\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     \u001b[43m_imports\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m     info = _get_contour_info(study, params, target, target_name)\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _get_contour_plot(info)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Optuna\\env\\Lib\\site-packages\\optuna\\_imports.py:94\u001b[39m, in \u001b[36m_DeferredImportExceptionContextManager.check\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._deferred \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     93\u001b[39m     exc_value, message = \u001b[38;5;28mself\u001b[39m._deferred\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc_value\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'."
     ]
    }
   ],
   "source": [
    "# 4. Contour Plot\n",
    "plot_contour(study).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0877b25b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Optuna\\env\\Lib\\site-packages\\optuna\\visualization\\_plotly_imports.py:7\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m try_import() \u001b[38;5;28;01mas\u001b[39;00m _imports:\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m plotly_version\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'plotly'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 5. Hyperparameter Importance\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mplot_param_importances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m)\u001b[49m.show()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Optuna\\env\\Lib\\site-packages\\optuna\\visualization\\_param_importances.py:167\u001b[39m, in \u001b[36mplot_param_importances\u001b[39m\u001b[34m(study, evaluator, params, target, target_name)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplot_param_importances\u001b[39m(\n\u001b[32m    114\u001b[39m     study: Study,\n\u001b[32m    115\u001b[39m     evaluator: BaseImportanceEvaluator | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    119\u001b[39m     target_name: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mObjective Value\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    120\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mgo.Figure\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    121\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Plot hyperparameter importances.\u001b[39;00m\n\u001b[32m    122\u001b[39m \n\u001b[32m    123\u001b[39m \u001b[33;03m    .. seealso::\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    164\u001b[39m \u001b[33;03m        A :class:`plotly.graph_objects.Figure` object.\u001b[39;00m\n\u001b[32m    165\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[43m_imports\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    168\u001b[39m     importances_infos = _get_importances_infos(study, evaluator, params, target, target_name)\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _get_importances_plot(importances_infos, study)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Optuna\\env\\Lib\\site-packages\\optuna\\_imports.py:94\u001b[39m, in \u001b[36m_DeferredImportExceptionContextManager.check\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._deferred \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     93\u001b[39m     exc_value, message = \u001b[38;5;28mself\u001b[39m._deferred\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc_value\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'."
     ]
    }
   ],
   "source": [
    "# 5. Hyperparameter Importance\n",
    "plot_param_importances(study).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3383dd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimaising ML multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "55034f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "58d14b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Choose the algorithm to tune\n",
    "    classifier_name = trial.suggest_categorical('classifier', ['SVM', 'RandomForest', 'GradientBoosting'])\n",
    "\n",
    "    if classifier_name == 'SVM':\n",
    "        # SVM hyperparameters\n",
    "        c = trial.suggest_float('C', 0.1, 100, log=True)\n",
    "        kernel = trial.suggest_categorical('kernel', ['linear', 'rbf', 'poly', 'sigmoid'])\n",
    "        gamma = trial.suggest_categorical('gamma', ['scale', 'auto'])\n",
    "\n",
    "        model = SVC(C=c, kernel=kernel, gamma=gamma, random_state=42)\n",
    "\n",
    "    elif classifier_name == 'RandomForest':\n",
    "        # Random Forest hyperparameters\n",
    "        n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "        max_depth = trial.suggest_int('max_depth', 3, 20)\n",
    "        min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
    "        min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "        bootstrap = trial.suggest_categorical('bootstrap', [True, False])\n",
    "\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            bootstrap=bootstrap,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    elif classifier_name == 'GradientBoosting':\n",
    "        # Gradient Boosting hyperparameters\n",
    "        n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "        learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3, log=True)\n",
    "        max_depth = trial.suggest_int('max_depth', 3, 20)\n",
    "        min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
    "        min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "\n",
    "        model = GradientBoostingClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    # Perform cross-validation and return the mean accuracy\n",
    "    score = cross_val_score(model, X_train, y_train, cv=3, scoring='accuracy').mean()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9beba9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:00:45,776] A new study created in memory with name: no-name-c665052c-f76b-4b60-b574-0bb45b637d91\n",
      "[I 2025-08-16 22:00:47,781] Trial 0 finished with value: 0.7690875232774674 and parameters: {'classifier': 'GradientBoosting', 'n_estimators': 96, 'learning_rate': 0.02051608719951715, 'max_depth': 14, 'min_samples_split': 9, 'min_samples_leaf': 10}. Best is trial 0 with value: 0.7690875232774674.\n",
      "[I 2025-08-16 22:00:47,841] Trial 1 finished with value: 0.7858472998137801 and parameters: {'classifier': 'SVM', 'C': 1.9294669536846905, 'kernel': 'linear', 'gamma': 'auto'}. Best is trial 1 with value: 0.7858472998137801.\n",
      "[I 2025-08-16 22:01:01,062] Trial 2 finished with value: 0.7374301675977654 and parameters: {'classifier': 'GradientBoosting', 'n_estimators': 256, 'learning_rate': 0.058740592869685956, 'max_depth': 16, 'min_samples_split': 5, 'min_samples_leaf': 8}. Best is trial 1 with value: 0.7858472998137801.\n",
      "[I 2025-08-16 22:01:26,148] Trial 3 finished with value: 0.6610800744878956 and parameters: {'classifier': 'GradientBoosting', 'n_estimators': 202, 'learning_rate': 0.06752527512102555, 'max_depth': 20, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 1 with value: 0.7858472998137801.\n",
      "[I 2025-08-16 22:01:28,109] Trial 4 finished with value: 0.7579143389199254 and parameters: {'classifier': 'RandomForest', 'n_estimators': 96, 'max_depth': 11, 'min_samples_split': 9, 'min_samples_leaf': 6, 'bootstrap': True}. Best is trial 1 with value: 0.7858472998137801.\n",
      "[I 2025-08-16 22:01:42,750] Trial 5 finished with value: 0.7392923649906891 and parameters: {'classifier': 'GradientBoosting', 'n_estimators': 257, 'learning_rate': 0.017749557872102337, 'max_depth': 13, 'min_samples_split': 8, 'min_samples_leaf': 2}. Best is trial 1 with value: 0.7858472998137801.\n",
      "[I 2025-08-16 22:01:43,602] Trial 6 finished with value: 0.7597765363128492 and parameters: {'classifier': 'GradientBoosting', 'n_estimators': 159, 'learning_rate': 0.05667436228362053, 'max_depth': 6, 'min_samples_split': 9, 'min_samples_leaf': 8}. Best is trial 1 with value: 0.7858472998137801.\n",
      "[I 2025-08-16 22:01:44,189] Trial 7 finished with value: 0.7728119180633147 and parameters: {'classifier': 'RandomForest', 'n_estimators': 185, 'max_depth': 19, 'min_samples_split': 3, 'min_samples_leaf': 7, 'bootstrap': False}. Best is trial 1 with value: 0.7858472998137801.\n",
      "[I 2025-08-16 22:01:44,205] Trial 8 finished with value: 0.7169459962756052 and parameters: {'classifier': 'SVM', 'C': 0.15821854480324404, 'kernel': 'poly', 'gamma': 'auto'}. Best is trial 1 with value: 0.7858472998137801.\n",
      "[I 2025-08-16 22:01:44,768] Trial 9 finished with value: 0.7709497206703911 and parameters: {'classifier': 'RandomForest', 'n_estimators': 190, 'max_depth': 13, 'min_samples_split': 5, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 1 with value: 0.7858472998137801.\n",
      "[I 2025-08-16 22:01:44,821] Trial 10 finished with value: 0.7858472998137801 and parameters: {'classifier': 'SVM', 'C': 10.838289966282453, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 1 with value: 0.7858472998137801.\n",
      "[I 2025-08-16 22:01:44,865] Trial 11 finished with value: 0.7858472998137801 and parameters: {'classifier': 'SVM', 'C': 12.108848032741916, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 1 with value: 0.7858472998137801.\n",
      "[I 2025-08-16 22:01:44,897] Trial 12 finished with value: 0.7858472998137801 and parameters: {'classifier': 'SVM', 'C': 3.492699575763524, 'kernel': 'linear', 'gamma': 'auto'}. Best is trial 1 with value: 0.7858472998137801.\n",
      "[I 2025-08-16 22:01:45,134] Trial 13 finished with value: 0.7858472998137801 and parameters: {'classifier': 'SVM', 'C': 98.12420587140208, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 1 with value: 0.7858472998137801.\n",
      "[I 2025-08-16 22:01:45,148] Trial 14 finished with value: 0.7448789571694601 and parameters: {'classifier': 'SVM', 'C': 0.8206681518112704, 'kernel': 'sigmoid', 'gamma': 'auto'}. Best is trial 1 with value: 0.7858472998137801.\n",
      "[I 2025-08-16 22:01:45,182] Trial 15 finished with value: 0.7579143389199254 and parameters: {'classifier': 'SVM', 'C': 10.693457092059294, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 1 with value: 0.7858472998137801.\n",
      "[I 2025-08-16 22:01:45,203] Trial 16 finished with value: 0.7839851024208566 and parameters: {'classifier': 'SVM', 'C': 1.1397893162033395, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 1 with value: 0.7858472998137801.\n",
      "[I 2025-08-16 22:01:45,259] Trial 17 finished with value: 0.7858472998137801 and parameters: {'classifier': 'SVM', 'C': 16.45565810388114, 'kernel': 'linear', 'gamma': 'auto'}. Best is trial 1 with value: 0.7858472998137801.\n",
      "[I 2025-08-16 22:01:45,275] Trial 18 finished with value: 0.7709497206703911 and parameters: {'classifier': 'SVM', 'C': 2.7362452871194693, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 1 with value: 0.7858472998137801.\n",
      "[I 2025-08-16 22:01:45,306] Trial 19 finished with value: 0.702048417132216 and parameters: {'classifier': 'SVM', 'C': 50.301743124418564, 'kernel': 'sigmoid', 'gamma': 'auto'}. Best is trial 1 with value: 0.7858472998137801.\n",
      "[I 2025-08-16 22:01:45,502] Trial 20 finished with value: 0.756052141527002 and parameters: {'classifier': 'RandomForest', 'n_estimators': 56, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 4, 'bootstrap': True}. Best is trial 1 with value: 0.7858472998137801.\n",
      "[I 2025-08-16 22:01:45,550] Trial 21 finished with value: 0.7858472998137801 and parameters: {'classifier': 'SVM', 'C': 11.240973021994762, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 1 with value: 0.7858472998137801.\n",
      "[I 2025-08-16 22:01:45,582] Trial 22 finished with value: 0.7858472998137801 and parameters: {'classifier': 'SVM', 'C': 4.555979238048639, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 1 with value: 0.7858472998137801.\n",
      "[I 2025-08-16 22:01:45,673] Trial 23 finished with value: 0.7858472998137801 and parameters: {'classifier': 'SVM', 'C': 25.74849264245345, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 1 with value: 0.7858472998137801.\n",
      "[I 2025-08-16 22:01:45,704] Trial 24 finished with value: 0.7281191806331471 and parameters: {'classifier': 'SVM', 'C': 1.096603332394435, 'kernel': 'poly', 'gamma': 'scale'}. Best is trial 1 with value: 0.7858472998137801.\n",
      "[I 2025-08-16 22:01:45,738] Trial 25 finished with value: 0.7858472998137801 and parameters: {'classifier': 'SVM', 'C': 5.74824839588953, 'kernel': 'linear', 'gamma': 'auto'}. Best is trial 1 with value: 0.7858472998137801.\n",
      "[I 2025-08-16 22:01:45,752] Trial 26 finished with value: 0.7858472998137803 and parameters: {'classifier': 'SVM', 'C': 0.3253827754654021, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 26 with value: 0.7858472998137803.\n",
      "[I 2025-08-16 22:01:45,768] Trial 27 finished with value: 0.7877094972067038 and parameters: {'classifier': 'SVM', 'C': 0.17833705666551217, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 27 with value: 0.7877094972067038.\n",
      "[I 2025-08-16 22:01:45,799] Trial 28 finished with value: 0.707635009310987 and parameters: {'classifier': 'SVM', 'C': 0.1081803662608189, 'kernel': 'poly', 'gamma': 'auto'}. Best is trial 27 with value: 0.7877094972067038.\n",
      "[I 2025-08-16 22:01:46,745] Trial 29 finished with value: 0.7709497206703911 and parameters: {'classifier': 'RandomForest', 'n_estimators': 288, 'max_depth': 9, 'min_samples_split': 7, 'min_samples_leaf': 4, 'bootstrap': False}. Best is trial 27 with value: 0.7877094972067038.\n",
      "[I 2025-08-16 22:01:47,295] Trial 30 finished with value: 0.7541899441340781 and parameters: {'classifier': 'GradientBoosting', 'n_estimators': 145, 'learning_rate': 0.25795850558757066, 'max_depth': 3, 'min_samples_split': 6, 'min_samples_leaf': 10}. Best is trial 27 with value: 0.7877094972067038.\n",
      "[I 2025-08-16 22:01:47,316] Trial 31 finished with value: 0.7858472998137803 and parameters: {'classifier': 'SVM', 'C': 0.3013497977523368, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 27 with value: 0.7877094972067038.\n",
      "[I 2025-08-16 22:01:47,344] Trial 32 finished with value: 0.7858472998137803 and parameters: {'classifier': 'SVM', 'C': 0.27693228877669407, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 27 with value: 0.7877094972067038.\n",
      "[I 2025-08-16 22:01:47,370] Trial 33 finished with value: 0.7821229050279329 and parameters: {'classifier': 'SVM', 'C': 0.3630731913305216, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 27 with value: 0.7877094972067038.\n",
      "[I 2025-08-16 22:01:47,391] Trial 34 finished with value: 0.7858472998137803 and parameters: {'classifier': 'SVM', 'C': 0.32139203180191783, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 27 with value: 0.7877094972067038.\n",
      "[I 2025-08-16 22:01:47,412] Trial 35 finished with value: 0.756052141527002 and parameters: {'classifier': 'SVM', 'C': 0.33491501100104887, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 27 with value: 0.7877094972067038.\n",
      "[I 2025-08-16 22:01:47,801] Trial 36 finished with value: 0.74487895716946 and parameters: {'classifier': 'GradientBoosting', 'n_estimators': 50, 'learning_rate': 0.1972640764792912, 'max_depth': 8, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 27 with value: 0.7877094972067038.\n",
      "[I 2025-08-16 22:01:47,822] Trial 37 finished with value: 0.7858472998137803 and parameters: {'classifier': 'SVM', 'C': 0.23490228784373757, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 27 with value: 0.7877094972067038.\n",
      "[I 2025-08-16 22:01:50,193] Trial 38 finished with value: 0.6983240223463687 and parameters: {'classifier': 'GradientBoosting', 'n_estimators': 231, 'learning_rate': 0.010731318115949167, 'max_depth': 16, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 27 with value: 0.7877094972067038.\n",
      "[I 2025-08-16 22:01:50,752] Trial 39 finished with value: 0.7635009310986964 and parameters: {'classifier': 'RandomForest', 'n_estimators': 126, 'max_depth': 17, 'min_samples_split': 7, 'min_samples_leaf': 4, 'bootstrap': True}. Best is trial 27 with value: 0.7877094972067038.\n",
      "[I 2025-08-16 22:01:50,774] Trial 40 finished with value: 0.7672253258845437 and parameters: {'classifier': 'SVM', 'C': 0.5708047576302696, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 27 with value: 0.7877094972067038.\n",
      "[I 2025-08-16 22:01:50,794] Trial 41 finished with value: 0.7858472998137803 and parameters: {'classifier': 'SVM', 'C': 0.21650999482783137, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 27 with value: 0.7877094972067038.\n",
      "[I 2025-08-16 22:01:50,822] Trial 42 finished with value: 0.7839851024208566 and parameters: {'classifier': 'SVM', 'C': 0.4388774436479353, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 27 with value: 0.7877094972067038.\n",
      "[I 2025-08-16 22:01:50,836] Trial 43 finished with value: 0.7895716945996275 and parameters: {'classifier': 'SVM', 'C': 0.11889704493610485, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:01:50,856] Trial 44 finished with value: 0.7877094972067038 and parameters: {'classifier': 'SVM', 'C': 0.10211897713453198, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:01:50,877] Trial 45 finished with value: 0.7895716945996275 and parameters: {'classifier': 'SVM', 'C': 0.11996381735254051, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:01:52,635] Trial 46 finished with value: 0.7355679702048418 and parameters: {'classifier': 'GradientBoosting', 'n_estimators': 290, 'learning_rate': 0.13055184488603466, 'max_depth': 6, 'min_samples_split': 4, 'min_samples_leaf': 9}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:01:52,670] Trial 47 finished with value: 0.7895716945996275 and parameters: {'classifier': 'SVM', 'C': 0.13681581123369563, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:01:53,426] Trial 48 finished with value: 0.7783985102420856 and parameters: {'classifier': 'RandomForest', 'n_estimators': 220, 'max_depth': 6, 'min_samples_split': 10, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:01:53,447] Trial 49 finished with value: 0.7094972067039106 and parameters: {'classifier': 'SVM', 'C': 0.1151162009802627, 'kernel': 'poly', 'gamma': 'scale'}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:01:53,474] Trial 50 finished with value: 0.7895716945996275 and parameters: {'classifier': 'SVM', 'C': 0.15016335534449826, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:01:53,503] Trial 51 finished with value: 0.7877094972067038 and parameters: {'classifier': 'SVM', 'C': 0.15581468683325408, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:01:53,530] Trial 52 finished with value: 0.7895716945996275 and parameters: {'classifier': 'SVM', 'C': 0.13980884113274392, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:01:53,565] Trial 53 finished with value: 0.7877094972067038 and parameters: {'classifier': 'SVM', 'C': 0.16327978260972695, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:01:53,593] Trial 54 finished with value: 0.7877094972067038 and parameters: {'classifier': 'SVM', 'C': 0.16925719481179977, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:01:53,620] Trial 55 finished with value: 0.7877094972067038 and parameters: {'classifier': 'SVM', 'C': 0.15986605037212379, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:01:53,651] Trial 56 finished with value: 0.7858472998137803 and parameters: {'classifier': 'SVM', 'C': 0.19334660155694142, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:01:53,672] Trial 57 finished with value: 0.7653631284916201 and parameters: {'classifier': 'SVM', 'C': 0.10030873696546441, 'kernel': 'sigmoid', 'gamma': 'scale'}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:01:53,699] Trial 58 finished with value: 0.7653631284916201 and parameters: {'classifier': 'SVM', 'C': 0.555662709691718, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:01:54,133] Trial 59 finished with value: 0.7597765363128491 and parameters: {'classifier': 'RandomForest', 'n_estimators': 95, 'max_depth': 10, 'min_samples_split': 7, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:01:54,158] Trial 60 finished with value: 0.7895716945996275 and parameters: {'classifier': 'SVM', 'C': 0.15053158774670505, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:01:54,176] Trial 61 finished with value: 0.7895716945996275 and parameters: {'classifier': 'SVM', 'C': 0.13707849557103527, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:01:54,189] Trial 62 finished with value: 0.7895716945996275 and parameters: {'classifier': 'SVM', 'C': 0.13225295023502534, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:01:54,212] Trial 63 finished with value: 0.7858472998137803 and parameters: {'classifier': 'SVM', 'C': 0.2230891115446507, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:01:54,229] Trial 64 finished with value: 0.7895716945996275 and parameters: {'classifier': 'SVM', 'C': 0.12828861306053246, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:01:54,253] Trial 65 finished with value: 0.7858472998137803 and parameters: {'classifier': 'SVM', 'C': 0.22599323536647845, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:01:54,270] Trial 66 finished with value: 0.7895716945996275 and parameters: {'classifier': 'SVM', 'C': 0.12871724666551218, 'kernel': 'linear', 'gamma': 'auto'}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:01:55,578] Trial 67 finished with value: 0.7672253258845437 and parameters: {'classifier': 'GradientBoosting', 'n_estimators': 131, 'learning_rate': 0.029107072196555915, 'max_depth': 18, 'min_samples_split': 3, 'min_samples_leaf': 7}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:01:55,704] Trial 68 finished with value: 0.7839851024208566 and parameters: {'classifier': 'SVM', 'C': 0.46187380997518046, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:01:55,808] Trial 69 finished with value: 0.7113594040968342 and parameters: {'classifier': 'SVM', 'C': 0.1431743694076442, 'kernel': 'poly', 'gamma': 'scale'}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:01:55,954] Trial 70 finished with value: 0.7709497206703911 and parameters: {'classifier': 'SVM', 'C': 1.8950514996447085, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:01:56,059] Trial 71 finished with value: 0.7895716945996275 and parameters: {'classifier': 'SVM', 'C': 0.14055247656681916, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:01:56,148] Trial 72 finished with value: 0.7858472998137803 and parameters: {'classifier': 'SVM', 'C': 0.20077741309947528, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:01:56,259] Trial 73 finished with value: 0.7895716945996275 and parameters: {'classifier': 'SVM', 'C': 0.1310850179921296, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:01:56,356] Trial 74 finished with value: 0.7877094972067038 and parameters: {'classifier': 'SVM', 'C': 0.1005896254884712, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:01:56,462] Trial 75 finished with value: 0.7690875232774674 and parameters: {'classifier': 'SVM', 'C': 0.2279876927124643, 'kernel': 'sigmoid', 'gamma': 'auto'}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:01:56,564] Trial 76 finished with value: 0.7877094972067038 and parameters: {'classifier': 'SVM', 'C': 0.17795102080030378, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:01:56,670] Trial 77 finished with value: 0.7858472998137803 and parameters: {'classifier': 'SVM', 'C': 0.2556610566460956, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:01:58,170] Trial 78 finished with value: 0.7728119180633147 and parameters: {'classifier': 'RandomForest', 'n_estimators': 77, 'max_depth': 15, 'min_samples_split': 6, 'min_samples_leaf': 5, 'bootstrap': True}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:01:58,259] Trial 79 finished with value: 0.7895716945996275 and parameters: {'classifier': 'SVM', 'C': 0.13153805771745755, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:02:03,821] Trial 80 finished with value: 0.7560521415270017 and parameters: {'classifier': 'GradientBoosting', 'n_estimators': 167, 'learning_rate': 0.11235957149879591, 'max_depth': 8, 'min_samples_split': 8, 'min_samples_leaf': 8}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:02:03,919] Trial 81 finished with value: 0.7895716945996275 and parameters: {'classifier': 'SVM', 'C': 0.12802030997156058, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:02:04,008] Trial 82 finished with value: 0.7895716945996275 and parameters: {'classifier': 'SVM', 'C': 0.12679341065092745, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:02:04,100] Trial 83 finished with value: 0.7877094972067038 and parameters: {'classifier': 'SVM', 'C': 0.1715148727059038, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:02:04,197] Trial 84 finished with value: 0.7877094972067038 and parameters: {'classifier': 'SVM', 'C': 0.10092339647972846, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:02:04,281] Trial 85 finished with value: 0.7858472998137803 and parameters: {'classifier': 'SVM', 'C': 0.2647149812829191, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:02:04,363] Trial 86 finished with value: 0.7858472998137803 and parameters: {'classifier': 'SVM', 'C': 0.18910196709234545, 'kernel': 'linear', 'gamma': 'auto'}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:02:04,447] Trial 87 finished with value: 0.7895716945996275 and parameters: {'classifier': 'SVM', 'C': 0.15130489318384382, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:02:04,543] Trial 88 finished with value: 0.7113594040968342 and parameters: {'classifier': 'SVM', 'C': 0.12404291638520967, 'kernel': 'poly', 'gamma': 'scale'}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:02:04,633] Trial 89 finished with value: 0.7839851024208566 and parameters: {'classifier': 'SVM', 'C': 0.35712773853613666, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:02:05,099] Trial 90 finished with value: 0.7858472998137801 and parameters: {'classifier': 'SVM', 'C': 42.4248397399297, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:02:05,190] Trial 91 finished with value: 0.7895716945996275 and parameters: {'classifier': 'SVM', 'C': 0.12177422386663435, 'kernel': 'linear', 'gamma': 'auto'}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:02:05,280] Trial 92 finished with value: 0.7877094972067038 and parameters: {'classifier': 'SVM', 'C': 0.15523402857035584, 'kernel': 'linear', 'gamma': 'auto'}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:02:05,371] Trial 93 finished with value: 0.7858472998137803 and parameters: {'classifier': 'SVM', 'C': 0.2771617186734401, 'kernel': 'linear', 'gamma': 'auto'}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:02:05,455] Trial 94 finished with value: 0.7858472998137803 and parameters: {'classifier': 'SVM', 'C': 0.1964282146686995, 'kernel': 'linear', 'gamma': 'auto'}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:02:05,544] Trial 95 finished with value: 0.6983240223463687 and parameters: {'classifier': 'SVM', 'C': 6.562759607882616, 'kernel': 'sigmoid', 'gamma': 'auto'}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:02:05,669] Trial 96 finished with value: 0.750465549348231 and parameters: {'classifier': 'SVM', 'C': 0.129737496319614, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:02:09,487] Trial 97 finished with value: 0.7653631284916201 and parameters: {'classifier': 'RandomForest', 'n_estimators': 262, 'max_depth': 12, 'min_samples_split': 5, 'min_samples_leaf': 3, 'bootstrap': False}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:02:19,520] Trial 98 finished with value: 0.7467411545623835 and parameters: {'classifier': 'GradientBoosting', 'n_estimators': 223, 'learning_rate': 0.03306633013305415, 'max_depth': 20, 'min_samples_split': 3, 'min_samples_leaf': 9}. Best is trial 43 with value: 0.7895716945996275.\n",
      "[I 2025-08-16 22:02:19,618] Trial 99 finished with value: 0.7895716945996275 and parameters: {'classifier': 'SVM', 'C': 0.11321426901560551, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 43 with value: 0.7895716945996275.\n"
     ]
    }
   ],
   "source": [
    "# Create a study and optimize it using CmaEsSampler\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fca7974d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial parameters: {'classifier': 'SVM', 'C': 0.11889704493610485, 'kernel': 'linear', 'gamma': 'scale'}\n",
      "Best trial accuracy: 0.7895716945996275\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the best trial\n",
    "best_trial = study.best_trial\n",
    "print(\"Best trial parameters:\", best_trial.params)\n",
    "print(\"Best trial accuracy:\", best_trial.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e8840287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>value</th>\n",
       "      <th>datetime_start</th>\n",
       "      <th>datetime_complete</th>\n",
       "      <th>duration</th>\n",
       "      <th>params_C</th>\n",
       "      <th>params_bootstrap</th>\n",
       "      <th>params_classifier</th>\n",
       "      <th>params_gamma</th>\n",
       "      <th>params_kernel</th>\n",
       "      <th>params_learning_rate</th>\n",
       "      <th>params_max_depth</th>\n",
       "      <th>params_min_samples_leaf</th>\n",
       "      <th>params_min_samples_split</th>\n",
       "      <th>params_n_estimators</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.769088</td>\n",
       "      <td>2025-08-16 22:00:45.782094</td>\n",
       "      <td>2025-08-16 22:00:47.781680</td>\n",
       "      <td>0 days 00:00:01.999586</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.020516</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.785847</td>\n",
       "      <td>2025-08-16 22:00:47.781680</td>\n",
       "      <td>2025-08-16 22:00:47.841584</td>\n",
       "      <td>0 days 00:00:00.059904</td>\n",
       "      <td>1.929467</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SVM</td>\n",
       "      <td>auto</td>\n",
       "      <td>linear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.737430</td>\n",
       "      <td>2025-08-16 22:00:47.844172</td>\n",
       "      <td>2025-08-16 22:01:01.062618</td>\n",
       "      <td>0 days 00:00:13.218446</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.058741</td>\n",
       "      <td>16.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.661080</td>\n",
       "      <td>2025-08-16 22:01:01.062618</td>\n",
       "      <td>2025-08-16 22:01:26.148774</td>\n",
       "      <td>0 days 00:00:25.086156</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.067525</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.757914</td>\n",
       "      <td>2025-08-16 22:01:26.153423</td>\n",
       "      <td>2025-08-16 22:01:28.109371</td>\n",
       "      <td>0 days 00:00:01.955948</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>0.698324</td>\n",
       "      <td>2025-08-16 22:02:05.455668</td>\n",
       "      <td>2025-08-16 22:02:05.544078</td>\n",
       "      <td>0 days 00:00:00.088410</td>\n",
       "      <td>6.562760</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SVM</td>\n",
       "      <td>auto</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>0.750466</td>\n",
       "      <td>2025-08-16 22:02:05.544078</td>\n",
       "      <td>2025-08-16 22:02:05.669248</td>\n",
       "      <td>0 days 00:00:00.125170</td>\n",
       "      <td>0.129737</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SVM</td>\n",
       "      <td>scale</td>\n",
       "      <td>rbf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>0.765363</td>\n",
       "      <td>2025-08-16 22:02:05.669248</td>\n",
       "      <td>2025-08-16 22:02:09.487725</td>\n",
       "      <td>0 days 00:00:03.818477</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>262.0</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>0.746741</td>\n",
       "      <td>2025-08-16 22:02:09.487725</td>\n",
       "      <td>2025-08-16 22:02:19.520295</td>\n",
       "      <td>0 days 00:00:10.032570</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.033066</td>\n",
       "      <td>20.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>223.0</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>0.789572</td>\n",
       "      <td>2025-08-16 22:02:19.523267</td>\n",
       "      <td>2025-08-16 22:02:19.618073</td>\n",
       "      <td>0 days 00:00:00.094806</td>\n",
       "      <td>0.113214</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SVM</td>\n",
       "      <td>scale</td>\n",
       "      <td>linear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    number     value             datetime_start          datetime_complete  \\\n",
       "0        0  0.769088 2025-08-16 22:00:45.782094 2025-08-16 22:00:47.781680   \n",
       "1        1  0.785847 2025-08-16 22:00:47.781680 2025-08-16 22:00:47.841584   \n",
       "2        2  0.737430 2025-08-16 22:00:47.844172 2025-08-16 22:01:01.062618   \n",
       "3        3  0.661080 2025-08-16 22:01:01.062618 2025-08-16 22:01:26.148774   \n",
       "4        4  0.757914 2025-08-16 22:01:26.153423 2025-08-16 22:01:28.109371   \n",
       "..     ...       ...                        ...                        ...   \n",
       "95      95  0.698324 2025-08-16 22:02:05.455668 2025-08-16 22:02:05.544078   \n",
       "96      96  0.750466 2025-08-16 22:02:05.544078 2025-08-16 22:02:05.669248   \n",
       "97      97  0.765363 2025-08-16 22:02:05.669248 2025-08-16 22:02:09.487725   \n",
       "98      98  0.746741 2025-08-16 22:02:09.487725 2025-08-16 22:02:19.520295   \n",
       "99      99  0.789572 2025-08-16 22:02:19.523267 2025-08-16 22:02:19.618073   \n",
       "\n",
       "                 duration  params_C params_bootstrap params_classifier  \\\n",
       "0  0 days 00:00:01.999586       NaN              NaN  GradientBoosting   \n",
       "1  0 days 00:00:00.059904  1.929467              NaN               SVM   \n",
       "2  0 days 00:00:13.218446       NaN              NaN  GradientBoosting   \n",
       "3  0 days 00:00:25.086156       NaN              NaN  GradientBoosting   \n",
       "4  0 days 00:00:01.955948       NaN             True      RandomForest   \n",
       "..                    ...       ...              ...               ...   \n",
       "95 0 days 00:00:00.088410  6.562760              NaN               SVM   \n",
       "96 0 days 00:00:00.125170  0.129737              NaN               SVM   \n",
       "97 0 days 00:00:03.818477       NaN            False      RandomForest   \n",
       "98 0 days 00:00:10.032570       NaN              NaN  GradientBoosting   \n",
       "99 0 days 00:00:00.094806  0.113214              NaN               SVM   \n",
       "\n",
       "   params_gamma params_kernel  params_learning_rate  params_max_depth  \\\n",
       "0           NaN           NaN              0.020516              14.0   \n",
       "1          auto        linear                   NaN               NaN   \n",
       "2           NaN           NaN              0.058741              16.0   \n",
       "3           NaN           NaN              0.067525              20.0   \n",
       "4           NaN           NaN                   NaN              11.0   \n",
       "..          ...           ...                   ...               ...   \n",
       "95         auto       sigmoid                   NaN               NaN   \n",
       "96        scale           rbf                   NaN               NaN   \n",
       "97          NaN           NaN                   NaN              12.0   \n",
       "98          NaN           NaN              0.033066              20.0   \n",
       "99        scale        linear                   NaN               NaN   \n",
       "\n",
       "    params_min_samples_leaf  params_min_samples_split  params_n_estimators  \\\n",
       "0                      10.0                       9.0                 96.0   \n",
       "1                       NaN                       NaN                  NaN   \n",
       "2                       8.0                       5.0                256.0   \n",
       "3                       1.0                       4.0                202.0   \n",
       "4                       6.0                       9.0                 96.0   \n",
       "..                      ...                       ...                  ...   \n",
       "95                      NaN                       NaN                  NaN   \n",
       "96                      NaN                       NaN                  NaN   \n",
       "97                      3.0                       5.0                262.0   \n",
       "98                      9.0                       3.0                223.0   \n",
       "99                      NaN                       NaN                  NaN   \n",
       "\n",
       "       state  \n",
       "0   COMPLETE  \n",
       "1   COMPLETE  \n",
       "2   COMPLETE  \n",
       "3   COMPLETE  \n",
       "4   COMPLETE  \n",
       "..       ...  \n",
       "95  COMPLETE  \n",
       "96  COMPLETE  \n",
       "97  COMPLETE  \n",
       "98  COMPLETE  \n",
       "99  COMPLETE  \n",
       "\n",
       "[100 rows x 16 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.trials_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f371625f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "params_classifier\n",
       "SVM                 78\n",
       "GradientBoosting    12\n",
       "RandomForest        10\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.trials_dataframe()['params_classifier'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "93414f7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "params_classifier\n",
       "GradientBoosting    0.739137\n",
       "RandomForest        0.766853\n",
       "SVM                 0.776035\n",
       "Name: value, dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.trials_dataframe().groupby('params_classifier')['value'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f8ac9dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna[visualization] in d:\\optuna\\env\\lib\\site-packages (4.4.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in d:\\optuna\\env\\lib\\site-packages (from optuna[visualization]) (1.16.4)\n",
      "Requirement already satisfied: colorlog in d:\\optuna\\env\\lib\\site-packages (from optuna[visualization]) (6.9.0)\n",
      "Requirement already satisfied: numpy in d:\\optuna\\env\\lib\\site-packages (from optuna[visualization]) (2.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\optuna\\env\\lib\\site-packages (from optuna[visualization]) (25.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in d:\\optuna\\env\\lib\\site-packages (from optuna[visualization]) (2.0.43)\n",
      "Requirement already satisfied: tqdm in d:\\optuna\\env\\lib\\site-packages (from optuna[visualization]) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in d:\\optuna\\env\\lib\\site-packages (from optuna[visualization]) (6.0.2)\n",
      "Requirement already satisfied: Mako in d:\\optuna\\env\\lib\\site-packages (from alembic>=1.5.0->optuna[visualization]) (1.3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in d:\\optuna\\env\\lib\\site-packages (from alembic>=1.5.0->optuna[visualization]) (4.14.1)\n",
      "Requirement already satisfied: greenlet>=1 in d:\\optuna\\env\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna[visualization]) (3.2.4)\n",
      "Requirement already satisfied: colorama in d:\\optuna\\env\\lib\\site-packages (from colorlog->optuna[visualization]) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in d:\\optuna\\env\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna[visualization]) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: optuna 4.4.0 does not provide the extra 'visualization'\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install optuna[visualization]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2aeb29bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly in d:\\optuna\\env\\lib\\site-packages (6.3.0)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in d:\\optuna\\env\\lib\\site-packages (from plotly) (2.1.2)\n",
      "Requirement already satisfied: packaging in d:\\optuna\\env\\lib\\site-packages (from plotly) (25.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install plotly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d1e191c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.3.0\n"
     ]
    }
   ],
   "source": [
    "import plotly\n",
    "print(plotly.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9518919f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Optuna\\env\\Lib\\site-packages\\optuna\\visualization\\_plotly_imports.py:7\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m try_import() \u001b[38;5;28;01mas\u001b[39;00m _imports:\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m plotly_version\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'plotly'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 1. Optimization History\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptuna\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvisualization\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m plot_optimization_history\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mplot_optimization_history\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m)\u001b[49m.show()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Optuna\\env\\Lib\\site-packages\\optuna\\visualization\\_optimization_history.py:200\u001b[39m, in \u001b[36mplot_optimization_history\u001b[39m\u001b[34m(study, target, target_name, error_bar)\u001b[39m\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplot_optimization_history\u001b[39m(\n\u001b[32m    173\u001b[39m     study: Study | Sequence[Study],\n\u001b[32m    174\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    177\u001b[39m     error_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    178\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mgo.Figure\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    179\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Plot optimization history of all trials in a study.\u001b[39;00m\n\u001b[32m    180\u001b[39m \n\u001b[32m    181\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    197\u001b[39m \u001b[33;03m        A :class:`plotly.graph_objects.Figure` object.\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m     \u001b[43m_imports\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m     info_list = _get_optimization_history_info_list(study, target, target_name, error_bar)\n\u001b[32m    203\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _get_optimization_history_plot(info_list, target_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Optuna\\env\\Lib\\site-packages\\optuna\\_imports.py:94\u001b[39m, in \u001b[36m_DeferredImportExceptionContextManager.check\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._deferred \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     93\u001b[39m     exc_value, message = \u001b[38;5;28mself\u001b[39m._deferred\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc_value\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'."
     ]
    }
   ],
   "source": [
    "# 1. Optimization History\n",
    "from optuna.visualization import plot_optimization_history\n",
    "plot_optimization_history(study).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7d681ee8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Optuna\\env\\Lib\\site-packages\\optuna\\visualization\\_plotly_imports.py:7\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m try_import() \u001b[38;5;28;01mas\u001b[39;00m _imports:\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m plotly_version\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'plotly'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 3. Slice Plot\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mplot_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m)\u001b[49m.show()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Optuna\\env\\Lib\\site-packages\\optuna\\visualization\\_slice.py:172\u001b[39m, in \u001b[36mplot_slice\u001b[39m\u001b[34m(study, params, target, target_name)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplot_slice\u001b[39m(\n\u001b[32m    144\u001b[39m     study: Study,\n\u001b[32m    145\u001b[39m     params: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    148\u001b[39m     target_name: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mObjective Value\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    149\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mgo.Figure\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    150\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Plot the parameter relationship as slice plot in a study.\u001b[39;00m\n\u001b[32m    151\u001b[39m \n\u001b[32m    152\u001b[39m \u001b[33;03m    Note that, if a parameter contains missing values, a trial with missing values is not plotted.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    169\u001b[39m \u001b[33;03m        A :class:`plotly.graph_objects.Figure` object.\u001b[39;00m\n\u001b[32m    170\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m     \u001b[43m_imports\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _get_slice_plot(_get_slice_plot_info(study, params, target, target_name))\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Optuna\\env\\Lib\\site-packages\\optuna\\_imports.py:94\u001b[39m, in \u001b[36m_DeferredImportExceptionContextManager.check\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._deferred \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     93\u001b[39m     exc_value, message = \u001b[38;5;28mself\u001b[39m._deferred\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc_value\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'."
     ]
    }
   ],
   "source": [
    "# 3. Slice Plot\n",
    "plot_slice(study).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b8b5d982",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Optuna\\env\\Lib\\site-packages\\optuna\\visualization\\_plotly_imports.py:7\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m try_import() \u001b[38;5;28;01mas\u001b[39;00m _imports:\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m plotly_version\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'plotly'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 5. Hyperparameter Importance\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mplot_param_importances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m)\u001b[49m.show()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Optuna\\env\\Lib\\site-packages\\optuna\\visualization\\_param_importances.py:167\u001b[39m, in \u001b[36mplot_param_importances\u001b[39m\u001b[34m(study, evaluator, params, target, target_name)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplot_param_importances\u001b[39m(\n\u001b[32m    114\u001b[39m     study: Study,\n\u001b[32m    115\u001b[39m     evaluator: BaseImportanceEvaluator | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    119\u001b[39m     target_name: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mObjective Value\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    120\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mgo.Figure\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    121\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Plot hyperparameter importances.\u001b[39;00m\n\u001b[32m    122\u001b[39m \n\u001b[32m    123\u001b[39m \u001b[33;03m    .. seealso::\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    164\u001b[39m \u001b[33;03m        A :class:`plotly.graph_objects.Figure` object.\u001b[39;00m\n\u001b[32m    165\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[43m_imports\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    168\u001b[39m     importances_infos = _get_importances_infos(study, evaluator, params, target, target_name)\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _get_importances_plot(importances_infos, study)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Optuna\\env\\Lib\\site-packages\\optuna\\_imports.py:94\u001b[39m, in \u001b[36m_DeferredImportExceptionContextManager.check\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._deferred \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     93\u001b[39m     exc_value, message = \u001b[38;5;28mself\u001b[39m._deferred\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc_value\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'."
     ]
    }
   ],
   "source": [
    "# 5. Hyperparameter Importance\n",
    "plot_param_importances(study).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "21839f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:47,863] A new study created in memory with name: no-name-541da5c9-06f0-4902-993c-10aa7f1b7e1c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.81681\teval-mlogloss:0.80563\n",
      "[1]\ttrain-mlogloss:0.71075\teval-mlogloss:0.69364\n",
      "[2]\ttrain-mlogloss:0.57124\teval-mlogloss:0.54449\n",
      "[3]\ttrain-mlogloss:0.45411\teval-mlogloss:0.41671\n",
      "[4]\ttrain-mlogloss:0.37084\teval-mlogloss:0.33212\n",
      "[5]\ttrain-mlogloss:0.30803\teval-mlogloss:0.26361\n",
      "[6]\ttrain-mlogloss:0.26238\teval-mlogloss:0.20851\n",
      "[7]\ttrain-mlogloss:0.23028\teval-mlogloss:0.17089\n",
      "[8]\ttrain-mlogloss:0.20750\teval-mlogloss:0.14298\n",
      "[9]\ttrain-mlogloss:0.19483\teval-mlogloss:0.13404\n",
      "[10]\ttrain-mlogloss:0.19130\teval-mlogloss:0.13363\n",
      "[11]\ttrain-mlogloss:0.18484\teval-mlogloss:0.12604\n",
      "[12]\ttrain-mlogloss:0.18347\teval-mlogloss:0.12587\n",
      "[13]\ttrain-mlogloss:0.17866\teval-mlogloss:0.11895\n",
      "[14]\ttrain-mlogloss:0.17848\teval-mlogloss:0.11850\n",
      "[15]\ttrain-mlogloss:0.17820\teval-mlogloss:0.11799\n",
      "[16]\ttrain-mlogloss:0.17720\teval-mlogloss:0.11728\n",
      "[17]\ttrain-mlogloss:0.17705\teval-mlogloss:0.11734\n",
      "[18]\ttrain-mlogloss:0.17715\teval-mlogloss:0.11701\n",
      "[19]\ttrain-mlogloss:0.17027\teval-mlogloss:0.10959\n",
      "[20]\ttrain-mlogloss:0.17021\teval-mlogloss:0.10958\n",
      "[21]\ttrain-mlogloss:0.17021\teval-mlogloss:0.10923\n",
      "[22]\ttrain-mlogloss:0.16908\teval-mlogloss:0.10744\n",
      "[23]\ttrain-mlogloss:0.16458\teval-mlogloss:0.09933\n",
      "[24]\ttrain-mlogloss:0.16445\teval-mlogloss:0.09848\n",
      "[25]\ttrain-mlogloss:0.16012\teval-mlogloss:0.09203\n",
      "[26]\ttrain-mlogloss:0.15932\teval-mlogloss:0.09127\n",
      "[27]\ttrain-mlogloss:0.15966\teval-mlogloss:0.09262\n",
      "[28]\ttrain-mlogloss:0.15954\teval-mlogloss:0.09248\n",
      "[29]\ttrain-mlogloss:0.15892\teval-mlogloss:0.09249\n",
      "[30]\ttrain-mlogloss:0.15855\teval-mlogloss:0.09160\n",
      "[31]\ttrain-mlogloss:0.15861\teval-mlogloss:0.09171\n",
      "[32]\ttrain-mlogloss:0.15845\teval-mlogloss:0.09123\n",
      "[33]\ttrain-mlogloss:0.15790\teval-mlogloss:0.09108\n",
      "[34]\ttrain-mlogloss:0.15743\teval-mlogloss:0.08962\n",
      "[35]\ttrain-mlogloss:0.15742\teval-mlogloss:0.08985\n",
      "[36]\ttrain-mlogloss:0.15741\teval-mlogloss:0.08986\n",
      "[37]\ttrain-mlogloss:0.15694\teval-mlogloss:0.09031\n",
      "[38]\ttrain-mlogloss:0.15692\teval-mlogloss:0.09037\n",
      "[39]\ttrain-mlogloss:0.15659\teval-mlogloss:0.08976\n",
      "[40]\ttrain-mlogloss:0.15624\teval-mlogloss:0.08940\n",
      "[41]\ttrain-mlogloss:0.15590\teval-mlogloss:0.08888\n",
      "[42]\ttrain-mlogloss:0.15584\teval-mlogloss:0.08894\n",
      "[43]\ttrain-mlogloss:0.15606\teval-mlogloss:0.08906\n",
      "[44]\ttrain-mlogloss:0.15596\teval-mlogloss:0.08919\n",
      "[45]\ttrain-mlogloss:0.15538\teval-mlogloss:0.08860\n",
      "[46]\ttrain-mlogloss:0.15488\teval-mlogloss:0.08731\n",
      "[47]\ttrain-mlogloss:0.15477\teval-mlogloss:0.08629\n",
      "[48]\ttrain-mlogloss:0.15475\teval-mlogloss:0.08540\n",
      "[49]\ttrain-mlogloss:0.15454\teval-mlogloss:0.08503\n",
      "[50]\ttrain-mlogloss:0.15457\teval-mlogloss:0.08493\n",
      "[51]\ttrain-mlogloss:0.15448\teval-mlogloss:0.08525\n",
      "[52]\ttrain-mlogloss:0.15411\teval-mlogloss:0.08531\n",
      "[53]\ttrain-mlogloss:0.15422\teval-mlogloss:0.08541\n",
      "[54]\ttrain-mlogloss:0.15383\teval-mlogloss:0.08506\n",
      "[55]\ttrain-mlogloss:0.15382\teval-mlogloss:0.08442\n",
      "[56]\ttrain-mlogloss:0.15353\teval-mlogloss:0.08433\n",
      "[57]\ttrain-mlogloss:0.15325\teval-mlogloss:0.08370\n",
      "[58]\ttrain-mlogloss:0.15334\teval-mlogloss:0.08371\n",
      "[59]\ttrain-mlogloss:0.15288\teval-mlogloss:0.08376\n",
      "[60]\ttrain-mlogloss:0.15268\teval-mlogloss:0.08403\n",
      "[61]\ttrain-mlogloss:0.15275\teval-mlogloss:0.08426\n",
      "[62]\ttrain-mlogloss:0.15271\teval-mlogloss:0.08443\n",
      "[63]\ttrain-mlogloss:0.15256\teval-mlogloss:0.08390\n",
      "[64]\ttrain-mlogloss:0.15247\teval-mlogloss:0.08308\n",
      "[65]\ttrain-mlogloss:0.15271\teval-mlogloss:0.08362\n",
      "[66]\ttrain-mlogloss:0.15195\teval-mlogloss:0.08212\n",
      "[67]\ttrain-mlogloss:0.15166\teval-mlogloss:0.08208\n",
      "[68]\ttrain-mlogloss:0.15166\teval-mlogloss:0.08206\n",
      "[69]\ttrain-mlogloss:0.15150\teval-mlogloss:0.08160\n",
      "[70]\ttrain-mlogloss:0.15158\teval-mlogloss:0.08150\n",
      "[71]\ttrain-mlogloss:0.15159\teval-mlogloss:0.08149\n",
      "[72]\ttrain-mlogloss:0.15162\teval-mlogloss:0.08133\n",
      "[73]\ttrain-mlogloss:0.15162\teval-mlogloss:0.08104\n",
      "[74]\ttrain-mlogloss:0.15160\teval-mlogloss:0.08123\n",
      "[75]\ttrain-mlogloss:0.15162\teval-mlogloss:0.08129\n",
      "[76]\ttrain-mlogloss:0.15184\teval-mlogloss:0.08174\n",
      "[77]\ttrain-mlogloss:0.15164\teval-mlogloss:0.08164\n",
      "[78]\ttrain-mlogloss:0.15192\teval-mlogloss:0.08230\n",
      "[79]\ttrain-mlogloss:0.15183\teval-mlogloss:0.08227\n",
      "[80]\ttrain-mlogloss:0.15172\teval-mlogloss:0.08225\n",
      "[81]\ttrain-mlogloss:0.15172\teval-mlogloss:0.08257\n",
      "[82]\ttrain-mlogloss:0.15179\teval-mlogloss:0.08281\n",
      "[83]\ttrain-mlogloss:0.15188\teval-mlogloss:0.08284\n",
      "[84]\ttrain-mlogloss:0.15202\teval-mlogloss:0.08276\n",
      "[85]\ttrain-mlogloss:0.15179\teval-mlogloss:0.08246\n",
      "[86]\ttrain-mlogloss:0.15178\teval-mlogloss:0.08253\n",
      "[87]\ttrain-mlogloss:0.15164\teval-mlogloss:0.08253\n",
      "[88]\ttrain-mlogloss:0.15131\teval-mlogloss:0.08230\n",
      "[89]\ttrain-mlogloss:0.15121\teval-mlogloss:0.08210\n",
      "[90]\ttrain-mlogloss:0.15123\teval-mlogloss:0.08204\n",
      "[91]\ttrain-mlogloss:0.15127\teval-mlogloss:0.08222\n",
      "[92]\ttrain-mlogloss:0.15117\teval-mlogloss:0.08217\n",
      "[93]\ttrain-mlogloss:0.15102\teval-mlogloss:0.08199\n",
      "[94]\ttrain-mlogloss:0.15103\teval-mlogloss:0.08204\n",
      "[95]\ttrain-mlogloss:0.15128\teval-mlogloss:0.08245\n",
      "[96]\ttrain-mlogloss:0.15139\teval-mlogloss:0.08266\n",
      "[97]\ttrain-mlogloss:0.15114\teval-mlogloss:0.08217\n",
      "[98]\ttrain-mlogloss:0.15164\teval-mlogloss:0.08353\n",
      "[99]\ttrain-mlogloss:0.15138\teval-mlogloss:0.08307\n",
      "[100]\ttrain-mlogloss:0.15121\teval-mlogloss:0.08251\n",
      "[101]\ttrain-mlogloss:0.15098\teval-mlogloss:0.08261\n",
      "[102]\ttrain-mlogloss:0.15096\teval-mlogloss:0.08279\n",
      "[103]\ttrain-mlogloss:0.15084\teval-mlogloss:0.08177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:49,959] Trial 0 finished with value: 1.0 and parameters: {'lambda': 0.00014405154340233053, 'alpha': 1.0028087182541891e-05, 'eta': 0.24399632761008344, 'gamma': 3.146022815330369e-05, 'max_depth': 6, 'min_child_weight': 5, 'subsample': 0.5976578373605221, 'colsample_bytree': 0.6061634604717767}. Best is trial 0 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.98739\teval-mlogloss:0.98306\n",
      "[1]\ttrain-mlogloss:0.89280\teval-mlogloss:0.88517\n",
      "[2]\ttrain-mlogloss:0.80992\teval-mlogloss:0.79829\n",
      "[3]\ttrain-mlogloss:0.73773\teval-mlogloss:0.72202\n",
      "[4]\ttrain-mlogloss:0.67499\teval-mlogloss:0.65742\n",
      "[5]\ttrain-mlogloss:0.61922\teval-mlogloss:0.59873\n",
      "[6]\ttrain-mlogloss:0.57064\teval-mlogloss:0.54661\n",
      "[7]\ttrain-mlogloss:0.52641\teval-mlogloss:0.50025\n",
      "[8]\ttrain-mlogloss:0.48771\teval-mlogloss:0.45926\n",
      "[9]\ttrain-mlogloss:0.45182\teval-mlogloss:0.42107\n",
      "[10]\ttrain-mlogloss:0.41901\teval-mlogloss:0.39035\n",
      "[11]\ttrain-mlogloss:0.38951\teval-mlogloss:0.35923\n",
      "[12]\ttrain-mlogloss:0.36330\teval-mlogloss:0.33190\n",
      "[13]\ttrain-mlogloss:0.33800\teval-mlogloss:0.30601\n",
      "[14]\ttrain-mlogloss:0.31605\teval-mlogloss:0.28388\n",
      "[15]\ttrain-mlogloss:0.29632\teval-mlogloss:0.26322\n",
      "[16]\ttrain-mlogloss:0.27849\teval-mlogloss:0.24457\n",
      "[17]\ttrain-mlogloss:0.26220\teval-mlogloss:0.22691\n",
      "[18]\ttrain-mlogloss:0.24727\teval-mlogloss:0.21145\n",
      "[19]\ttrain-mlogloss:0.23307\teval-mlogloss:0.19776\n",
      "[20]\ttrain-mlogloss:0.21974\teval-mlogloss:0.18581\n",
      "[21]\ttrain-mlogloss:0.20725\teval-mlogloss:0.17484\n",
      "[22]\ttrain-mlogloss:0.19637\teval-mlogloss:0.16362\n",
      "[23]\ttrain-mlogloss:0.18523\teval-mlogloss:0.15431\n",
      "[24]\ttrain-mlogloss:0.17640\teval-mlogloss:0.14456\n",
      "[25]\ttrain-mlogloss:0.16767\teval-mlogloss:0.13514\n",
      "[26]\ttrain-mlogloss:0.16027\teval-mlogloss:0.12690\n",
      "[27]\ttrain-mlogloss:0.15337\teval-mlogloss:0.11938\n",
      "[28]\ttrain-mlogloss:0.14669\teval-mlogloss:0.11208\n",
      "[29]\ttrain-mlogloss:0.14156\teval-mlogloss:0.10592\n",
      "[30]\ttrain-mlogloss:0.13627\teval-mlogloss:0.10062\n",
      "[31]\ttrain-mlogloss:0.13146\teval-mlogloss:0.09539\n",
      "[32]\ttrain-mlogloss:0.12619\teval-mlogloss:0.09153\n",
      "[33]\ttrain-mlogloss:0.12150\teval-mlogloss:0.08785\n",
      "[34]\ttrain-mlogloss:0.11758\teval-mlogloss:0.08401\n",
      "[35]\ttrain-mlogloss:0.11436\teval-mlogloss:0.08111\n",
      "[36]\ttrain-mlogloss:0.11098\teval-mlogloss:0.07726\n",
      "[37]\ttrain-mlogloss:0.10721\teval-mlogloss:0.07460\n",
      "[38]\ttrain-mlogloss:0.10450\teval-mlogloss:0.07184\n",
      "[39]\ttrain-mlogloss:0.10162\teval-mlogloss:0.06984\n",
      "[40]\ttrain-mlogloss:0.09927\teval-mlogloss:0.06791\n",
      "[41]\ttrain-mlogloss:0.09816\teval-mlogloss:0.06632\n",
      "[42]\ttrain-mlogloss:0.09607\teval-mlogloss:0.06467\n",
      "[43]\ttrain-mlogloss:0.09517\teval-mlogloss:0.06332\n",
      "[44]\ttrain-mlogloss:0.09379\teval-mlogloss:0.06152\n",
      "[45]\ttrain-mlogloss:0.09264\teval-mlogloss:0.05978\n",
      "[46]\ttrain-mlogloss:0.09186\teval-mlogloss:0.05839\n",
      "[47]\ttrain-mlogloss:0.09109\teval-mlogloss:0.05728\n",
      "[48]\ttrain-mlogloss:0.08974\teval-mlogloss:0.05698\n",
      "[49]\ttrain-mlogloss:0.08895\teval-mlogloss:0.05535\n",
      "[50]\ttrain-mlogloss:0.08844\teval-mlogloss:0.05420\n",
      "[51]\ttrain-mlogloss:0.08776\teval-mlogloss:0.05361\n",
      "[52]\ttrain-mlogloss:0.08723\teval-mlogloss:0.05301\n",
      "[53]\ttrain-mlogloss:0.08680\teval-mlogloss:0.05221\n",
      "[54]\ttrain-mlogloss:0.08614\teval-mlogloss:0.05147\n",
      "[55]\ttrain-mlogloss:0.08564\teval-mlogloss:0.05133\n",
      "[56]\ttrain-mlogloss:0.08515\teval-mlogloss:0.05116\n",
      "[57]\ttrain-mlogloss:0.08476\teval-mlogloss:0.05084\n",
      "[58]\ttrain-mlogloss:0.08423\teval-mlogloss:0.05036\n",
      "[59]\ttrain-mlogloss:0.08355\teval-mlogloss:0.04945\n",
      "[60]\ttrain-mlogloss:0.08317\teval-mlogloss:0.04848\n",
      "[61]\ttrain-mlogloss:0.08277\teval-mlogloss:0.04779\n",
      "[62]\ttrain-mlogloss:0.08229\teval-mlogloss:0.04701\n",
      "[63]\ttrain-mlogloss:0.08205\teval-mlogloss:0.04681\n",
      "[64]\ttrain-mlogloss:0.08177\teval-mlogloss:0.04704\n",
      "[65]\ttrain-mlogloss:0.08151\teval-mlogloss:0.04670\n",
      "[66]\ttrain-mlogloss:0.08093\teval-mlogloss:0.04591\n",
      "[67]\ttrain-mlogloss:0.08065\teval-mlogloss:0.04542\n",
      "[68]\ttrain-mlogloss:0.08030\teval-mlogloss:0.04586\n",
      "[69]\ttrain-mlogloss:0.07988\teval-mlogloss:0.04529\n",
      "[70]\ttrain-mlogloss:0.07964\teval-mlogloss:0.04487\n",
      "[71]\ttrain-mlogloss:0.07924\teval-mlogloss:0.04502\n",
      "[72]\ttrain-mlogloss:0.07893\teval-mlogloss:0.04544\n",
      "[73]\ttrain-mlogloss:0.07855\teval-mlogloss:0.04491\n",
      "[74]\ttrain-mlogloss:0.07820\teval-mlogloss:0.04433\n",
      "[75]\ttrain-mlogloss:0.07788\teval-mlogloss:0.04398\n",
      "[76]\ttrain-mlogloss:0.07765\teval-mlogloss:0.04382\n",
      "[77]\ttrain-mlogloss:0.07731\teval-mlogloss:0.04320\n",
      "[78]\ttrain-mlogloss:0.07710\teval-mlogloss:0.04287\n",
      "[79]\ttrain-mlogloss:0.07691\teval-mlogloss:0.04289\n",
      "[80]\ttrain-mlogloss:0.07667\teval-mlogloss:0.04327\n",
      "[81]\ttrain-mlogloss:0.07638\teval-mlogloss:0.04278\n",
      "[82]\ttrain-mlogloss:0.07619\teval-mlogloss:0.04232\n",
      "[83]\ttrain-mlogloss:0.07600\teval-mlogloss:0.04193\n",
      "[84]\ttrain-mlogloss:0.07581\teval-mlogloss:0.04183\n",
      "[85]\ttrain-mlogloss:0.07564\teval-mlogloss:0.04197\n",
      "[86]\ttrain-mlogloss:0.07545\teval-mlogloss:0.04139\n",
      "[87]\ttrain-mlogloss:0.07529\teval-mlogloss:0.04148\n",
      "[88]\ttrain-mlogloss:0.07512\teval-mlogloss:0.04115\n",
      "[89]\ttrain-mlogloss:0.07495\teval-mlogloss:0.04135\n",
      "[90]\ttrain-mlogloss:0.07480\teval-mlogloss:0.04148\n",
      "[91]\ttrain-mlogloss:0.07458\teval-mlogloss:0.04165\n",
      "[92]\ttrain-mlogloss:0.07438\teval-mlogloss:0.04143\n",
      "[93]\ttrain-mlogloss:0.07420\teval-mlogloss:0.04108\n",
      "[94]\ttrain-mlogloss:0.07404\teval-mlogloss:0.04063\n",
      "[95]\ttrain-mlogloss:0.07386\teval-mlogloss:0.04085\n",
      "[96]\ttrain-mlogloss:0.07376\teval-mlogloss:0.04045\n",
      "[97]\ttrain-mlogloss:0.07364\teval-mlogloss:0.04053\n",
      "[98]\ttrain-mlogloss:0.07349\teval-mlogloss:0.04047\n",
      "[99]\ttrain-mlogloss:0.07331\teval-mlogloss:0.04047\n",
      "[100]\ttrain-mlogloss:0.07318\teval-mlogloss:0.04066\n",
      "[101]\ttrain-mlogloss:0.07301\teval-mlogloss:0.04040\n",
      "[102]\ttrain-mlogloss:0.07287\teval-mlogloss:0.04003\n",
      "[103]\ttrain-mlogloss:0.07271\teval-mlogloss:0.04004\n",
      "[104]\ttrain-mlogloss:0.07258\teval-mlogloss:0.04015\n",
      "[105]\ttrain-mlogloss:0.07237\teval-mlogloss:0.03962\n",
      "[106]\ttrain-mlogloss:0.07223\teval-mlogloss:0.03957\n",
      "[107]\ttrain-mlogloss:0.07211\teval-mlogloss:0.03976\n",
      "[108]\ttrain-mlogloss:0.07197\teval-mlogloss:0.03948\n",
      "[109]\ttrain-mlogloss:0.07183\teval-mlogloss:0.03909\n",
      "[110]\ttrain-mlogloss:0.07169\teval-mlogloss:0.03893\n",
      "[111]\ttrain-mlogloss:0.07157\teval-mlogloss:0.03888\n",
      "[112]\ttrain-mlogloss:0.07142\teval-mlogloss:0.03897\n",
      "[113]\ttrain-mlogloss:0.07129\teval-mlogloss:0.03907\n",
      "[114]\ttrain-mlogloss:0.07117\teval-mlogloss:0.03925\n",
      "[115]\ttrain-mlogloss:0.07106\teval-mlogloss:0.03943\n",
      "[116]\ttrain-mlogloss:0.07093\teval-mlogloss:0.03921\n",
      "[117]\ttrain-mlogloss:0.07082\teval-mlogloss:0.03917\n",
      "[118]\ttrain-mlogloss:0.07070\teval-mlogloss:0.03883\n",
      "[119]\ttrain-mlogloss:0.07059\teval-mlogloss:0.03894\n",
      "[120]\ttrain-mlogloss:0.07049\teval-mlogloss:0.03914\n",
      "[121]\ttrain-mlogloss:0.07038\teval-mlogloss:0.03880\n",
      "[122]\ttrain-mlogloss:0.07022\teval-mlogloss:0.03828\n",
      "[123]\ttrain-mlogloss:0.07012\teval-mlogloss:0.03839\n",
      "[124]\ttrain-mlogloss:0.07001\teval-mlogloss:0.03827\n",
      "[125]\ttrain-mlogloss:0.06990\teval-mlogloss:0.03845\n",
      "[126]\ttrain-mlogloss:0.06980\teval-mlogloss:0.03834\n",
      "[127]\ttrain-mlogloss:0.06971\teval-mlogloss:0.03850\n",
      "[128]\ttrain-mlogloss:0.06960\teval-mlogloss:0.03824\n",
      "[129]\ttrain-mlogloss:0.06950\teval-mlogloss:0.03821\n",
      "[130]\ttrain-mlogloss:0.06935\teval-mlogloss:0.03813\n",
      "[131]\ttrain-mlogloss:0.06924\teval-mlogloss:0.03777\n",
      "[132]\ttrain-mlogloss:0.06912\teval-mlogloss:0.03798\n",
      "[133]\ttrain-mlogloss:0.06903\teval-mlogloss:0.03786\n",
      "[134]\ttrain-mlogloss:0.06894\teval-mlogloss:0.03801\n",
      "[135]\ttrain-mlogloss:0.06885\teval-mlogloss:0.03797\n",
      "[136]\ttrain-mlogloss:0.06876\teval-mlogloss:0.03786\n",
      "[137]\ttrain-mlogloss:0.06866\teval-mlogloss:0.03796\n",
      "[138]\ttrain-mlogloss:0.06856\teval-mlogloss:0.03813\n",
      "[139]\ttrain-mlogloss:0.06855\teval-mlogloss:0.03820\n",
      "[140]\ttrain-mlogloss:0.06847\teval-mlogloss:0.03834\n",
      "[141]\ttrain-mlogloss:0.06838\teval-mlogloss:0.03802\n",
      "[142]\ttrain-mlogloss:0.06826\teval-mlogloss:0.03793\n",
      "[143]\ttrain-mlogloss:0.06816\teval-mlogloss:0.03801\n",
      "[144]\ttrain-mlogloss:0.06807\teval-mlogloss:0.03771\n",
      "[145]\ttrain-mlogloss:0.06798\teval-mlogloss:0.03789\n",
      "[146]\ttrain-mlogloss:0.06789\teval-mlogloss:0.03762\n",
      "[147]\ttrain-mlogloss:0.06781\teval-mlogloss:0.03779\n",
      "[148]\ttrain-mlogloss:0.06773\teval-mlogloss:0.03751\n",
      "[149]\ttrain-mlogloss:0.06763\teval-mlogloss:0.03753\n",
      "[150]\ttrain-mlogloss:0.06754\teval-mlogloss:0.03762\n",
      "[151]\ttrain-mlogloss:0.06742\teval-mlogloss:0.03750\n",
      "[152]\ttrain-mlogloss:0.06733\teval-mlogloss:0.03732\n",
      "[153]\ttrain-mlogloss:0.06724\teval-mlogloss:0.03734\n",
      "[154]\ttrain-mlogloss:0.06715\teval-mlogloss:0.03707\n",
      "[155]\ttrain-mlogloss:0.06707\teval-mlogloss:0.03725\n",
      "[156]\ttrain-mlogloss:0.06699\teval-mlogloss:0.03737\n",
      "[157]\ttrain-mlogloss:0.06697\teval-mlogloss:0.03722\n",
      "[158]\ttrain-mlogloss:0.06689\teval-mlogloss:0.03741\n",
      "[159]\ttrain-mlogloss:0.06680\teval-mlogloss:0.03750\n",
      "[160]\ttrain-mlogloss:0.06673\teval-mlogloss:0.03733\n",
      "[161]\ttrain-mlogloss:0.06663\teval-mlogloss:0.03731\n",
      "[162]\ttrain-mlogloss:0.06656\teval-mlogloss:0.03748\n",
      "[163]\ttrain-mlogloss:0.06649\teval-mlogloss:0.03739\n",
      "[164]\ttrain-mlogloss:0.06643\teval-mlogloss:0.03722\n",
      "[165]\ttrain-mlogloss:0.06636\teval-mlogloss:0.03738\n",
      "[166]\ttrain-mlogloss:0.06628\teval-mlogloss:0.03716\n",
      "[167]\ttrain-mlogloss:0.06623\teval-mlogloss:0.03739\n",
      "[168]\ttrain-mlogloss:0.06616\teval-mlogloss:0.03746\n",
      "[169]\ttrain-mlogloss:0.06610\teval-mlogloss:0.03745\n",
      "[170]\ttrain-mlogloss:0.06602\teval-mlogloss:0.03731\n",
      "[171]\ttrain-mlogloss:0.06596\teval-mlogloss:0.03714\n",
      "[172]\ttrain-mlogloss:0.06587\teval-mlogloss:0.03684\n",
      "[173]\ttrain-mlogloss:0.06581\teval-mlogloss:0.03679\n",
      "[174]\ttrain-mlogloss:0.06573\teval-mlogloss:0.03661\n",
      "[175]\ttrain-mlogloss:0.06566\teval-mlogloss:0.03677\n",
      "[176]\ttrain-mlogloss:0.06560\teval-mlogloss:0.03692\n",
      "[177]\ttrain-mlogloss:0.06552\teval-mlogloss:0.03666\n",
      "[178]\ttrain-mlogloss:0.06546\teval-mlogloss:0.03681\n",
      "[179]\ttrain-mlogloss:0.06540\teval-mlogloss:0.03684\n",
      "[180]\ttrain-mlogloss:0.06534\teval-mlogloss:0.03676\n",
      "[181]\ttrain-mlogloss:0.06528\teval-mlogloss:0.03686\n",
      "[182]\ttrain-mlogloss:0.06521\teval-mlogloss:0.03658\n",
      "[183]\ttrain-mlogloss:0.06516\teval-mlogloss:0.03641\n",
      "[184]\ttrain-mlogloss:0.06511\teval-mlogloss:0.03641\n",
      "[185]\ttrain-mlogloss:0.06505\teval-mlogloss:0.03665\n",
      "[186]\ttrain-mlogloss:0.06500\teval-mlogloss:0.03643\n",
      "[187]\ttrain-mlogloss:0.06498\teval-mlogloss:0.03636\n",
      "[188]\ttrain-mlogloss:0.06492\teval-mlogloss:0.03644\n",
      "[189]\ttrain-mlogloss:0.06487\teval-mlogloss:0.03659\n",
      "[190]\ttrain-mlogloss:0.06483\teval-mlogloss:0.03642\n",
      "[191]\ttrain-mlogloss:0.06478\teval-mlogloss:0.03635\n",
      "[192]\ttrain-mlogloss:0.06473\teval-mlogloss:0.03635\n",
      "[193]\ttrain-mlogloss:0.06471\teval-mlogloss:0.03644\n",
      "[194]\ttrain-mlogloss:0.06466\teval-mlogloss:0.03652\n",
      "[195]\ttrain-mlogloss:0.06460\teval-mlogloss:0.03625\n",
      "[196]\ttrain-mlogloss:0.06454\teval-mlogloss:0.03607\n",
      "[197]\ttrain-mlogloss:0.06452\teval-mlogloss:0.03605\n",
      "[198]\ttrain-mlogloss:0.06447\teval-mlogloss:0.03605\n",
      "[199]\ttrain-mlogloss:0.06443\teval-mlogloss:0.03612\n",
      "[200]\ttrain-mlogloss:0.06438\teval-mlogloss:0.03613\n",
      "[201]\ttrain-mlogloss:0.06433\teval-mlogloss:0.03606\n",
      "[202]\ttrain-mlogloss:0.06431\teval-mlogloss:0.03595\n",
      "[203]\ttrain-mlogloss:0.06428\teval-mlogloss:0.03602\n",
      "[204]\ttrain-mlogloss:0.06424\teval-mlogloss:0.03603\n",
      "[205]\ttrain-mlogloss:0.06420\teval-mlogloss:0.03601\n",
      "[206]\ttrain-mlogloss:0.06415\teval-mlogloss:0.03610\n",
      "[207]\ttrain-mlogloss:0.06411\teval-mlogloss:0.03611\n",
      "[208]\ttrain-mlogloss:0.06410\teval-mlogloss:0.03606\n",
      "[209]\ttrain-mlogloss:0.06406\teval-mlogloss:0.03607\n",
      "[210]\ttrain-mlogloss:0.06404\teval-mlogloss:0.03603\n",
      "[211]\ttrain-mlogloss:0.06403\teval-mlogloss:0.03598\n",
      "[212]\ttrain-mlogloss:0.06401\teval-mlogloss:0.03597\n",
      "[213]\ttrain-mlogloss:0.06399\teval-mlogloss:0.03604\n",
      "[214]\ttrain-mlogloss:0.06396\teval-mlogloss:0.03610\n",
      "[215]\ttrain-mlogloss:0.06393\teval-mlogloss:0.03605\n",
      "[216]\ttrain-mlogloss:0.06390\teval-mlogloss:0.03599\n",
      "[217]\ttrain-mlogloss:0.06389\teval-mlogloss:0.03610\n",
      "[218]\ttrain-mlogloss:0.06386\teval-mlogloss:0.03611\n",
      "[219]\ttrain-mlogloss:0.06383\teval-mlogloss:0.03608\n",
      "[220]\ttrain-mlogloss:0.06382\teval-mlogloss:0.03609\n",
      "[221]\ttrain-mlogloss:0.06380\teval-mlogloss:0.03603\n",
      "[222]\ttrain-mlogloss:0.06378\teval-mlogloss:0.03601\n",
      "[223]\ttrain-mlogloss:0.06376\teval-mlogloss:0.03602\n",
      "[224]\ttrain-mlogloss:0.06373\teval-mlogloss:0.03604\n",
      "[225]\ttrain-mlogloss:0.06371\teval-mlogloss:0.03612\n",
      "[226]\ttrain-mlogloss:0.06368\teval-mlogloss:0.03614\n",
      "[227]\ttrain-mlogloss:0.06367\teval-mlogloss:0.03618\n",
      "[228]\ttrain-mlogloss:0.06365\teval-mlogloss:0.03616\n",
      "[229]\ttrain-mlogloss:0.06363\teval-mlogloss:0.03636\n",
      "[230]\ttrain-mlogloss:0.06360\teval-mlogloss:0.03624\n",
      "[231]\ttrain-mlogloss:0.06359\teval-mlogloss:0.03621\n",
      "[232]\ttrain-mlogloss:0.06357\teval-mlogloss:0.03626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:50,769] Trial 1 finished with value: 1.0 and parameters: {'lambda': 0.5735843410244273, 'alpha': 2.0494419209983586e-06, 'eta': 0.08777659961553262, 'gamma': 0.009276442907966639, 'max_depth': 5, 'min_child_weight': 3, 'subsample': 0.9718450880936284, 'colsample_bytree': 0.9964878436449474}. Best is trial 0 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.87800\teval-mlogloss:0.86070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:50,783] Trial 2 pruned. Trial was pruned at iteration 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.85952\teval-mlogloss:0.85062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:50,799] Trial 3 pruned. Trial was pruned at iteration 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.07633\teval-mlogloss:1.07538\n",
      "[1]\ttrain-mlogloss:1.06365\teval-mlogloss:1.06200\n",
      "[2]\ttrain-mlogloss:1.04989\teval-mlogloss:1.05080\n",
      "[3]\ttrain-mlogloss:1.02526\teval-mlogloss:1.02394\n",
      "[4]\ttrain-mlogloss:1.01550\teval-mlogloss:1.01422\n",
      "[5]\ttrain-mlogloss:0.99867\teval-mlogloss:0.99659\n",
      "[6]\ttrain-mlogloss:0.97490\teval-mlogloss:0.97043\n",
      "[7]\ttrain-mlogloss:0.94997\teval-mlogloss:0.94269\n",
      "[8]\ttrain-mlogloss:0.93943\teval-mlogloss:0.93347\n",
      "[9]\ttrain-mlogloss:0.91488\teval-mlogloss:0.90781\n",
      "[10]\ttrain-mlogloss:0.89770\teval-mlogloss:0.89032\n",
      "[11]\ttrain-mlogloss:0.88872\teval-mlogloss:0.88249\n",
      "[12]\ttrain-mlogloss:0.86955\teval-mlogloss:0.86191\n",
      "[13]\ttrain-mlogloss:0.85461\teval-mlogloss:0.84674\n",
      "[14]\ttrain-mlogloss:0.83414\teval-mlogloss:0.82436\n",
      "[15]\ttrain-mlogloss:0.82291\teval-mlogloss:0.81456\n",
      "[16]\ttrain-mlogloss:0.81370\teval-mlogloss:0.80377\n",
      "[17]\ttrain-mlogloss:0.79817\teval-mlogloss:0.78834\n",
      "[18]\ttrain-mlogloss:0.78262\teval-mlogloss:0.77231\n",
      "[19]\ttrain-mlogloss:0.77864\teval-mlogloss:0.76834\n",
      "[20]\ttrain-mlogloss:0.77434\teval-mlogloss:0.76404\n",
      "[21]\ttrain-mlogloss:0.76179\teval-mlogloss:0.75089\n",
      "[22]\ttrain-mlogloss:0.74382\teval-mlogloss:0.73081\n",
      "[23]\ttrain-mlogloss:0.72645\teval-mlogloss:0.71240\n",
      "[24]\ttrain-mlogloss:0.71240\teval-mlogloss:0.69681\n",
      "[25]\ttrain-mlogloss:0.69675\teval-mlogloss:0.67915\n",
      "[26]\ttrain-mlogloss:0.69298\teval-mlogloss:0.67533\n",
      "[27]\ttrain-mlogloss:0.68007\teval-mlogloss:0.66142\n",
      "[28]\ttrain-mlogloss:0.67141\teval-mlogloss:0.65323\n",
      "[29]\ttrain-mlogloss:0.66320\teval-mlogloss:0.64445\n",
      "[30]\ttrain-mlogloss:0.65796\teval-mlogloss:0.63876\n",
      "[31]\ttrain-mlogloss:0.64698\teval-mlogloss:0.62624\n",
      "[32]\ttrain-mlogloss:0.63314\teval-mlogloss:0.61091\n",
      "[33]\ttrain-mlogloss:0.62859\teval-mlogloss:0.60574\n",
      "[34]\ttrain-mlogloss:0.62100\teval-mlogloss:0.59700\n",
      "[35]\ttrain-mlogloss:0.61235\teval-mlogloss:0.58783\n",
      "[36]\ttrain-mlogloss:0.60498\teval-mlogloss:0.58032\n",
      "[37]\ttrain-mlogloss:0.59273\teval-mlogloss:0.56694\n",
      "[38]\ttrain-mlogloss:0.58442\teval-mlogloss:0.55833\n",
      "[39]\ttrain-mlogloss:0.57612\teval-mlogloss:0.54910\n",
      "[40]\ttrain-mlogloss:0.56983\teval-mlogloss:0.54302\n",
      "[41]\ttrain-mlogloss:0.56139\teval-mlogloss:0.53349\n",
      "[42]\ttrain-mlogloss:0.55262\teval-mlogloss:0.52350\n",
      "[43]\ttrain-mlogloss:0.54490\teval-mlogloss:0.51500\n",
      "[44]\ttrain-mlogloss:0.53682\teval-mlogloss:0.50705\n",
      "[45]\ttrain-mlogloss:0.52589\teval-mlogloss:0.49583\n",
      "[46]\ttrain-mlogloss:0.52329\teval-mlogloss:0.49282\n",
      "[47]\ttrain-mlogloss:0.51693\teval-mlogloss:0.48567\n",
      "[48]\ttrain-mlogloss:0.51074\teval-mlogloss:0.47917\n",
      "[49]\ttrain-mlogloss:0.50383\teval-mlogloss:0.47181\n",
      "[50]\ttrain-mlogloss:0.49575\teval-mlogloss:0.46259\n",
      "[51]\ttrain-mlogloss:0.48864\teval-mlogloss:0.45505\n",
      "[52]\ttrain-mlogloss:0.48458\teval-mlogloss:0.45057\n",
      "[53]\ttrain-mlogloss:0.47659\teval-mlogloss:0.44196\n",
      "[54]\ttrain-mlogloss:0.47181\teval-mlogloss:0.43712\n",
      "[55]\ttrain-mlogloss:0.47005\teval-mlogloss:0.43625\n",
      "[56]\ttrain-mlogloss:0.46516\teval-mlogloss:0.43045\n",
      "[57]\ttrain-mlogloss:0.46060\teval-mlogloss:0.42590\n",
      "[58]\ttrain-mlogloss:0.45491\teval-mlogloss:0.42016\n",
      "[59]\ttrain-mlogloss:0.44613\teval-mlogloss:0.41001\n",
      "[60]\ttrain-mlogloss:0.44101\teval-mlogloss:0.40443\n",
      "[61]\ttrain-mlogloss:0.43771\teval-mlogloss:0.40114\n",
      "[62]\ttrain-mlogloss:0.43054\teval-mlogloss:0.39331\n",
      "[63]\ttrain-mlogloss:0.42766\teval-mlogloss:0.39024\n",
      "[64]\ttrain-mlogloss:0.42253\teval-mlogloss:0.38463\n",
      "[65]\ttrain-mlogloss:0.41889\teval-mlogloss:0.38025\n",
      "[66]\ttrain-mlogloss:0.41240\teval-mlogloss:0.37302\n",
      "[67]\ttrain-mlogloss:0.40908\teval-mlogloss:0.36981\n",
      "[68]\ttrain-mlogloss:0.40430\teval-mlogloss:0.36451\n",
      "[69]\ttrain-mlogloss:0.40214\teval-mlogloss:0.36281\n",
      "[70]\ttrain-mlogloss:0.40042\teval-mlogloss:0.36095\n",
      "[71]\ttrain-mlogloss:0.39403\teval-mlogloss:0.35361\n",
      "[72]\ttrain-mlogloss:0.39318\teval-mlogloss:0.35325\n",
      "[73]\ttrain-mlogloss:0.38935\teval-mlogloss:0.34934\n",
      "[74]\ttrain-mlogloss:0.38320\teval-mlogloss:0.34288\n",
      "[75]\ttrain-mlogloss:0.38032\teval-mlogloss:0.33997\n",
      "[76]\ttrain-mlogloss:0.37927\teval-mlogloss:0.33912\n",
      "[77]\ttrain-mlogloss:0.37478\teval-mlogloss:0.33436\n",
      "[78]\ttrain-mlogloss:0.37024\teval-mlogloss:0.32957\n",
      "[79]\ttrain-mlogloss:0.36628\teval-mlogloss:0.32518\n",
      "[80]\ttrain-mlogloss:0.36462\teval-mlogloss:0.32369\n",
      "[81]\ttrain-mlogloss:0.36260\teval-mlogloss:0.32180\n",
      "[82]\ttrain-mlogloss:0.35939\teval-mlogloss:0.31859\n",
      "[83]\ttrain-mlogloss:0.35668\teval-mlogloss:0.31563\n",
      "[84]\ttrain-mlogloss:0.35249\teval-mlogloss:0.31116\n",
      "[85]\ttrain-mlogloss:0.35035\teval-mlogloss:0.30879\n",
      "[86]\ttrain-mlogloss:0.34747\teval-mlogloss:0.30593\n",
      "[87]\ttrain-mlogloss:0.34375\teval-mlogloss:0.30183\n",
      "[88]\ttrain-mlogloss:0.34091\teval-mlogloss:0.29880\n",
      "[89]\ttrain-mlogloss:0.33769\teval-mlogloss:0.29539\n",
      "[90]\ttrain-mlogloss:0.33604\teval-mlogloss:0.29356\n",
      "[91]\ttrain-mlogloss:0.33404\teval-mlogloss:0.29106\n",
      "[92]\ttrain-mlogloss:0.33222\teval-mlogloss:0.28916\n",
      "[93]\ttrain-mlogloss:0.33017\teval-mlogloss:0.28734\n",
      "[94]\ttrain-mlogloss:0.32564\teval-mlogloss:0.28266\n",
      "[95]\ttrain-mlogloss:0.32285\teval-mlogloss:0.27974\n",
      "[96]\ttrain-mlogloss:0.32002\teval-mlogloss:0.27668\n",
      "[97]\ttrain-mlogloss:0.31828\teval-mlogloss:0.27482\n",
      "[98]\ttrain-mlogloss:0.31471\teval-mlogloss:0.27068\n",
      "[99]\ttrain-mlogloss:0.31403\teval-mlogloss:0.27043\n",
      "[100]\ttrain-mlogloss:0.31235\teval-mlogloss:0.26871\n",
      "[101]\ttrain-mlogloss:0.31051\teval-mlogloss:0.26689\n",
      "[102]\ttrain-mlogloss:0.30785\teval-mlogloss:0.26387\n",
      "[103]\ttrain-mlogloss:0.30533\teval-mlogloss:0.26153\n",
      "[104]\ttrain-mlogloss:0.30232\teval-mlogloss:0.25826\n",
      "[105]\ttrain-mlogloss:0.30002\teval-mlogloss:0.25548\n",
      "[106]\ttrain-mlogloss:0.29704\teval-mlogloss:0.25196\n",
      "[107]\ttrain-mlogloss:0.29596\teval-mlogloss:0.25083\n",
      "[108]\ttrain-mlogloss:0.29349\teval-mlogloss:0.24834\n",
      "[109]\ttrain-mlogloss:0.29163\teval-mlogloss:0.24628\n",
      "[110]\ttrain-mlogloss:0.29015\teval-mlogloss:0.24455\n",
      "[111]\ttrain-mlogloss:0.28651\teval-mlogloss:0.24009\n",
      "[112]\ttrain-mlogloss:0.28441\teval-mlogloss:0.23783\n",
      "[113]\ttrain-mlogloss:0.28357\teval-mlogloss:0.23684\n",
      "[114]\ttrain-mlogloss:0.28099\teval-mlogloss:0.23381\n",
      "[115]\ttrain-mlogloss:0.27810\teval-mlogloss:0.23037\n",
      "[116]\ttrain-mlogloss:0.27749\teval-mlogloss:0.22963\n",
      "[117]\ttrain-mlogloss:0.27501\teval-mlogloss:0.22661\n",
      "[118]\ttrain-mlogloss:0.27500\teval-mlogloss:0.22660\n",
      "[119]\ttrain-mlogloss:0.27294\teval-mlogloss:0.22436\n",
      "[120]\ttrain-mlogloss:0.27095\teval-mlogloss:0.22207\n",
      "[121]\ttrain-mlogloss:0.26904\teval-mlogloss:0.22015\n",
      "[122]\ttrain-mlogloss:0.26769\teval-mlogloss:0.21865\n",
      "[123]\ttrain-mlogloss:0.26629\teval-mlogloss:0.21729\n",
      "[124]\ttrain-mlogloss:0.26460\teval-mlogloss:0.21522\n",
      "[125]\ttrain-mlogloss:0.26377\teval-mlogloss:0.21435\n",
      "[126]\ttrain-mlogloss:0.26232\teval-mlogloss:0.21276\n",
      "[127]\ttrain-mlogloss:0.26086\teval-mlogloss:0.21122\n",
      "[128]\ttrain-mlogloss:0.25892\teval-mlogloss:0.20877\n",
      "[129]\ttrain-mlogloss:0.25723\teval-mlogloss:0.20655\n",
      "[130]\ttrain-mlogloss:0.25580\teval-mlogloss:0.20512\n",
      "[131]\ttrain-mlogloss:0.25452\teval-mlogloss:0.20342\n",
      "[132]\ttrain-mlogloss:0.25314\teval-mlogloss:0.20197\n",
      "[133]\ttrain-mlogloss:0.25277\teval-mlogloss:0.20147\n",
      "[134]\ttrain-mlogloss:0.25112\teval-mlogloss:0.19953\n",
      "[135]\ttrain-mlogloss:0.24959\teval-mlogloss:0.19758\n",
      "[136]\ttrain-mlogloss:0.24900\teval-mlogloss:0.19679\n",
      "[137]\ttrain-mlogloss:0.24683\teval-mlogloss:0.19415\n",
      "[138]\ttrain-mlogloss:0.24479\teval-mlogloss:0.19189\n",
      "[139]\ttrain-mlogloss:0.24469\teval-mlogloss:0.19188\n",
      "[140]\ttrain-mlogloss:0.24337\teval-mlogloss:0.19026\n",
      "[141]\ttrain-mlogloss:0.24146\teval-mlogloss:0.18801\n",
      "[142]\ttrain-mlogloss:0.24045\teval-mlogloss:0.18682\n",
      "[143]\ttrain-mlogloss:0.23944\teval-mlogloss:0.18566\n",
      "[144]\ttrain-mlogloss:0.23770\teval-mlogloss:0.18357\n",
      "[145]\ttrain-mlogloss:0.23694\teval-mlogloss:0.18259\n",
      "[146]\ttrain-mlogloss:0.23498\teval-mlogloss:0.18041\n",
      "[147]\ttrain-mlogloss:0.23397\teval-mlogloss:0.17903\n",
      "[148]\ttrain-mlogloss:0.23250\teval-mlogloss:0.17728\n",
      "[149]\ttrain-mlogloss:0.23197\teval-mlogloss:0.17673\n",
      "[150]\ttrain-mlogloss:0.23159\teval-mlogloss:0.17626\n",
      "[151]\ttrain-mlogloss:0.23119\teval-mlogloss:0.17573\n",
      "[152]\ttrain-mlogloss:0.23120\teval-mlogloss:0.17571\n",
      "[153]\ttrain-mlogloss:0.23020\teval-mlogloss:0.17463\n",
      "[154]\ttrain-mlogloss:0.22848\teval-mlogloss:0.17279\n",
      "[155]\ttrain-mlogloss:0.22776\teval-mlogloss:0.17185\n",
      "[156]\ttrain-mlogloss:0.22706\teval-mlogloss:0.17085\n",
      "[157]\ttrain-mlogloss:0.22640\teval-mlogloss:0.17002\n",
      "[158]\ttrain-mlogloss:0.22539\teval-mlogloss:0.16872\n",
      "[159]\ttrain-mlogloss:0.22525\teval-mlogloss:0.16868\n",
      "[160]\ttrain-mlogloss:0.22513\teval-mlogloss:0.16873\n",
      "[161]\ttrain-mlogloss:0.22512\teval-mlogloss:0.16870\n",
      "[162]\ttrain-mlogloss:0.22491\teval-mlogloss:0.16845\n",
      "[163]\ttrain-mlogloss:0.22484\teval-mlogloss:0.16844\n",
      "[164]\ttrain-mlogloss:0.22482\teval-mlogloss:0.16837\n",
      "[165]\ttrain-mlogloss:0.22463\teval-mlogloss:0.16848\n",
      "[166]\ttrain-mlogloss:0.22360\teval-mlogloss:0.16731\n",
      "[167]\ttrain-mlogloss:0.22312\teval-mlogloss:0.16676\n",
      "[168]\ttrain-mlogloss:0.22231\teval-mlogloss:0.16592\n",
      "[169]\ttrain-mlogloss:0.22168\teval-mlogloss:0.16521\n",
      "[170]\ttrain-mlogloss:0.22109\teval-mlogloss:0.16459\n",
      "[171]\ttrain-mlogloss:0.22097\teval-mlogloss:0.16437\n",
      "[172]\ttrain-mlogloss:0.22022\teval-mlogloss:0.16339\n",
      "[173]\ttrain-mlogloss:0.21958\teval-mlogloss:0.16266\n",
      "[174]\ttrain-mlogloss:0.21944\teval-mlogloss:0.16244\n",
      "[175]\ttrain-mlogloss:0.21868\teval-mlogloss:0.16154\n",
      "[176]\ttrain-mlogloss:0.21862\teval-mlogloss:0.16144\n",
      "[177]\ttrain-mlogloss:0.21815\teval-mlogloss:0.16089\n",
      "[178]\ttrain-mlogloss:0.21760\teval-mlogloss:0.16017\n",
      "[179]\ttrain-mlogloss:0.21721\teval-mlogloss:0.15960\n",
      "[180]\ttrain-mlogloss:0.21674\teval-mlogloss:0.15904\n",
      "[181]\ttrain-mlogloss:0.21671\teval-mlogloss:0.15901\n",
      "[182]\ttrain-mlogloss:0.21601\teval-mlogloss:0.15806\n",
      "[183]\ttrain-mlogloss:0.21537\teval-mlogloss:0.15756\n",
      "[184]\ttrain-mlogloss:0.21435\teval-mlogloss:0.15662\n",
      "[185]\ttrain-mlogloss:0.21434\teval-mlogloss:0.15659\n",
      "[186]\ttrain-mlogloss:0.21421\teval-mlogloss:0.15671\n",
      "[187]\ttrain-mlogloss:0.21421\teval-mlogloss:0.15671\n",
      "[188]\ttrain-mlogloss:0.21420\teval-mlogloss:0.15672\n",
      "[189]\ttrain-mlogloss:0.21382\teval-mlogloss:0.15625\n",
      "[190]\ttrain-mlogloss:0.21382\teval-mlogloss:0.15625\n",
      "[191]\ttrain-mlogloss:0.21382\teval-mlogloss:0.15614\n",
      "[192]\ttrain-mlogloss:0.21371\teval-mlogloss:0.15620\n",
      "[193]\ttrain-mlogloss:0.21358\teval-mlogloss:0.15587\n",
      "[194]\ttrain-mlogloss:0.21325\teval-mlogloss:0.15564\n",
      "[195]\ttrain-mlogloss:0.21234\teval-mlogloss:0.15446\n",
      "[196]\ttrain-mlogloss:0.21217\teval-mlogloss:0.15428\n",
      "[197]\ttrain-mlogloss:0.21190\teval-mlogloss:0.15396\n",
      "[198]\ttrain-mlogloss:0.21178\teval-mlogloss:0.15409\n",
      "[199]\ttrain-mlogloss:0.21100\teval-mlogloss:0.15284\n",
      "[200]\ttrain-mlogloss:0.21090\teval-mlogloss:0.15297\n",
      "[201]\ttrain-mlogloss:0.21074\teval-mlogloss:0.15279\n",
      "[202]\ttrain-mlogloss:0.21074\teval-mlogloss:0.15279\n",
      "[203]\ttrain-mlogloss:0.21074\teval-mlogloss:0.15279\n",
      "[204]\ttrain-mlogloss:0.21071\teval-mlogloss:0.15275\n",
      "[205]\ttrain-mlogloss:0.20955\teval-mlogloss:0.15144\n",
      "[206]\ttrain-mlogloss:0.20955\teval-mlogloss:0.15145\n",
      "[207]\ttrain-mlogloss:0.20952\teval-mlogloss:0.15147\n",
      "[208]\ttrain-mlogloss:0.20926\teval-mlogloss:0.15125\n",
      "[209]\ttrain-mlogloss:0.20865\teval-mlogloss:0.15056\n",
      "[210]\ttrain-mlogloss:0.20865\teval-mlogloss:0.15055\n",
      "[211]\ttrain-mlogloss:0.20842\teval-mlogloss:0.15034\n",
      "[212]\ttrain-mlogloss:0.20767\teval-mlogloss:0.14910\n",
      "[213]\ttrain-mlogloss:0.20744\teval-mlogloss:0.14878\n",
      "[214]\ttrain-mlogloss:0.20683\teval-mlogloss:0.14822\n",
      "[215]\ttrain-mlogloss:0.20678\teval-mlogloss:0.14814\n",
      "[216]\ttrain-mlogloss:0.20668\teval-mlogloss:0.14799\n",
      "[217]\ttrain-mlogloss:0.20664\teval-mlogloss:0.14795\n",
      "[218]\ttrain-mlogloss:0.20627\teval-mlogloss:0.14744\n",
      "[219]\ttrain-mlogloss:0.20550\teval-mlogloss:0.14672\n",
      "[220]\ttrain-mlogloss:0.20519\teval-mlogloss:0.14637\n",
      "[221]\ttrain-mlogloss:0.20515\teval-mlogloss:0.14634\n",
      "[222]\ttrain-mlogloss:0.20514\teval-mlogloss:0.14633\n",
      "[223]\ttrain-mlogloss:0.20503\teval-mlogloss:0.14621\n",
      "[224]\ttrain-mlogloss:0.20429\teval-mlogloss:0.14536\n",
      "[225]\ttrain-mlogloss:0.20415\teval-mlogloss:0.14543\n",
      "[226]\ttrain-mlogloss:0.20407\teval-mlogloss:0.14549\n",
      "[227]\ttrain-mlogloss:0.20352\teval-mlogloss:0.14495\n",
      "[228]\ttrain-mlogloss:0.20338\teval-mlogloss:0.14477\n",
      "[229]\ttrain-mlogloss:0.20317\teval-mlogloss:0.14448\n",
      "[230]\ttrain-mlogloss:0.20312\teval-mlogloss:0.14441\n",
      "[231]\ttrain-mlogloss:0.20313\teval-mlogloss:0.14438\n",
      "[232]\ttrain-mlogloss:0.20313\teval-mlogloss:0.14439\n",
      "[233]\ttrain-mlogloss:0.20268\teval-mlogloss:0.14386\n",
      "[234]\ttrain-mlogloss:0.20262\teval-mlogloss:0.14381\n",
      "[235]\ttrain-mlogloss:0.20257\teval-mlogloss:0.14384\n",
      "[236]\ttrain-mlogloss:0.20206\teval-mlogloss:0.14313\n",
      "[237]\ttrain-mlogloss:0.20206\teval-mlogloss:0.14313\n",
      "[238]\ttrain-mlogloss:0.20190\teval-mlogloss:0.14299\n",
      "[239]\ttrain-mlogloss:0.20185\teval-mlogloss:0.14293\n",
      "[240]\ttrain-mlogloss:0.20179\teval-mlogloss:0.14283\n",
      "[241]\ttrain-mlogloss:0.20168\teval-mlogloss:0.14297\n",
      "[242]\ttrain-mlogloss:0.20168\teval-mlogloss:0.14299\n",
      "[243]\ttrain-mlogloss:0.20168\teval-mlogloss:0.14299\n",
      "[244]\ttrain-mlogloss:0.20168\teval-mlogloss:0.14299\n",
      "[245]\ttrain-mlogloss:0.20160\teval-mlogloss:0.14289\n",
      "[246]\ttrain-mlogloss:0.20158\teval-mlogloss:0.14288\n",
      "[247]\ttrain-mlogloss:0.20098\teval-mlogloss:0.14205\n",
      "[248]\ttrain-mlogloss:0.20098\teval-mlogloss:0.14205\n",
      "[249]\ttrain-mlogloss:0.20098\teval-mlogloss:0.14205\n",
      "[250]\ttrain-mlogloss:0.20085\teval-mlogloss:0.14182\n",
      "[251]\ttrain-mlogloss:0.20085\teval-mlogloss:0.14182\n",
      "[252]\ttrain-mlogloss:0.20086\teval-mlogloss:0.14184\n",
      "[253]\ttrain-mlogloss:0.20086\teval-mlogloss:0.14185\n",
      "[254]\ttrain-mlogloss:0.20051\teval-mlogloss:0.14143\n",
      "[255]\ttrain-mlogloss:0.20040\teval-mlogloss:0.14151\n",
      "[256]\ttrain-mlogloss:0.20004\teval-mlogloss:0.14100\n",
      "[257]\ttrain-mlogloss:0.19945\teval-mlogloss:0.14018\n",
      "[258]\ttrain-mlogloss:0.19897\teval-mlogloss:0.13967\n",
      "[259]\ttrain-mlogloss:0.19831\teval-mlogloss:0.13896\n",
      "[260]\ttrain-mlogloss:0.19830\teval-mlogloss:0.13897\n",
      "[261]\ttrain-mlogloss:0.19828\teval-mlogloss:0.13896\n",
      "[262]\ttrain-mlogloss:0.19819\teval-mlogloss:0.13890\n",
      "[263]\ttrain-mlogloss:0.19817\teval-mlogloss:0.13890\n",
      "[264]\ttrain-mlogloss:0.19773\teval-mlogloss:0.13828\n",
      "[265]\ttrain-mlogloss:0.19777\teval-mlogloss:0.13829\n",
      "[266]\ttrain-mlogloss:0.19771\teval-mlogloss:0.13820\n",
      "[267]\ttrain-mlogloss:0.19770\teval-mlogloss:0.13821\n",
      "[268]\ttrain-mlogloss:0.19730\teval-mlogloss:0.13783\n",
      "[269]\ttrain-mlogloss:0.19699\teval-mlogloss:0.13748\n",
      "[270]\ttrain-mlogloss:0.19642\teval-mlogloss:0.13674\n",
      "[271]\ttrain-mlogloss:0.19579\teval-mlogloss:0.13580\n",
      "[272]\ttrain-mlogloss:0.19579\teval-mlogloss:0.13580\n",
      "[273]\ttrain-mlogloss:0.19544\teval-mlogloss:0.13529\n",
      "[274]\ttrain-mlogloss:0.19544\teval-mlogloss:0.13528\n",
      "[275]\ttrain-mlogloss:0.19523\teval-mlogloss:0.13503\n",
      "[276]\ttrain-mlogloss:0.19523\teval-mlogloss:0.13503\n",
      "[277]\ttrain-mlogloss:0.19514\teval-mlogloss:0.13490\n",
      "[278]\ttrain-mlogloss:0.19513\teval-mlogloss:0.13490\n",
      "[279]\ttrain-mlogloss:0.19505\teval-mlogloss:0.13482\n",
      "[280]\ttrain-mlogloss:0.19450\teval-mlogloss:0.13417\n",
      "[281]\ttrain-mlogloss:0.19443\teval-mlogloss:0.13411\n",
      "[282]\ttrain-mlogloss:0.19443\teval-mlogloss:0.13409\n",
      "[283]\ttrain-mlogloss:0.19423\teval-mlogloss:0.13383\n",
      "[284]\ttrain-mlogloss:0.19423\teval-mlogloss:0.13383\n",
      "[285]\ttrain-mlogloss:0.19421\teval-mlogloss:0.13372\n",
      "[286]\ttrain-mlogloss:0.19409\teval-mlogloss:0.13356\n",
      "[287]\ttrain-mlogloss:0.19401\teval-mlogloss:0.13361\n",
      "[288]\ttrain-mlogloss:0.19401\teval-mlogloss:0.13357\n",
      "[289]\ttrain-mlogloss:0.19381\teval-mlogloss:0.13332\n",
      "[290]\ttrain-mlogloss:0.19371\teval-mlogloss:0.13325\n",
      "[291]\ttrain-mlogloss:0.19341\teval-mlogloss:0.13283\n",
      "[292]\ttrain-mlogloss:0.19333\teval-mlogloss:0.13279\n",
      "[293]\ttrain-mlogloss:0.19294\teval-mlogloss:0.13235\n",
      "[294]\ttrain-mlogloss:0.19299\teval-mlogloss:0.13237\n",
      "[295]\ttrain-mlogloss:0.19294\teval-mlogloss:0.13232\n",
      "[296]\ttrain-mlogloss:0.19294\teval-mlogloss:0.13232\n",
      "[297]\ttrain-mlogloss:0.19294\teval-mlogloss:0.13232\n",
      "[298]\ttrain-mlogloss:0.19293\teval-mlogloss:0.13230\n",
      "[299]\ttrain-mlogloss:0.19292\teval-mlogloss:0.13233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:52,325] Trial 4 finished with value: 1.0 and parameters: {'lambda': 5.6801237084600975e-05, 'alpha': 0.6569817697571978, 'eta': 0.025629906671144488, 'gamma': 0.0038442454699242888, 'max_depth': 5, 'min_child_weight': 4, 'subsample': 0.43459855135381326, 'colsample_bytree': 0.4453076520932994}. Best is trial 0 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.87713\teval-mlogloss:0.87446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:52,336] Trial 5 pruned. Trial was pruned at iteration 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.01814\teval-mlogloss:1.01649\n",
      "[1]\ttrain-mlogloss:0.94596\teval-mlogloss:0.93913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:52,346] Trial 6 pruned. Trial was pruned at iteration 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.02144\teval-mlogloss:1.02196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:52,357] Trial 7 pruned. Trial was pruned at iteration 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.04062\teval-mlogloss:1.03819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:52,367] Trial 8 pruned. Trial was pruned at iteration 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.08163\teval-mlogloss:1.08147\n",
      "[1]\ttrain-mlogloss:1.06502\teval-mlogloss:1.06421\n",
      "[2]\ttrain-mlogloss:1.04850\teval-mlogloss:1.04697\n",
      "[3]\ttrain-mlogloss:1.03232\teval-mlogloss:1.02984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:52,387] Trial 9 pruned. Trial was pruned at iteration 4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.77653\teval-mlogloss:0.78026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:52,442] Trial 10 pruned. Trial was pruned at iteration 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.93634\teval-mlogloss:0.93295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:52,511] Trial 11 pruned. Trial was pruned at iteration 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.94731\teval-mlogloss:0.94807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:52,564] Trial 12 pruned. Trial was pruned at iteration 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.82143\teval-mlogloss:0.80909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:52,620] Trial 13 pruned. Trial was pruned at iteration 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.97103\teval-mlogloss:0.96387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:52,675] Trial 14 pruned. Trial was pruned at iteration 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.82109\teval-mlogloss:0.80521\n",
      "[1]\ttrain-mlogloss:0.70071\teval-mlogloss:0.70199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:52,730] Trial 15 pruned. Trial was pruned at iteration 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.98645\teval-mlogloss:0.98703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:52,784] Trial 16 pruned. Trial was pruned at iteration 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.89494\teval-mlogloss:0.88769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:52,832] Trial 17 pruned. Trial was pruned at iteration 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.90767\teval-mlogloss:0.89683\n",
      "[1]\ttrain-mlogloss:0.83247\teval-mlogloss:0.81460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:52,886] Trial 18 pruned. Trial was pruned at iteration 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.89763\teval-mlogloss:0.88809\n",
      "[1]\ttrain-mlogloss:0.80583\teval-mlogloss:0.79955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:52,941] Trial 19 pruned. Trial was pruned at iteration 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.80111\teval-mlogloss:0.79203\n",
      "[1]\ttrain-mlogloss:0.68693\teval-mlogloss:0.68548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:52,999] Trial 20 pruned. Trial was pruned at iteration 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.08228\teval-mlogloss:1.08149\n",
      "[1]\ttrain-mlogloss:1.07336\teval-mlogloss:1.07128\n",
      "[2]\ttrain-mlogloss:1.06296\teval-mlogloss:1.06247\n",
      "[3]\ttrain-mlogloss:1.04409\teval-mlogloss:1.04246\n",
      "[4]\ttrain-mlogloss:1.03709\teval-mlogloss:1.03500\n",
      "[5]\ttrain-mlogloss:1.02443\teval-mlogloss:1.02193\n",
      "[6]\ttrain-mlogloss:1.00688\teval-mlogloss:1.00272\n",
      "[7]\ttrain-mlogloss:0.98817\teval-mlogloss:0.98269\n",
      "[8]\ttrain-mlogloss:0.97973\teval-mlogloss:0.97503\n",
      "[9]\ttrain-mlogloss:0.96061\teval-mlogloss:0.95508\n",
      "[10]\ttrain-mlogloss:0.94686\teval-mlogloss:0.94118\n",
      "[11]\ttrain-mlogloss:0.93961\teval-mlogloss:0.93429\n",
      "[12]\ttrain-mlogloss:0.92444\teval-mlogloss:0.91805\n",
      "[13]\ttrain-mlogloss:0.91270\teval-mlogloss:0.90623\n",
      "[14]\ttrain-mlogloss:0.89657\teval-mlogloss:0.88868\n",
      "[15]\ttrain-mlogloss:0.88755\teval-mlogloss:0.88045\n",
      "[16]\ttrain-mlogloss:0.87998\teval-mlogloss:0.87213\n",
      "[17]\ttrain-mlogloss:0.86730\teval-mlogloss:0.85956\n",
      "[18]\ttrain-mlogloss:0.85478\teval-mlogloss:0.84627\n",
      "[19]\ttrain-mlogloss:0.85158\teval-mlogloss:0.84271\n",
      "[20]\ttrain-mlogloss:0.84799\teval-mlogloss:0.83908\n",
      "[21]\ttrain-mlogloss:0.83745\teval-mlogloss:0.82807\n",
      "[22]\ttrain-mlogloss:0.82206\teval-mlogloss:0.81105\n",
      "[23]\ttrain-mlogloss:0.80781\teval-mlogloss:0.79574\n",
      "[24]\ttrain-mlogloss:0.79531\teval-mlogloss:0.78192\n",
      "[25]\ttrain-mlogloss:0.78209\teval-mlogloss:0.76711\n",
      "[26]\ttrain-mlogloss:0.77857\teval-mlogloss:0.76347\n",
      "[27]\ttrain-mlogloss:0.76731\teval-mlogloss:0.75145\n",
      "[28]\ttrain-mlogloss:0.75987\teval-mlogloss:0.74443\n",
      "[29]\ttrain-mlogloss:0.75329\teval-mlogloss:0.73742\n",
      "[30]\ttrain-mlogloss:0.74824\teval-mlogloss:0.73223\n",
      "[31]\ttrain-mlogloss:0.73860\teval-mlogloss:0.72090\n",
      "[32]\ttrain-mlogloss:0.72676\teval-mlogloss:0.70841\n",
      "[33]\ttrain-mlogloss:0.72241\teval-mlogloss:0.70357\n",
      "[34]\ttrain-mlogloss:0.71574\teval-mlogloss:0.69588\n",
      "[35]\ttrain-mlogloss:0.70796\teval-mlogloss:0.68771\n",
      "[36]\ttrain-mlogloss:0.70135\teval-mlogloss:0.68072\n",
      "[37]\ttrain-mlogloss:0.69080\teval-mlogloss:0.66984\n",
      "[38]\ttrain-mlogloss:0.68353\teval-mlogloss:0.66213\n",
      "[39]\ttrain-mlogloss:0.67600\teval-mlogloss:0.65378\n",
      "[40]\ttrain-mlogloss:0.67056\teval-mlogloss:0.64820\n",
      "[41]\ttrain-mlogloss:0.66282\teval-mlogloss:0.63951\n",
      "[42]\ttrain-mlogloss:0.65432\teval-mlogloss:0.63012\n",
      "[43]\ttrain-mlogloss:0.64725\teval-mlogloss:0.62239\n",
      "[44]\ttrain-mlogloss:0.63953\teval-mlogloss:0.61446\n",
      "[45]\ttrain-mlogloss:0.62937\teval-mlogloss:0.60403\n",
      "[46]\ttrain-mlogloss:0.62688\teval-mlogloss:0.60118\n",
      "[47]\ttrain-mlogloss:0.62074\teval-mlogloss:0.59445\n",
      "[48]\ttrain-mlogloss:0.61432\teval-mlogloss:0.58818\n",
      "[49]\ttrain-mlogloss:0.60764\teval-mlogloss:0.58103\n",
      "[50]\ttrain-mlogloss:0.59963\teval-mlogloss:0.57223\n",
      "[51]\ttrain-mlogloss:0.59292\teval-mlogloss:0.56546\n",
      "[52]\ttrain-mlogloss:0.58879\teval-mlogloss:0.56136\n",
      "[53]\ttrain-mlogloss:0.58111\teval-mlogloss:0.55313\n",
      "[54]\ttrain-mlogloss:0.57650\teval-mlogloss:0.54841\n",
      "[55]\ttrain-mlogloss:0.57451\teval-mlogloss:0.54738\n",
      "[56]\ttrain-mlogloss:0.56955\teval-mlogloss:0.54191\n",
      "[57]\ttrain-mlogloss:0.56460\teval-mlogloss:0.53650\n",
      "[58]\ttrain-mlogloss:0.55889\teval-mlogloss:0.53072\n",
      "[59]\ttrain-mlogloss:0.55032\teval-mlogloss:0.52093\n",
      "[60]\ttrain-mlogloss:0.54531\teval-mlogloss:0.51551\n",
      "[61]\ttrain-mlogloss:0.54232\teval-mlogloss:0.51234\n",
      "[62]\ttrain-mlogloss:0.53535\teval-mlogloss:0.50500\n",
      "[63]\ttrain-mlogloss:0.53226\teval-mlogloss:0.50178\n",
      "[64]\ttrain-mlogloss:0.52739\teval-mlogloss:0.49684\n",
      "[65]\ttrain-mlogloss:0.52351\teval-mlogloss:0.49248\n",
      "[66]\ttrain-mlogloss:0.51699\teval-mlogloss:0.48522\n",
      "[67]\ttrain-mlogloss:0.51352\teval-mlogloss:0.48146\n",
      "[68]\ttrain-mlogloss:0.50874\teval-mlogloss:0.47623\n",
      "[69]\ttrain-mlogloss:0.50560\teval-mlogloss:0.47357\n",
      "[70]\ttrain-mlogloss:0.50361\teval-mlogloss:0.47133\n",
      "[71]\ttrain-mlogloss:0.49700\teval-mlogloss:0.46379\n",
      "[72]\ttrain-mlogloss:0.49597\teval-mlogloss:0.46331\n",
      "[73]\ttrain-mlogloss:0.49209\teval-mlogloss:0.45934\n",
      "[74]\ttrain-mlogloss:0.48573\teval-mlogloss:0.45266\n",
      "[75]\ttrain-mlogloss:0.48269\teval-mlogloss:0.44952\n",
      "[76]\ttrain-mlogloss:0.48112\teval-mlogloss:0.44826\n",
      "[77]\ttrain-mlogloss:0.47640\teval-mlogloss:0.44332\n",
      "[78]\ttrain-mlogloss:0.47184\teval-mlogloss:0.43848\n",
      "[79]\ttrain-mlogloss:0.46722\teval-mlogloss:0.43338\n",
      "[80]\ttrain-mlogloss:0.46535\teval-mlogloss:0.43160\n",
      "[81]\ttrain-mlogloss:0.46315\teval-mlogloss:0.42955\n",
      "[82]\ttrain-mlogloss:0.45941\teval-mlogloss:0.42584\n",
      "[83]\ttrain-mlogloss:0.45649\teval-mlogloss:0.42275\n",
      "[84]\ttrain-mlogloss:0.45214\teval-mlogloss:0.41815\n",
      "[85]\ttrain-mlogloss:0.44979\teval-mlogloss:0.41548\n",
      "[86]\ttrain-mlogloss:0.44659\teval-mlogloss:0.41270\n",
      "[87]\ttrain-mlogloss:0.44258\teval-mlogloss:0.40812\n",
      "[88]\ttrain-mlogloss:0.43951\teval-mlogloss:0.40488\n",
      "[89]\ttrain-mlogloss:0.43568\teval-mlogloss:0.40100\n",
      "[90]\ttrain-mlogloss:0.43383\teval-mlogloss:0.39893\n",
      "[91]\ttrain-mlogloss:0.43110\teval-mlogloss:0.39608\n",
      "[92]\ttrain-mlogloss:0.42905\teval-mlogloss:0.39396\n",
      "[93]\ttrain-mlogloss:0.42673\teval-mlogloss:0.39181\n",
      "[94]\ttrain-mlogloss:0.42160\teval-mlogloss:0.38657\n",
      "[95]\ttrain-mlogloss:0.41816\teval-mlogloss:0.38282\n",
      "[96]\ttrain-mlogloss:0.41469\teval-mlogloss:0.37880\n",
      "[97]\ttrain-mlogloss:0.41272\teval-mlogloss:0.37650\n",
      "[98]\ttrain-mlogloss:0.40875\teval-mlogloss:0.37183\n",
      "[99]\ttrain-mlogloss:0.40797\teval-mlogloss:0.37141\n",
      "[100]\ttrain-mlogloss:0.40615\teval-mlogloss:0.36955\n",
      "[101]\ttrain-mlogloss:0.40422\teval-mlogloss:0.36742\n",
      "[102]\ttrain-mlogloss:0.40094\teval-mlogloss:0.36400\n",
      "[103]\ttrain-mlogloss:0.39817\teval-mlogloss:0.36124\n",
      "[104]\ttrain-mlogloss:0.39482\teval-mlogloss:0.35763\n",
      "[105]\ttrain-mlogloss:0.39206\teval-mlogloss:0.35456\n",
      "[106]\ttrain-mlogloss:0.38827\teval-mlogloss:0.35022\n",
      "[107]\ttrain-mlogloss:0.38688\teval-mlogloss:0.34878\n",
      "[108]\ttrain-mlogloss:0.38367\teval-mlogloss:0.34554\n",
      "[109]\ttrain-mlogloss:0.38120\teval-mlogloss:0.34285\n",
      "[110]\ttrain-mlogloss:0.37939\teval-mlogloss:0.34084\n",
      "[111]\ttrain-mlogloss:0.37497\teval-mlogloss:0.33591\n",
      "[112]\ttrain-mlogloss:0.37253\teval-mlogloss:0.33328\n",
      "[113]\ttrain-mlogloss:0.37143\teval-mlogloss:0.33197\n",
      "[114]\ttrain-mlogloss:0.36850\teval-mlogloss:0.32846\n",
      "[115]\ttrain-mlogloss:0.36525\teval-mlogloss:0.32469\n",
      "[116]\ttrain-mlogloss:0.36430\teval-mlogloss:0.32343\n",
      "[117]\ttrain-mlogloss:0.36088\teval-mlogloss:0.31952\n",
      "[118]\ttrain-mlogloss:0.36058\teval-mlogloss:0.31914\n",
      "[119]\ttrain-mlogloss:0.35835\teval-mlogloss:0.31673\n",
      "[120]\ttrain-mlogloss:0.35588\teval-mlogloss:0.31382\n",
      "[121]\ttrain-mlogloss:0.35256\teval-mlogloss:0.31046\n",
      "[122]\ttrain-mlogloss:0.35085\teval-mlogloss:0.30854\n",
      "[123]\ttrain-mlogloss:0.34869\teval-mlogloss:0.30602\n",
      "[124]\ttrain-mlogloss:0.34667\teval-mlogloss:0.30350\n",
      "[125]\ttrain-mlogloss:0.34542\teval-mlogloss:0.30215\n",
      "[126]\ttrain-mlogloss:0.34324\teval-mlogloss:0.30011\n",
      "[127]\ttrain-mlogloss:0.34096\teval-mlogloss:0.29770\n",
      "[128]\ttrain-mlogloss:0.33796\teval-mlogloss:0.29423\n",
      "[129]\ttrain-mlogloss:0.33566\teval-mlogloss:0.29142\n",
      "[130]\ttrain-mlogloss:0.33337\teval-mlogloss:0.28913\n",
      "[131]\ttrain-mlogloss:0.33183\teval-mlogloss:0.28715\n",
      "[132]\ttrain-mlogloss:0.33002\teval-mlogloss:0.28507\n",
      "[133]\ttrain-mlogloss:0.32941\teval-mlogloss:0.28424\n",
      "[134]\ttrain-mlogloss:0.32705\teval-mlogloss:0.28158\n",
      "[135]\ttrain-mlogloss:0.32539\teval-mlogloss:0.27966\n",
      "[136]\ttrain-mlogloss:0.32375\teval-mlogloss:0.27779\n",
      "[137]\ttrain-mlogloss:0.32111\teval-mlogloss:0.27464\n",
      "[138]\ttrain-mlogloss:0.31879\teval-mlogloss:0.27187\n",
      "[139]\ttrain-mlogloss:0.31767\teval-mlogloss:0.27080\n",
      "[140]\ttrain-mlogloss:0.31611\teval-mlogloss:0.26896\n",
      "[141]\ttrain-mlogloss:0.31381\teval-mlogloss:0.26626\n",
      "[142]\ttrain-mlogloss:0.31249\teval-mlogloss:0.26486\n",
      "[143]\ttrain-mlogloss:0.31095\teval-mlogloss:0.26311\n",
      "[144]\ttrain-mlogloss:0.30873\teval-mlogloss:0.26053\n",
      "[145]\ttrain-mlogloss:0.30747\teval-mlogloss:0.25914\n",
      "[146]\ttrain-mlogloss:0.30512\teval-mlogloss:0.25666\n",
      "[147]\ttrain-mlogloss:0.30340\teval-mlogloss:0.25448\n",
      "[148]\ttrain-mlogloss:0.30180\teval-mlogloss:0.25263\n",
      "[149]\ttrain-mlogloss:0.30007\teval-mlogloss:0.25095\n",
      "[150]\ttrain-mlogloss:0.29920\teval-mlogloss:0.24979\n",
      "[151]\ttrain-mlogloss:0.29824\teval-mlogloss:0.24876\n",
      "[152]\ttrain-mlogloss:0.29608\teval-mlogloss:0.24635\n",
      "[153]\ttrain-mlogloss:0.29480\teval-mlogloss:0.24504\n",
      "[154]\ttrain-mlogloss:0.29288\teval-mlogloss:0.24288\n",
      "[155]\ttrain-mlogloss:0.29161\teval-mlogloss:0.24145\n",
      "[156]\ttrain-mlogloss:0.29044\teval-mlogloss:0.24007\n",
      "[157]\ttrain-mlogloss:0.28807\teval-mlogloss:0.23749\n",
      "[158]\ttrain-mlogloss:0.28570\teval-mlogloss:0.23477\n",
      "[159]\ttrain-mlogloss:0.28495\teval-mlogloss:0.23404\n",
      "[160]\ttrain-mlogloss:0.28473\teval-mlogloss:0.23384\n",
      "[161]\ttrain-mlogloss:0.28439\teval-mlogloss:0.23340\n",
      "[162]\ttrain-mlogloss:0.28324\teval-mlogloss:0.23214\n",
      "[163]\ttrain-mlogloss:0.28322\teval-mlogloss:0.23212\n",
      "[164]\ttrain-mlogloss:0.28190\teval-mlogloss:0.23063\n",
      "[165]\ttrain-mlogloss:0.28141\teval-mlogloss:0.23033\n",
      "[166]\ttrain-mlogloss:0.28002\teval-mlogloss:0.22895\n",
      "[167]\ttrain-mlogloss:0.27883\teval-mlogloss:0.22777\n",
      "[168]\ttrain-mlogloss:0.27795\teval-mlogloss:0.22684\n",
      "[169]\ttrain-mlogloss:0.27671\teval-mlogloss:0.22552\n",
      "[170]\ttrain-mlogloss:0.27541\teval-mlogloss:0.22432\n",
      "[171]\ttrain-mlogloss:0.27477\teval-mlogloss:0.22347\n",
      "[172]\ttrain-mlogloss:0.27394\teval-mlogloss:0.22240\n",
      "[173]\ttrain-mlogloss:0.27291\teval-mlogloss:0.22123\n",
      "[174]\ttrain-mlogloss:0.27237\teval-mlogloss:0.22050\n",
      "[175]\ttrain-mlogloss:0.27150\teval-mlogloss:0.21943\n",
      "[176]\ttrain-mlogloss:0.27062\teval-mlogloss:0.21858\n",
      "[177]\ttrain-mlogloss:0.27006\teval-mlogloss:0.21794\n",
      "[178]\ttrain-mlogloss:0.26920\teval-mlogloss:0.21681\n",
      "[179]\ttrain-mlogloss:0.26784\teval-mlogloss:0.21535\n",
      "[180]\ttrain-mlogloss:0.26659\teval-mlogloss:0.21401\n",
      "[181]\ttrain-mlogloss:0.26648\teval-mlogloss:0.21385\n",
      "[182]\ttrain-mlogloss:0.26566\teval-mlogloss:0.21280\n",
      "[183]\ttrain-mlogloss:0.26455\teval-mlogloss:0.21173\n",
      "[184]\ttrain-mlogloss:0.26313\teval-mlogloss:0.21029\n",
      "[185]\ttrain-mlogloss:0.26252\teval-mlogloss:0.20957\n",
      "[186]\ttrain-mlogloss:0.26210\teval-mlogloss:0.20933\n",
      "[187]\ttrain-mlogloss:0.26140\teval-mlogloss:0.20860\n",
      "[188]\ttrain-mlogloss:0.26074\teval-mlogloss:0.20799\n",
      "[189]\ttrain-mlogloss:0.25991\teval-mlogloss:0.20699\n",
      "[190]\ttrain-mlogloss:0.25961\teval-mlogloss:0.20665\n",
      "[191]\ttrain-mlogloss:0.25926\teval-mlogloss:0.20616\n",
      "[192]\ttrain-mlogloss:0.25905\teval-mlogloss:0.20603\n",
      "[193]\ttrain-mlogloss:0.25818\teval-mlogloss:0.20498\n",
      "[194]\ttrain-mlogloss:0.25778\teval-mlogloss:0.20465\n",
      "[195]\ttrain-mlogloss:0.25662\teval-mlogloss:0.20323\n",
      "[196]\ttrain-mlogloss:0.25551\teval-mlogloss:0.20204\n",
      "[197]\ttrain-mlogloss:0.25500\teval-mlogloss:0.20151\n",
      "[198]\ttrain-mlogloss:0.25486\teval-mlogloss:0.20154\n",
      "[199]\ttrain-mlogloss:0.25394\teval-mlogloss:0.20036\n",
      "[200]\ttrain-mlogloss:0.25324\teval-mlogloss:0.19983\n",
      "[201]\ttrain-mlogloss:0.25299\teval-mlogloss:0.19959\n",
      "[202]\ttrain-mlogloss:0.25270\teval-mlogloss:0.19930\n",
      "[203]\ttrain-mlogloss:0.25244\teval-mlogloss:0.19893\n",
      "[204]\ttrain-mlogloss:0.25242\teval-mlogloss:0.19890\n",
      "[205]\ttrain-mlogloss:0.25120\teval-mlogloss:0.19750\n",
      "[206]\ttrain-mlogloss:0.25020\teval-mlogloss:0.19644\n",
      "[207]\ttrain-mlogloss:0.24959\teval-mlogloss:0.19593\n",
      "[208]\ttrain-mlogloss:0.24915\teval-mlogloss:0.19535\n",
      "[209]\ttrain-mlogloss:0.24836\teval-mlogloss:0.19451\n",
      "[210]\ttrain-mlogloss:0.24836\teval-mlogloss:0.19451\n",
      "[211]\ttrain-mlogloss:0.24800\teval-mlogloss:0.19412\n",
      "[212]\ttrain-mlogloss:0.24683\teval-mlogloss:0.19267\n",
      "[213]\ttrain-mlogloss:0.24586\teval-mlogloss:0.19140\n",
      "[214]\ttrain-mlogloss:0.24513\teval-mlogloss:0.19070\n",
      "[215]\ttrain-mlogloss:0.24437\teval-mlogloss:0.18995\n",
      "[216]\ttrain-mlogloss:0.24368\teval-mlogloss:0.18922\n",
      "[217]\ttrain-mlogloss:0.24303\teval-mlogloss:0.18827\n",
      "[218]\ttrain-mlogloss:0.24271\teval-mlogloss:0.18796\n",
      "[219]\ttrain-mlogloss:0.24171\teval-mlogloss:0.18687\n",
      "[220]\ttrain-mlogloss:0.24126\teval-mlogloss:0.18652\n",
      "[221]\ttrain-mlogloss:0.24122\teval-mlogloss:0.18646\n",
      "[222]\ttrain-mlogloss:0.24076\teval-mlogloss:0.18602\n",
      "[223]\ttrain-mlogloss:0.24065\teval-mlogloss:0.18589\n",
      "[224]\ttrain-mlogloss:0.24000\teval-mlogloss:0.18531\n",
      "[225]\ttrain-mlogloss:0.23983\teval-mlogloss:0.18527\n",
      "[226]\ttrain-mlogloss:0.23980\teval-mlogloss:0.18532\n",
      "[227]\ttrain-mlogloss:0.23865\teval-mlogloss:0.18401\n",
      "[228]\ttrain-mlogloss:0.23852\teval-mlogloss:0.18387\n",
      "[229]\ttrain-mlogloss:0.23835\teval-mlogloss:0.18364\n",
      "[230]\ttrain-mlogloss:0.23830\teval-mlogloss:0.18358\n",
      "[231]\ttrain-mlogloss:0.23806\teval-mlogloss:0.18329\n",
      "[232]\ttrain-mlogloss:0.23806\teval-mlogloss:0.18330\n",
      "[233]\ttrain-mlogloss:0.23754\teval-mlogloss:0.18270\n",
      "[234]\ttrain-mlogloss:0.23725\teval-mlogloss:0.18235\n",
      "[235]\ttrain-mlogloss:0.23724\teval-mlogloss:0.18233\n",
      "[236]\ttrain-mlogloss:0.23650\teval-mlogloss:0.18146\n",
      "[237]\ttrain-mlogloss:0.23610\teval-mlogloss:0.18109\n",
      "[238]\ttrain-mlogloss:0.23595\teval-mlogloss:0.18095\n",
      "[239]\ttrain-mlogloss:0.23589\teval-mlogloss:0.18086\n",
      "[240]\ttrain-mlogloss:0.23581\teval-mlogloss:0.18075\n",
      "[241]\ttrain-mlogloss:0.23572\teval-mlogloss:0.18080\n",
      "[242]\ttrain-mlogloss:0.23568\teval-mlogloss:0.18087\n",
      "[243]\ttrain-mlogloss:0.23568\teval-mlogloss:0.18087\n",
      "[244]\ttrain-mlogloss:0.23554\teval-mlogloss:0.18072\n",
      "[245]\ttrain-mlogloss:0.23536\teval-mlogloss:0.18052\n",
      "[246]\ttrain-mlogloss:0.23533\teval-mlogloss:0.18052\n",
      "[247]\ttrain-mlogloss:0.23460\teval-mlogloss:0.17956\n",
      "[248]\ttrain-mlogloss:0.23381\teval-mlogloss:0.17883\n",
      "[249]\ttrain-mlogloss:0.23346\teval-mlogloss:0.17840\n",
      "[250]\ttrain-mlogloss:0.23297\teval-mlogloss:0.17778\n",
      "[251]\ttrain-mlogloss:0.23296\teval-mlogloss:0.17776\n",
      "[252]\ttrain-mlogloss:0.23211\teval-mlogloss:0.17674\n",
      "[253]\ttrain-mlogloss:0.23203\teval-mlogloss:0.17678\n",
      "[254]\ttrain-mlogloss:0.23141\teval-mlogloss:0.17593\n",
      "[255]\ttrain-mlogloss:0.23131\teval-mlogloss:0.17597\n",
      "[256]\ttrain-mlogloss:0.23082\teval-mlogloss:0.17533\n",
      "[257]\ttrain-mlogloss:0.23034\teval-mlogloss:0.17468\n",
      "[258]\ttrain-mlogloss:0.22980\teval-mlogloss:0.17405\n",
      "[259]\ttrain-mlogloss:0.22915\teval-mlogloss:0.17336\n",
      "[260]\ttrain-mlogloss:0.22915\teval-mlogloss:0.17338\n",
      "[261]\ttrain-mlogloss:0.22793\teval-mlogloss:0.17204\n",
      "[262]\ttrain-mlogloss:0.22788\teval-mlogloss:0.17201\n",
      "[263]\ttrain-mlogloss:0.22760\teval-mlogloss:0.17172\n",
      "[264]\ttrain-mlogloss:0.22718\teval-mlogloss:0.17113\n",
      "[265]\ttrain-mlogloss:0.22720\teval-mlogloss:0.17116\n",
      "[266]\ttrain-mlogloss:0.22720\teval-mlogloss:0.17116\n",
      "[267]\ttrain-mlogloss:0.22720\teval-mlogloss:0.17117\n",
      "[268]\ttrain-mlogloss:0.22695\teval-mlogloss:0.17081\n",
      "[269]\ttrain-mlogloss:0.22665\teval-mlogloss:0.17047\n",
      "[270]\ttrain-mlogloss:0.22588\teval-mlogloss:0.16934\n",
      "[271]\ttrain-mlogloss:0.22514\teval-mlogloss:0.16831\n",
      "[272]\ttrain-mlogloss:0.22514\teval-mlogloss:0.16831\n",
      "[273]\ttrain-mlogloss:0.22481\teval-mlogloss:0.16784\n",
      "[274]\ttrain-mlogloss:0.22448\teval-mlogloss:0.16744\n",
      "[275]\ttrain-mlogloss:0.22409\teval-mlogloss:0.16699\n",
      "[276]\ttrain-mlogloss:0.22408\teval-mlogloss:0.16696\n",
      "[277]\ttrain-mlogloss:0.22358\teval-mlogloss:0.16636\n",
      "[278]\ttrain-mlogloss:0.22358\teval-mlogloss:0.16636\n",
      "[279]\ttrain-mlogloss:0.22312\teval-mlogloss:0.16572\n",
      "[280]\ttrain-mlogloss:0.22248\teval-mlogloss:0.16496\n",
      "[281]\ttrain-mlogloss:0.22245\teval-mlogloss:0.16490\n",
      "[282]\ttrain-mlogloss:0.22245\teval-mlogloss:0.16489\n",
      "[283]\ttrain-mlogloss:0.22219\teval-mlogloss:0.16433\n",
      "[284]\ttrain-mlogloss:0.22204\teval-mlogloss:0.16436\n",
      "[285]\ttrain-mlogloss:0.22203\teval-mlogloss:0.16432\n",
      "[286]\ttrain-mlogloss:0.22192\teval-mlogloss:0.16419\n",
      "[287]\ttrain-mlogloss:0.22189\teval-mlogloss:0.16418\n",
      "[288]\ttrain-mlogloss:0.22189\teval-mlogloss:0.16419\n",
      "[289]\ttrain-mlogloss:0.22124\teval-mlogloss:0.16351\n",
      "[290]\ttrain-mlogloss:0.22090\teval-mlogloss:0.16307\n",
      "[291]\ttrain-mlogloss:0.22050\teval-mlogloss:0.16256\n",
      "[292]\ttrain-mlogloss:0.22041\teval-mlogloss:0.16249\n",
      "[293]\ttrain-mlogloss:0.22004\teval-mlogloss:0.16206\n",
      "[294]\ttrain-mlogloss:0.22001\teval-mlogloss:0.16201\n",
      "[295]\ttrain-mlogloss:0.21997\teval-mlogloss:0.16197\n",
      "[296]\ttrain-mlogloss:0.21997\teval-mlogloss:0.16197\n",
      "[297]\ttrain-mlogloss:0.21997\teval-mlogloss:0.16197\n",
      "[298]\ttrain-mlogloss:0.21994\teval-mlogloss:0.16194\n",
      "[299]\ttrain-mlogloss:0.21992\teval-mlogloss:0.16199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:54,573] Trial 21 finished with value: 1.0 and parameters: {'lambda': 0.00011580897320385729, 'alpha': 0.7622959259094932, 'eta': 0.0190455022089102, 'gamma': 0.003256101226005533, 'max_depth': 5, 'min_child_weight': 4, 'subsample': 0.40187156456452505, 'colsample_bytree': 0.40721639163191137}. Best is trial 0 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.01532\teval-mlogloss:1.01250\n",
      "[1]\ttrain-mlogloss:0.97208\teval-mlogloss:0.96623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:54,633] Trial 22 pruned. Trial was pruned at iteration 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.05935\teval-mlogloss:1.05730\n",
      "[1]\ttrain-mlogloss:1.03903\teval-mlogloss:1.03643\n",
      "[2]\ttrain-mlogloss:1.01295\teval-mlogloss:1.01355\n",
      "[3]\ttrain-mlogloss:0.97102\teval-mlogloss:0.96836\n",
      "[4]\ttrain-mlogloss:0.95270\teval-mlogloss:0.95083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:54,711] Trial 23 pruned. Trial was pruned at iteration 4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.01081\teval-mlogloss:1.00827\n",
      "[1]\ttrain-mlogloss:0.96012\teval-mlogloss:0.95839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:54,770] Trial 24 pruned. Trial was pruned at iteration 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.94972\teval-mlogloss:0.94342\n",
      "[1]\ttrain-mlogloss:0.87857\teval-mlogloss:0.87813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:54,829] Trial 25 pruned. Trial was pruned at iteration 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.92646\teval-mlogloss:0.92019\n",
      "[1]\ttrain-mlogloss:0.85508\teval-mlogloss:0.84501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:54,904] Trial 26 pruned. Trial was pruned at iteration 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.05488\teval-mlogloss:1.05392\n",
      "[1]\ttrain-mlogloss:1.01372\teval-mlogloss:1.01008\n",
      "[2]\ttrain-mlogloss:0.97460\teval-mlogloss:0.96823\n",
      "[3]\ttrain-mlogloss:0.93650\teval-mlogloss:0.92903\n",
      "[4]\ttrain-mlogloss:0.90230\teval-mlogloss:0.89438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:54,973] Trial 27 pruned. Trial was pruned at iteration 4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.99850\teval-mlogloss:0.99465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:55,032] Trial 28 pruned. Trial was pruned at iteration 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.86012\teval-mlogloss:0.84854\n",
      "[1]\ttrain-mlogloss:0.78083\teval-mlogloss:0.75707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:55,084] Trial 29 pruned. Trial was pruned at iteration 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.92444\teval-mlogloss:0.91514\n",
      "[1]\ttrain-mlogloss:0.84792\teval-mlogloss:0.83582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:55,141] Trial 30 pruned. Trial was pruned at iteration 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.08926\teval-mlogloss:1.08880\n",
      "[1]\ttrain-mlogloss:1.08403\teval-mlogloss:1.08284\n",
      "[2]\ttrain-mlogloss:1.07790\teval-mlogloss:1.07765\n",
      "[3]\ttrain-mlogloss:1.06678\teval-mlogloss:1.06586\n",
      "[4]\ttrain-mlogloss:1.06255\teval-mlogloss:1.06135\n",
      "[5]\ttrain-mlogloss:1.05480\teval-mlogloss:1.05340\n",
      "[6]\ttrain-mlogloss:1.04424\teval-mlogloss:1.04186\n",
      "[7]\ttrain-mlogloss:1.03281\teval-mlogloss:1.02964\n",
      "[8]\ttrain-mlogloss:1.02745\teval-mlogloss:1.02475\n",
      "[9]\ttrain-mlogloss:1.01563\teval-mlogloss:1.01244\n",
      "[10]\ttrain-mlogloss:1.00696\teval-mlogloss:1.00370\n",
      "[11]\ttrain-mlogloss:1.00249\teval-mlogloss:0.99949\n",
      "[12]\ttrain-mlogloss:0.99283\teval-mlogloss:0.98918\n",
      "[13]\ttrain-mlogloss:0.98521\teval-mlogloss:0.98152\n",
      "[14]\ttrain-mlogloss:0.97467\teval-mlogloss:0.97009\n",
      "[15]\ttrain-mlogloss:0.96869\teval-mlogloss:0.96465\n",
      "[16]\ttrain-mlogloss:0.96364\teval-mlogloss:0.95913\n",
      "[17]\ttrain-mlogloss:0.95522\teval-mlogloss:0.95080\n",
      "[18]\ttrain-mlogloss:0.94680\teval-mlogloss:0.94189\n",
      "[19]\ttrain-mlogloss:0.94449\teval-mlogloss:0.93939\n",
      "[20]\ttrain-mlogloss:0.94193\teval-mlogloss:0.93685\n",
      "[21]\ttrain-mlogloss:0.93468\teval-mlogloss:0.92928\n",
      "[22]\ttrain-mlogloss:0.92422\teval-mlogloss:0.91777\n",
      "[23]\ttrain-mlogloss:0.91443\teval-mlogloss:0.90728\n",
      "[24]\ttrain-mlogloss:0.90559\teval-mlogloss:0.89782\n",
      "[25]\ttrain-mlogloss:0.89626\teval-mlogloss:0.88780\n",
      "[26]\ttrain-mlogloss:0.89356\teval-mlogloss:0.88509\n",
      "[27]\ttrain-mlogloss:0.88553\teval-mlogloss:0.87657\n",
      "[28]\ttrain-mlogloss:0.88011\teval-mlogloss:0.87150\n",
      "[29]\ttrain-mlogloss:0.87483\teval-mlogloss:0.86573\n",
      "[30]\ttrain-mlogloss:0.87101\teval-mlogloss:0.86184\n",
      "[31]\ttrain-mlogloss:0.86393\teval-mlogloss:0.85357\n",
      "[32]\ttrain-mlogloss:0.85527\teval-mlogloss:0.84446\n",
      "[33]\ttrain-mlogloss:0.85189\teval-mlogloss:0.84073\n",
      "[34]\ttrain-mlogloss:0.84669\teval-mlogloss:0.83487\n",
      "[35]\ttrain-mlogloss:0.84084\teval-mlogloss:0.82879\n",
      "[36]\ttrain-mlogloss:0.83586\teval-mlogloss:0.82383\n",
      "[37]\ttrain-mlogloss:0.82801\teval-mlogloss:0.81575\n",
      "[38]\ttrain-mlogloss:0.82232\teval-mlogloss:0.80979\n",
      "[39]\ttrain-mlogloss:0.81648\teval-mlogloss:0.80364\n",
      "[40]\ttrain-mlogloss:0.81213\teval-mlogloss:0.79945\n",
      "[41]\ttrain-mlogloss:0.80613\teval-mlogloss:0.79275\n",
      "[42]\ttrain-mlogloss:0.79960\teval-mlogloss:0.78561\n",
      "[43]\ttrain-mlogloss:0.79397\teval-mlogloss:0.77952\n",
      "[44]\ttrain-mlogloss:0.78798\teval-mlogloss:0.77335\n",
      "[45]\ttrain-mlogloss:0.77998\teval-mlogloss:0.76533\n",
      "[46]\ttrain-mlogloss:0.77769\teval-mlogloss:0.76256\n",
      "[47]\ttrain-mlogloss:0.77264\teval-mlogloss:0.75707\n",
      "[48]\ttrain-mlogloss:0.76715\teval-mlogloss:0.75171\n",
      "[49]\ttrain-mlogloss:0.76175\teval-mlogloss:0.74601\n",
      "[50]\ttrain-mlogloss:0.75526\teval-mlogloss:0.73896\n",
      "[51]\ttrain-mlogloss:0.74977\teval-mlogloss:0.73345\n",
      "[52]\ttrain-mlogloss:0.74622\teval-mlogloss:0.72995\n",
      "[53]\ttrain-mlogloss:0.73978\teval-mlogloss:0.72308\n",
      "[54]\ttrain-mlogloss:0.73584\teval-mlogloss:0.71910\n",
      "[55]\ttrain-mlogloss:0.73402\teval-mlogloss:0.71809\n",
      "[56]\ttrain-mlogloss:0.72971\teval-mlogloss:0.71322\n",
      "[57]\ttrain-mlogloss:0.72523\teval-mlogloss:0.70843\n",
      "[58]\ttrain-mlogloss:0.72031\teval-mlogloss:0.70338\n",
      "[59]\ttrain-mlogloss:0.71313\teval-mlogloss:0.69529\n",
      "[60]\ttrain-mlogloss:0.70817\teval-mlogloss:0.69010\n",
      "[61]\ttrain-mlogloss:0.70542\teval-mlogloss:0.68731\n",
      "[62]\ttrain-mlogloss:0.69940\teval-mlogloss:0.68103\n",
      "[63]\ttrain-mlogloss:0.69656\teval-mlogloss:0.67812\n",
      "[64]\ttrain-mlogloss:0.69187\teval-mlogloss:0.67315\n",
      "[65]\ttrain-mlogloss:0.68830\teval-mlogloss:0.66926\n",
      "[66]\ttrain-mlogloss:0.68208\teval-mlogloss:0.66269\n",
      "[67]\ttrain-mlogloss:0.67874\teval-mlogloss:0.65930\n",
      "[68]\ttrain-mlogloss:0.67441\teval-mlogloss:0.65493\n",
      "[69]\ttrain-mlogloss:0.67158\teval-mlogloss:0.65247\n",
      "[70]\ttrain-mlogloss:0.66956\teval-mlogloss:0.65022\n",
      "[71]\ttrain-mlogloss:0.66374\teval-mlogloss:0.64363\n",
      "[72]\ttrain-mlogloss:0.66245\teval-mlogloss:0.64319\n",
      "[73]\ttrain-mlogloss:0.65893\teval-mlogloss:0.63968\n",
      "[74]\ttrain-mlogloss:0.65314\teval-mlogloss:0.63364\n",
      "[75]\ttrain-mlogloss:0.64992\teval-mlogloss:0.63069\n",
      "[76]\ttrain-mlogloss:0.64817\teval-mlogloss:0.62917\n",
      "[77]\ttrain-mlogloss:0.64379\teval-mlogloss:0.62456\n",
      "[78]\ttrain-mlogloss:0.63951\teval-mlogloss:0.62032\n",
      "[79]\ttrain-mlogloss:0.63519\teval-mlogloss:0.61556\n",
      "[80]\ttrain-mlogloss:0.63336\teval-mlogloss:0.61378\n",
      "[81]\ttrain-mlogloss:0.63115\teval-mlogloss:0.61174\n",
      "[82]\ttrain-mlogloss:0.62741\teval-mlogloss:0.60804\n",
      "[83]\ttrain-mlogloss:0.62466\teval-mlogloss:0.60519\n",
      "[84]\ttrain-mlogloss:0.62058\teval-mlogloss:0.60075\n",
      "[85]\ttrain-mlogloss:0.61783\teval-mlogloss:0.59773\n",
      "[86]\ttrain-mlogloss:0.61467\teval-mlogloss:0.59473\n",
      "[87]\ttrain-mlogloss:0.61073\teval-mlogloss:0.59041\n",
      "[88]\ttrain-mlogloss:0.60780\teval-mlogloss:0.58727\n",
      "[89]\ttrain-mlogloss:0.60411\teval-mlogloss:0.58357\n",
      "[90]\ttrain-mlogloss:0.60197\teval-mlogloss:0.58125\n",
      "[91]\ttrain-mlogloss:0.59932\teval-mlogloss:0.57860\n",
      "[92]\ttrain-mlogloss:0.59704\teval-mlogloss:0.57601\n",
      "[93]\ttrain-mlogloss:0.59457\teval-mlogloss:0.57367\n",
      "[94]\ttrain-mlogloss:0.58958\teval-mlogloss:0.56855\n",
      "[95]\ttrain-mlogloss:0.58557\teval-mlogloss:0.56455\n",
      "[96]\ttrain-mlogloss:0.58204\teval-mlogloss:0.56043\n",
      "[97]\ttrain-mlogloss:0.57945\teval-mlogloss:0.55766\n",
      "[98]\ttrain-mlogloss:0.57547\teval-mlogloss:0.55322\n",
      "[99]\ttrain-mlogloss:0.57444\teval-mlogloss:0.55270\n",
      "[100]\ttrain-mlogloss:0.57247\teval-mlogloss:0.55093\n",
      "[101]\ttrain-mlogloss:0.57037\teval-mlogloss:0.54869\n",
      "[102]\ttrain-mlogloss:0.56707\teval-mlogloss:0.54532\n",
      "[103]\ttrain-mlogloss:0.56379\teval-mlogloss:0.54199\n",
      "[104]\ttrain-mlogloss:0.56024\teval-mlogloss:0.53838\n",
      "[105]\ttrain-mlogloss:0.55630\teval-mlogloss:0.53403\n",
      "[106]\ttrain-mlogloss:0.55220\teval-mlogloss:0.52944\n",
      "[107]\ttrain-mlogloss:0.55044\teval-mlogloss:0.52753\n",
      "[108]\ttrain-mlogloss:0.54716\teval-mlogloss:0.52423\n",
      "[109]\ttrain-mlogloss:0.54445\teval-mlogloss:0.52127\n",
      "[110]\ttrain-mlogloss:0.54233\teval-mlogloss:0.51891\n",
      "[111]\ttrain-mlogloss:0.53779\teval-mlogloss:0.51393\n",
      "[112]\ttrain-mlogloss:0.53495\teval-mlogloss:0.51073\n",
      "[113]\ttrain-mlogloss:0.53255\teval-mlogloss:0.50816\n",
      "[114]\ttrain-mlogloss:0.52941\teval-mlogloss:0.50445\n",
      "[115]\ttrain-mlogloss:0.52593\teval-mlogloss:0.50046\n",
      "[116]\ttrain-mlogloss:0.52462\teval-mlogloss:0.49868\n",
      "[117]\ttrain-mlogloss:0.52110\teval-mlogloss:0.49478\n",
      "[118]\ttrain-mlogloss:0.52049\teval-mlogloss:0.49435\n",
      "[119]\ttrain-mlogloss:0.51807\teval-mlogloss:0.49162\n",
      "[120]\ttrain-mlogloss:0.51519\teval-mlogloss:0.48834\n",
      "[121]\ttrain-mlogloss:0.51096\teval-mlogloss:0.48358\n",
      "[122]\ttrain-mlogloss:0.50867\teval-mlogloss:0.48130\n",
      "[123]\ttrain-mlogloss:0.50587\teval-mlogloss:0.47830\n",
      "[124]\ttrain-mlogloss:0.50340\teval-mlogloss:0.47556\n",
      "[125]\ttrain-mlogloss:0.50201\teval-mlogloss:0.47407\n",
      "[126]\ttrain-mlogloss:0.49941\teval-mlogloss:0.47167\n",
      "[127]\ttrain-mlogloss:0.49678\teval-mlogloss:0.46886\n",
      "[128]\ttrain-mlogloss:0.49355\teval-mlogloss:0.46530\n",
      "[129]\ttrain-mlogloss:0.49065\teval-mlogloss:0.46192\n",
      "[130]\ttrain-mlogloss:0.48718\teval-mlogloss:0.45813\n",
      "[131]\ttrain-mlogloss:0.48492\teval-mlogloss:0.45546\n",
      "[132]\ttrain-mlogloss:0.48226\teval-mlogloss:0.45242\n",
      "[133]\ttrain-mlogloss:0.48142\teval-mlogloss:0.45151\n",
      "[134]\ttrain-mlogloss:0.47861\teval-mlogloss:0.44833\n",
      "[135]\ttrain-mlogloss:0.47636\teval-mlogloss:0.44579\n",
      "[136]\ttrain-mlogloss:0.47385\teval-mlogloss:0.44298\n",
      "[137]\ttrain-mlogloss:0.47096\teval-mlogloss:0.43967\n",
      "[138]\ttrain-mlogloss:0.46797\teval-mlogloss:0.43630\n",
      "[139]\ttrain-mlogloss:0.46650\teval-mlogloss:0.43471\n",
      "[140]\ttrain-mlogloss:0.46418\teval-mlogloss:0.43212\n",
      "[141]\ttrain-mlogloss:0.46133\teval-mlogloss:0.42895\n",
      "[142]\ttrain-mlogloss:0.45959\teval-mlogloss:0.42712\n",
      "[143]\ttrain-mlogloss:0.45729\teval-mlogloss:0.42467\n",
      "[144]\ttrain-mlogloss:0.45474\teval-mlogloss:0.42182\n",
      "[145]\ttrain-mlogloss:0.45303\teval-mlogloss:0.41983\n",
      "[146]\ttrain-mlogloss:0.45004\teval-mlogloss:0.41649\n",
      "[147]\ttrain-mlogloss:0.44761\teval-mlogloss:0.41355\n",
      "[148]\ttrain-mlogloss:0.44555\teval-mlogloss:0.41121\n",
      "[149]\ttrain-mlogloss:0.44327\teval-mlogloss:0.40875\n",
      "[150]\ttrain-mlogloss:0.44146\teval-mlogloss:0.40689\n",
      "[151]\ttrain-mlogloss:0.43921\teval-mlogloss:0.40453\n",
      "[152]\ttrain-mlogloss:0.43573\teval-mlogloss:0.40076\n",
      "[153]\ttrain-mlogloss:0.43408\teval-mlogloss:0.39911\n",
      "[154]\ttrain-mlogloss:0.43144\teval-mlogloss:0.39619\n",
      "[155]\ttrain-mlogloss:0.42978\teval-mlogloss:0.39439\n",
      "[156]\ttrain-mlogloss:0.42774\teval-mlogloss:0.39226\n",
      "[157]\ttrain-mlogloss:0.42461\teval-mlogloss:0.38896\n",
      "[158]\ttrain-mlogloss:0.42143\teval-mlogloss:0.38535\n",
      "[159]\ttrain-mlogloss:0.42026\teval-mlogloss:0.38409\n",
      "[160]\ttrain-mlogloss:0.41965\teval-mlogloss:0.38360\n",
      "[161]\ttrain-mlogloss:0.41902\teval-mlogloss:0.38296\n",
      "[162]\ttrain-mlogloss:0.41676\teval-mlogloss:0.38054\n",
      "[163]\ttrain-mlogloss:0.41627\teval-mlogloss:0.38000\n",
      "[164]\ttrain-mlogloss:0.41381\teval-mlogloss:0.37728\n",
      "[165]\ttrain-mlogloss:0.41314\teval-mlogloss:0.37675\n",
      "[166]\ttrain-mlogloss:0.41123\teval-mlogloss:0.37479\n",
      "[167]\ttrain-mlogloss:0.40906\teval-mlogloss:0.37246\n",
      "[168]\ttrain-mlogloss:0.40771\teval-mlogloss:0.37109\n",
      "[169]\ttrain-mlogloss:0.40587\teval-mlogloss:0.36905\n",
      "[170]\ttrain-mlogloss:0.40396\teval-mlogloss:0.36718\n",
      "[171]\ttrain-mlogloss:0.40257\teval-mlogloss:0.36554\n",
      "[172]\ttrain-mlogloss:0.40059\teval-mlogloss:0.36329\n",
      "[173]\ttrain-mlogloss:0.39917\teval-mlogloss:0.36169\n",
      "[174]\ttrain-mlogloss:0.39741\teval-mlogloss:0.35951\n",
      "[175]\ttrain-mlogloss:0.39604\teval-mlogloss:0.35788\n",
      "[176]\ttrain-mlogloss:0.39490\teval-mlogloss:0.35674\n",
      "[177]\ttrain-mlogloss:0.39401\teval-mlogloss:0.35567\n",
      "[178]\ttrain-mlogloss:0.39195\teval-mlogloss:0.35316\n",
      "[179]\ttrain-mlogloss:0.39000\teval-mlogloss:0.35094\n",
      "[180]\ttrain-mlogloss:0.38761\teval-mlogloss:0.34838\n",
      "[181]\ttrain-mlogloss:0.38619\teval-mlogloss:0.34685\n",
      "[182]\ttrain-mlogloss:0.38468\teval-mlogloss:0.34527\n",
      "[183]\ttrain-mlogloss:0.38302\teval-mlogloss:0.34360\n",
      "[184]\ttrain-mlogloss:0.38112\teval-mlogloss:0.34172\n",
      "[185]\ttrain-mlogloss:0.38004\teval-mlogloss:0.34041\n",
      "[186]\ttrain-mlogloss:0.37951\teval-mlogloss:0.33990\n",
      "[187]\ttrain-mlogloss:0.37842\teval-mlogloss:0.33872\n",
      "[188]\ttrain-mlogloss:0.37744\teval-mlogloss:0.33776\n",
      "[189]\ttrain-mlogloss:0.37613\teval-mlogloss:0.33637\n",
      "[190]\ttrain-mlogloss:0.37531\teval-mlogloss:0.33554\n",
      "[191]\ttrain-mlogloss:0.37471\teval-mlogloss:0.33486\n",
      "[192]\ttrain-mlogloss:0.37328\teval-mlogloss:0.33321\n",
      "[193]\ttrain-mlogloss:0.37205\teval-mlogloss:0.33180\n",
      "[194]\ttrain-mlogloss:0.37095\teval-mlogloss:0.33045\n",
      "[195]\ttrain-mlogloss:0.36919\teval-mlogloss:0.32834\n",
      "[196]\ttrain-mlogloss:0.36720\teval-mlogloss:0.32606\n",
      "[197]\ttrain-mlogloss:0.36635\teval-mlogloss:0.32523\n",
      "[198]\ttrain-mlogloss:0.36521\teval-mlogloss:0.32389\n",
      "[199]\ttrain-mlogloss:0.36351\teval-mlogloss:0.32190\n",
      "[200]\ttrain-mlogloss:0.36264\teval-mlogloss:0.32110\n",
      "[201]\ttrain-mlogloss:0.36213\teval-mlogloss:0.32062\n",
      "[202]\ttrain-mlogloss:0.36136\teval-mlogloss:0.32000\n",
      "[203]\ttrain-mlogloss:0.35946\teval-mlogloss:0.31770\n",
      "[204]\ttrain-mlogloss:0.35849\teval-mlogloss:0.31662\n",
      "[205]\ttrain-mlogloss:0.35626\teval-mlogloss:0.31406\n",
      "[206]\ttrain-mlogloss:0.35469\teval-mlogloss:0.31246\n",
      "[207]\ttrain-mlogloss:0.35352\teval-mlogloss:0.31130\n",
      "[208]\ttrain-mlogloss:0.35275\teval-mlogloss:0.31047\n",
      "[209]\ttrain-mlogloss:0.35166\teval-mlogloss:0.30933\n",
      "[210]\ttrain-mlogloss:0.35149\teval-mlogloss:0.30922\n",
      "[211]\ttrain-mlogloss:0.35004\teval-mlogloss:0.30775\n",
      "[212]\ttrain-mlogloss:0.34860\teval-mlogloss:0.30595\n",
      "[213]\ttrain-mlogloss:0.34692\teval-mlogloss:0.30412\n",
      "[214]\ttrain-mlogloss:0.34517\teval-mlogloss:0.30232\n",
      "[215]\ttrain-mlogloss:0.34410\teval-mlogloss:0.30138\n",
      "[216]\ttrain-mlogloss:0.34238\teval-mlogloss:0.29968\n",
      "[217]\ttrain-mlogloss:0.34084\teval-mlogloss:0.29796\n",
      "[218]\ttrain-mlogloss:0.33983\teval-mlogloss:0.29679\n",
      "[219]\ttrain-mlogloss:0.33838\teval-mlogloss:0.29507\n",
      "[220]\ttrain-mlogloss:0.33672\teval-mlogloss:0.29323\n",
      "[221]\ttrain-mlogloss:0.33628\teval-mlogloss:0.29266\n",
      "[222]\ttrain-mlogloss:0.33436\teval-mlogloss:0.29055\n",
      "[223]\ttrain-mlogloss:0.33351\teval-mlogloss:0.28956\n",
      "[224]\ttrain-mlogloss:0.33206\teval-mlogloss:0.28795\n",
      "[225]\ttrain-mlogloss:0.33162\teval-mlogloss:0.28739\n",
      "[226]\ttrain-mlogloss:0.33090\teval-mlogloss:0.28665\n",
      "[227]\ttrain-mlogloss:0.32938\teval-mlogloss:0.28504\n",
      "[228]\ttrain-mlogloss:0.32888\teval-mlogloss:0.28456\n",
      "[229]\ttrain-mlogloss:0.32852\teval-mlogloss:0.28420\n",
      "[230]\ttrain-mlogloss:0.32742\teval-mlogloss:0.28307\n",
      "[231]\ttrain-mlogloss:0.32646\teval-mlogloss:0.28196\n",
      "[232]\ttrain-mlogloss:0.32592\teval-mlogloss:0.28141\n",
      "[233]\ttrain-mlogloss:0.32476\teval-mlogloss:0.28027\n",
      "[234]\ttrain-mlogloss:0.32297\teval-mlogloss:0.27830\n",
      "[235]\ttrain-mlogloss:0.32225\teval-mlogloss:0.27760\n",
      "[236]\ttrain-mlogloss:0.32094\teval-mlogloss:0.27627\n",
      "[237]\ttrain-mlogloss:0.31972\teval-mlogloss:0.27509\n",
      "[238]\ttrain-mlogloss:0.31929\teval-mlogloss:0.27460\n",
      "[239]\ttrain-mlogloss:0.31848\teval-mlogloss:0.27368\n",
      "[240]\ttrain-mlogloss:0.31757\teval-mlogloss:0.27279\n",
      "[241]\ttrain-mlogloss:0.31690\teval-mlogloss:0.27217\n",
      "[242]\ttrain-mlogloss:0.31669\teval-mlogloss:0.27210\n",
      "[243]\ttrain-mlogloss:0.31605\teval-mlogloss:0.27139\n",
      "[244]\ttrain-mlogloss:0.31526\teval-mlogloss:0.27062\n",
      "[245]\ttrain-mlogloss:0.31425\teval-mlogloss:0.26952\n",
      "[246]\ttrain-mlogloss:0.31408\teval-mlogloss:0.26945\n",
      "[247]\ttrain-mlogloss:0.31310\teval-mlogloss:0.26822\n",
      "[248]\ttrain-mlogloss:0.31181\teval-mlogloss:0.26668\n",
      "[249]\ttrain-mlogloss:0.31069\teval-mlogloss:0.26531\n",
      "[250]\ttrain-mlogloss:0.30984\teval-mlogloss:0.26433\n",
      "[251]\ttrain-mlogloss:0.30945\teval-mlogloss:0.26384\n",
      "[252]\ttrain-mlogloss:0.30852\teval-mlogloss:0.26273\n",
      "[253]\ttrain-mlogloss:0.30824\teval-mlogloss:0.26253\n",
      "[254]\ttrain-mlogloss:0.30695\teval-mlogloss:0.26105\n",
      "[255]\ttrain-mlogloss:0.30630\teval-mlogloss:0.26035\n",
      "[256]\ttrain-mlogloss:0.30550\teval-mlogloss:0.25952\n",
      "[257]\ttrain-mlogloss:0.30388\teval-mlogloss:0.25770\n",
      "[258]\ttrain-mlogloss:0.30315\teval-mlogloss:0.25691\n",
      "[259]\ttrain-mlogloss:0.30185\teval-mlogloss:0.25558\n",
      "[260]\ttrain-mlogloss:0.30072\teval-mlogloss:0.25434\n",
      "[261]\ttrain-mlogloss:0.29909\teval-mlogloss:0.25254\n",
      "[262]\ttrain-mlogloss:0.29893\teval-mlogloss:0.25248\n",
      "[263]\ttrain-mlogloss:0.29761\teval-mlogloss:0.25105\n",
      "[264]\ttrain-mlogloss:0.29665\teval-mlogloss:0.24988\n",
      "[265]\ttrain-mlogloss:0.29593\teval-mlogloss:0.24916\n",
      "[266]\ttrain-mlogloss:0.29531\teval-mlogloss:0.24837\n",
      "[267]\ttrain-mlogloss:0.29498\teval-mlogloss:0.24797\n",
      "[268]\ttrain-mlogloss:0.29362\teval-mlogloss:0.24653\n",
      "[269]\ttrain-mlogloss:0.29299\teval-mlogloss:0.24598\n",
      "[270]\ttrain-mlogloss:0.29193\teval-mlogloss:0.24456\n",
      "[271]\ttrain-mlogloss:0.29087\teval-mlogloss:0.24319\n",
      "[272]\ttrain-mlogloss:0.29010\teval-mlogloss:0.24241\n",
      "[273]\ttrain-mlogloss:0.28929\teval-mlogloss:0.24153\n",
      "[274]\ttrain-mlogloss:0.28852\teval-mlogloss:0.24076\n",
      "[275]\ttrain-mlogloss:0.28745\teval-mlogloss:0.23962\n",
      "[276]\ttrain-mlogloss:0.28709\teval-mlogloss:0.23922\n",
      "[277]\ttrain-mlogloss:0.28611\teval-mlogloss:0.23806\n",
      "[278]\ttrain-mlogloss:0.28560\teval-mlogloss:0.23756\n",
      "[279]\ttrain-mlogloss:0.28499\teval-mlogloss:0.23677\n",
      "[280]\ttrain-mlogloss:0.28432\teval-mlogloss:0.23582\n",
      "[281]\ttrain-mlogloss:0.28379\teval-mlogloss:0.23528\n",
      "[282]\ttrain-mlogloss:0.28274\teval-mlogloss:0.23402\n",
      "[283]\ttrain-mlogloss:0.28194\teval-mlogloss:0.23300\n",
      "[284]\ttrain-mlogloss:0.28143\teval-mlogloss:0.23257\n",
      "[285]\ttrain-mlogloss:0.28138\teval-mlogloss:0.23249\n",
      "[286]\ttrain-mlogloss:0.28018\teval-mlogloss:0.23109\n",
      "[287]\ttrain-mlogloss:0.27992\teval-mlogloss:0.23083\n",
      "[288]\ttrain-mlogloss:0.27897\teval-mlogloss:0.22985\n",
      "[289]\ttrain-mlogloss:0.27808\teval-mlogloss:0.22894\n",
      "[290]\ttrain-mlogloss:0.27722\teval-mlogloss:0.22798\n",
      "[291]\ttrain-mlogloss:0.27621\teval-mlogloss:0.22688\n",
      "[292]\ttrain-mlogloss:0.27599\teval-mlogloss:0.22664\n",
      "[293]\ttrain-mlogloss:0.27541\teval-mlogloss:0.22597\n",
      "[294]\ttrain-mlogloss:0.27520\teval-mlogloss:0.22571\n",
      "[295]\ttrain-mlogloss:0.27428\teval-mlogloss:0.22474\n",
      "[296]\ttrain-mlogloss:0.27392\teval-mlogloss:0.22437\n",
      "[297]\ttrain-mlogloss:0.27286\teval-mlogloss:0.22322\n",
      "[298]\ttrain-mlogloss:0.27238\teval-mlogloss:0.22268\n",
      "[299]\ttrain-mlogloss:0.27209\teval-mlogloss:0.22256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:56,710] Trial 31 finished with value: 1.0 and parameters: {'lambda': 0.00021030204768717305, 'alpha': 0.6895098515642172, 'eta': 0.010848191249761628, 'gamma': 0.0046844865250825225, 'max_depth': 5, 'min_child_weight': 4, 'subsample': 0.404053560014979, 'colsample_bytree': 0.41165625583462245}. Best is trial 0 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.07517\teval-mlogloss:1.07444\n",
      "[1]\ttrain-mlogloss:1.06185\teval-mlogloss:1.06127\n",
      "[2]\ttrain-mlogloss:1.04624\teval-mlogloss:1.04813\n",
      "[3]\ttrain-mlogloss:1.02075\teval-mlogloss:1.02024\n",
      "[4]\ttrain-mlogloss:1.01002\teval-mlogloss:1.00957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:56,773] Trial 32 pruned. Trial was pruned at iteration 4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.06886\teval-mlogloss:1.06779\n",
      "[1]\ttrain-mlogloss:1.05438\teval-mlogloss:1.05135\n",
      "[2]\ttrain-mlogloss:1.03080\teval-mlogloss:1.02644\n",
      "[3]\ttrain-mlogloss:1.00194\teval-mlogloss:0.99585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:56,832] Trial 33 pruned. Trial was pruned at iteration 4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.03564\teval-mlogloss:1.03339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:56,882] Trial 34 pruned. Trial was pruned at iteration 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.00465\teval-mlogloss:1.00202\n",
      "[1]\ttrain-mlogloss:0.95554\teval-mlogloss:0.95593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:56,928] Trial 35 pruned. Trial was pruned at iteration 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.04691\teval-mlogloss:1.04774\n",
      "[1]\ttrain-mlogloss:1.01917\teval-mlogloss:1.02300\n",
      "[2]\ttrain-mlogloss:0.98664\teval-mlogloss:0.99640\n",
      "[3]\ttrain-mlogloss:0.93675\teval-mlogloss:0.94239\n",
      "[4]\ttrain-mlogloss:0.91408\teval-mlogloss:0.92002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:56,990] Trial 36 pruned. Trial was pruned at iteration 4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.94173\teval-mlogloss:0.93406\n",
      "[1]\ttrain-mlogloss:0.87074\teval-mlogloss:0.85941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:57,030] Trial 37 pruned. Trial was pruned at iteration 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.97537\teval-mlogloss:0.97619\n",
      "[1]\ttrain-mlogloss:0.86915\teval-mlogloss:0.86186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:57,088] Trial 38 pruned. Trial was pruned at iteration 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.07636\teval-mlogloss:1.07515\n",
      "[1]\ttrain-mlogloss:1.06457\teval-mlogloss:1.06346\n",
      "[2]\ttrain-mlogloss:1.04536\teval-mlogloss:1.04297\n",
      "[3]\ttrain-mlogloss:1.02359\teval-mlogloss:1.02021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:57,147] Trial 39 pruned. Trial was pruned at iteration 4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.04692\teval-mlogloss:1.04536\n",
      "[1]\ttrain-mlogloss:1.01335\teval-mlogloss:1.01864\n",
      "[2]\ttrain-mlogloss:0.96866\teval-mlogloss:0.97194\n",
      "[3]\ttrain-mlogloss:0.92430\teval-mlogloss:0.92471\n",
      "[4]\ttrain-mlogloss:0.88282\teval-mlogloss:0.88255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:57,214] Trial 40 pruned. Trial was pruned at iteration 4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.08847\teval-mlogloss:1.08796\n",
      "[1]\ttrain-mlogloss:1.08227\teval-mlogloss:1.08119\n",
      "[2]\ttrain-mlogloss:1.07543\teval-mlogloss:1.07540\n",
      "[3]\ttrain-mlogloss:1.06334\teval-mlogloss:1.06257\n",
      "[4]\ttrain-mlogloss:1.05869\teval-mlogloss:1.05754\n",
      "[5]\ttrain-mlogloss:1.05041\teval-mlogloss:1.04901\n",
      "[6]\ttrain-mlogloss:1.03863\teval-mlogloss:1.03612\n",
      "[7]\ttrain-mlogloss:1.02627\teval-mlogloss:1.02289\n",
      "[8]\ttrain-mlogloss:1.02043\teval-mlogloss:1.01762\n",
      "[9]\ttrain-mlogloss:1.00771\teval-mlogloss:1.00435\n",
      "[10]\ttrain-mlogloss:0.99799\teval-mlogloss:0.99431\n",
      "[11]\ttrain-mlogloss:0.99304\teval-mlogloss:0.99032\n",
      "[12]\ttrain-mlogloss:0.98247\teval-mlogloss:0.97883\n",
      "[13]\ttrain-mlogloss:0.97425\teval-mlogloss:0.97056\n",
      "[14]\ttrain-mlogloss:0.96289\teval-mlogloss:0.95821\n",
      "[15]\ttrain-mlogloss:0.95642\teval-mlogloss:0.95232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:57,315] Trial 41 pruned. Trial was pruned at iteration 16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.08967\teval-mlogloss:1.08922\n",
      "[1]\ttrain-mlogloss:1.08509\teval-mlogloss:1.08361\n",
      "[2]\ttrain-mlogloss:1.07940\teval-mlogloss:1.07862\n",
      "[3]\ttrain-mlogloss:1.06950\teval-mlogloss:1.06785\n",
      "[4]\ttrain-mlogloss:1.06511\teval-mlogloss:1.06375\n",
      "[5]\ttrain-mlogloss:1.05783\teval-mlogloss:1.05648\n",
      "[6]\ttrain-mlogloss:1.04750\teval-mlogloss:1.04520\n",
      "[7]\ttrain-mlogloss:1.03664\teval-mlogloss:1.03338\n",
      "[8]\ttrain-mlogloss:1.03156\teval-mlogloss:1.02867\n",
      "[9]\ttrain-mlogloss:1.02032\teval-mlogloss:1.01696\n",
      "[10]\ttrain-mlogloss:1.01246\teval-mlogloss:1.00902\n",
      "[11]\ttrain-mlogloss:1.00824\teval-mlogloss:1.00568\n",
      "[12]\ttrain-mlogloss:0.99893\teval-mlogloss:0.99556\n",
      "[13]\ttrain-mlogloss:0.99156\teval-mlogloss:0.98796\n",
      "[14]\ttrain-mlogloss:0.98146\teval-mlogloss:0.97704\n",
      "[15]\ttrain-mlogloss:0.97591\teval-mlogloss:0.97224\n",
      "[16]\ttrain-mlogloss:0.97116\teval-mlogloss:0.96701\n",
      "[17]\ttrain-mlogloss:0.96319\teval-mlogloss:0.95937\n",
      "[18]\ttrain-mlogloss:0.95502\teval-mlogloss:0.95075\n",
      "[19]\ttrain-mlogloss:0.95269\teval-mlogloss:0.94848\n",
      "[20]\ttrain-mlogloss:0.95029\teval-mlogloss:0.94617\n",
      "[21]\ttrain-mlogloss:0.94327\teval-mlogloss:0.93898\n",
      "[22]\ttrain-mlogloss:0.93356\teval-mlogloss:0.92824\n",
      "[23]\ttrain-mlogloss:0.92393\teval-mlogloss:0.91809\n",
      "[24]\ttrain-mlogloss:0.91580\teval-mlogloss:0.90949\n",
      "[25]\ttrain-mlogloss:0.90677\teval-mlogloss:0.89978\n",
      "[26]\ttrain-mlogloss:0.90443\teval-mlogloss:0.89752\n",
      "[27]\ttrain-mlogloss:0.89693\teval-mlogloss:0.88962\n",
      "[28]\ttrain-mlogloss:0.89175\teval-mlogloss:0.88489\n",
      "[29]\ttrain-mlogloss:0.88687\teval-mlogloss:0.87982\n",
      "[30]\ttrain-mlogloss:0.88282\teval-mlogloss:0.87590\n",
      "[31]\ttrain-mlogloss:0.87596\teval-mlogloss:0.86857\n",
      "[32]\ttrain-mlogloss:0.86741\teval-mlogloss:0.85973\n",
      "[33]\ttrain-mlogloss:0.86437\teval-mlogloss:0.85634\n",
      "[34]\ttrain-mlogloss:0.85966\teval-mlogloss:0.85097\n",
      "[35]\ttrain-mlogloss:0.85422\teval-mlogloss:0.84553\n",
      "[36]\ttrain-mlogloss:0.84952\teval-mlogloss:0.84146\n",
      "[37]\ttrain-mlogloss:0.84141\teval-mlogloss:0.83280\n",
      "[38]\ttrain-mlogloss:0.83587\teval-mlogloss:0.82715\n",
      "[39]\ttrain-mlogloss:0.83006\teval-mlogloss:0.82102\n",
      "[40]\ttrain-mlogloss:0.82586\teval-mlogloss:0.81698\n",
      "[41]\ttrain-mlogloss:0.82009\teval-mlogloss:0.81053\n",
      "[42]\ttrain-mlogloss:0.81448\teval-mlogloss:0.80444\n",
      "[43]\ttrain-mlogloss:0.80894\teval-mlogloss:0.79870\n",
      "[44]\ttrain-mlogloss:0.80294\teval-mlogloss:0.79243\n",
      "[45]\ttrain-mlogloss:0.79517\teval-mlogloss:0.78452\n",
      "[46]\ttrain-mlogloss:0.79302\teval-mlogloss:0.78189\n",
      "[47]\ttrain-mlogloss:0.78834\teval-mlogloss:0.77659\n",
      "[48]\ttrain-mlogloss:0.78318\teval-mlogloss:0.77151\n",
      "[49]\ttrain-mlogloss:0.77794\teval-mlogloss:0.76610\n",
      "[50]\ttrain-mlogloss:0.77168\teval-mlogloss:0.75930\n",
      "[51]\ttrain-mlogloss:0.76620\teval-mlogloss:0.75347\n",
      "[52]\ttrain-mlogloss:0.76272\teval-mlogloss:0.75012\n",
      "[53]\ttrain-mlogloss:0.75616\teval-mlogloss:0.74330\n",
      "[54]\ttrain-mlogloss:0.75234\teval-mlogloss:0.73930\n",
      "[55]\ttrain-mlogloss:0.75070\teval-mlogloss:0.73842\n",
      "[56]\ttrain-mlogloss:0.74663\teval-mlogloss:0.73382\n",
      "[57]\ttrain-mlogloss:0.74209\teval-mlogloss:0.72926\n",
      "[58]\ttrain-mlogloss:0.73713\teval-mlogloss:0.72404\n",
      "[59]\ttrain-mlogloss:0.73024\teval-mlogloss:0.71628\n",
      "[60]\ttrain-mlogloss:0.72522\teval-mlogloss:0.71097\n",
      "[61]\ttrain-mlogloss:0.72233\teval-mlogloss:0.70806\n",
      "[62]\ttrain-mlogloss:0.71625\teval-mlogloss:0.70149\n",
      "[63]\ttrain-mlogloss:0.71359\teval-mlogloss:0.69876\n",
      "[64]\ttrain-mlogloss:0.70906\teval-mlogloss:0.69395\n",
      "[65]\ttrain-mlogloss:0.70552\teval-mlogloss:0.69008\n",
      "[66]\ttrain-mlogloss:0.69931\teval-mlogloss:0.68328\n",
      "[67]\ttrain-mlogloss:0.69606\teval-mlogloss:0.68007\n",
      "[68]\ttrain-mlogloss:0.69161\teval-mlogloss:0.67523\n",
      "[69]\ttrain-mlogloss:0.68877\teval-mlogloss:0.67268\n",
      "[70]\ttrain-mlogloss:0.68676\teval-mlogloss:0.67044\n",
      "[71]\ttrain-mlogloss:0.68067\teval-mlogloss:0.66377\n",
      "[72]\ttrain-mlogloss:0.67947\teval-mlogloss:0.66295\n",
      "[73]\ttrain-mlogloss:0.67615\teval-mlogloss:0.65961\n",
      "[74]\ttrain-mlogloss:0.67019\teval-mlogloss:0.65347\n",
      "[75]\ttrain-mlogloss:0.66723\teval-mlogloss:0.65014\n",
      "[76]\ttrain-mlogloss:0.66563\teval-mlogloss:0.64864\n",
      "[77]\ttrain-mlogloss:0.66136\teval-mlogloss:0.64418\n",
      "[78]\ttrain-mlogloss:0.65721\teval-mlogloss:0.64005\n",
      "[79]\ttrain-mlogloss:0.65318\teval-mlogloss:0.63544\n",
      "[80]\ttrain-mlogloss:0.65157\teval-mlogloss:0.63385\n",
      "[81]\ttrain-mlogloss:0.64945\teval-mlogloss:0.63185\n",
      "[82]\ttrain-mlogloss:0.64586\teval-mlogloss:0.62826\n",
      "[83]\ttrain-mlogloss:0.64327\teval-mlogloss:0.62559\n",
      "[84]\ttrain-mlogloss:0.63945\teval-mlogloss:0.62151\n",
      "[85]\ttrain-mlogloss:0.63671\teval-mlogloss:0.61851\n",
      "[86]\ttrain-mlogloss:0.63370\teval-mlogloss:0.61547\n",
      "[87]\ttrain-mlogloss:0.62991\teval-mlogloss:0.61136\n",
      "[88]\ttrain-mlogloss:0.62693\teval-mlogloss:0.60820\n",
      "[89]\ttrain-mlogloss:0.62331\teval-mlogloss:0.60454\n",
      "[90]\ttrain-mlogloss:0.62137\teval-mlogloss:0.60229\n",
      "[91]\ttrain-mlogloss:0.61900\teval-mlogloss:0.59993\n",
      "[92]\ttrain-mlogloss:0.61683\teval-mlogloss:0.59767\n",
      "[93]\ttrain-mlogloss:0.61454\teval-mlogloss:0.59546\n",
      "[94]\ttrain-mlogloss:0.60949\teval-mlogloss:0.59036\n",
      "[95]\ttrain-mlogloss:0.60571\teval-mlogloss:0.58658\n",
      "[96]\ttrain-mlogloss:0.60233\teval-mlogloss:0.58303\n",
      "[97]\ttrain-mlogloss:0.59976\teval-mlogloss:0.58026\n",
      "[98]\ttrain-mlogloss:0.59590\teval-mlogloss:0.57602\n",
      "[99]\ttrain-mlogloss:0.59504\teval-mlogloss:0.57549\n",
      "[100]\ttrain-mlogloss:0.59310\teval-mlogloss:0.57371\n",
      "[101]\ttrain-mlogloss:0.59107\teval-mlogloss:0.57154\n",
      "[102]\ttrain-mlogloss:0.58787\teval-mlogloss:0.56823\n",
      "[103]\ttrain-mlogloss:0.58455\teval-mlogloss:0.56469\n",
      "[104]\ttrain-mlogloss:0.58108\teval-mlogloss:0.56112\n",
      "[105]\ttrain-mlogloss:0.57745\teval-mlogloss:0.55710\n",
      "[106]\ttrain-mlogloss:0.57334\teval-mlogloss:0.55243\n",
      "[107]\ttrain-mlogloss:0.57165\teval-mlogloss:0.55060\n",
      "[108]\ttrain-mlogloss:0.56856\teval-mlogloss:0.54740\n",
      "[109]\ttrain-mlogloss:0.56584\teval-mlogloss:0.54448\n",
      "[110]\ttrain-mlogloss:0.56373\teval-mlogloss:0.54200\n",
      "[111]\ttrain-mlogloss:0.55918\teval-mlogloss:0.53690\n",
      "[112]\ttrain-mlogloss:0.55649\teval-mlogloss:0.53428\n",
      "[113]\ttrain-mlogloss:0.55354\teval-mlogloss:0.53080\n",
      "[114]\ttrain-mlogloss:0.55040\teval-mlogloss:0.52709\n",
      "[115]\ttrain-mlogloss:0.54701\teval-mlogloss:0.52320\n",
      "[116]\ttrain-mlogloss:0.54562\teval-mlogloss:0.52170\n",
      "[117]\ttrain-mlogloss:0.54226\teval-mlogloss:0.51800\n",
      "[118]\ttrain-mlogloss:0.54165\teval-mlogloss:0.51757\n",
      "[119]\ttrain-mlogloss:0.53901\teval-mlogloss:0.51469\n",
      "[120]\ttrain-mlogloss:0.53575\teval-mlogloss:0.51110\n",
      "[121]\ttrain-mlogloss:0.53160\teval-mlogloss:0.50651\n",
      "[122]\ttrain-mlogloss:0.52937\teval-mlogloss:0.50436\n",
      "[123]\ttrain-mlogloss:0.52658\teval-mlogloss:0.50138\n",
      "[124]\ttrain-mlogloss:0.52412\teval-mlogloss:0.49884\n",
      "[125]\ttrain-mlogloss:0.52268\teval-mlogloss:0.49733\n",
      "[126]\ttrain-mlogloss:0.52004\teval-mlogloss:0.49461\n",
      "[127]\ttrain-mlogloss:0.51759\teval-mlogloss:0.49204\n",
      "[128]\ttrain-mlogloss:0.51435\teval-mlogloss:0.48834\n",
      "[129]\ttrain-mlogloss:0.51126\teval-mlogloss:0.48482\n",
      "[130]\ttrain-mlogloss:0.50785\teval-mlogloss:0.48098\n",
      "[131]\ttrain-mlogloss:0.50565\teval-mlogloss:0.47842\n",
      "[132]\ttrain-mlogloss:0.50263\teval-mlogloss:0.47519\n",
      "[133]\ttrain-mlogloss:0.50188\teval-mlogloss:0.47443\n",
      "[134]\ttrain-mlogloss:0.49932\teval-mlogloss:0.47159\n",
      "[135]\ttrain-mlogloss:0.49712\teval-mlogloss:0.46895\n",
      "[136]\ttrain-mlogloss:0.49464\teval-mlogloss:0.46616\n",
      "[137]\ttrain-mlogloss:0.49183\teval-mlogloss:0.46304\n",
      "[138]\ttrain-mlogloss:0.48862\teval-mlogloss:0.45958\n",
      "[139]\ttrain-mlogloss:0.48725\teval-mlogloss:0.45808\n",
      "[140]\ttrain-mlogloss:0.48493\teval-mlogloss:0.45548\n",
      "[141]\ttrain-mlogloss:0.48221\teval-mlogloss:0.45247\n",
      "[142]\ttrain-mlogloss:0.48047\teval-mlogloss:0.45068\n",
      "[143]\ttrain-mlogloss:0.47819\teval-mlogloss:0.44823\n",
      "[144]\ttrain-mlogloss:0.47565\teval-mlogloss:0.44537\n",
      "[145]\ttrain-mlogloss:0.47396\teval-mlogloss:0.44337\n",
      "[146]\ttrain-mlogloss:0.47092\teval-mlogloss:0.43991\n",
      "[147]\ttrain-mlogloss:0.46850\teval-mlogloss:0.43713\n",
      "[148]\ttrain-mlogloss:0.46582\teval-mlogloss:0.43408\n",
      "[149]\ttrain-mlogloss:0.46367\teval-mlogloss:0.43181\n",
      "[150]\ttrain-mlogloss:0.46188\teval-mlogloss:0.42999\n",
      "[151]\ttrain-mlogloss:0.45968\teval-mlogloss:0.42769\n",
      "[152]\ttrain-mlogloss:0.45634\teval-mlogloss:0.42400\n",
      "[153]\ttrain-mlogloss:0.45474\teval-mlogloss:0.42237\n",
      "[154]\ttrain-mlogloss:0.45179\teval-mlogloss:0.41928\n",
      "[155]\ttrain-mlogloss:0.44997\teval-mlogloss:0.41717\n",
      "[156]\ttrain-mlogloss:0.44799\teval-mlogloss:0.41511\n",
      "[157]\ttrain-mlogloss:0.44495\teval-mlogloss:0.41191\n",
      "[158]\ttrain-mlogloss:0.44172\teval-mlogloss:0.40835\n",
      "[159]\ttrain-mlogloss:0.44055\teval-mlogloss:0.40715\n",
      "[160]\ttrain-mlogloss:0.43991\teval-mlogloss:0.40661\n",
      "[161]\ttrain-mlogloss:0.43924\teval-mlogloss:0.40597\n",
      "[162]\ttrain-mlogloss:0.43702\teval-mlogloss:0.40357\n",
      "[163]\ttrain-mlogloss:0.43651\teval-mlogloss:0.40314\n",
      "[164]\ttrain-mlogloss:0.43392\teval-mlogloss:0.40031\n",
      "[165]\ttrain-mlogloss:0.43310\teval-mlogloss:0.39961\n",
      "[166]\ttrain-mlogloss:0.43115\teval-mlogloss:0.39750\n",
      "[167]\ttrain-mlogloss:0.42887\teval-mlogloss:0.39495\n",
      "[168]\ttrain-mlogloss:0.42668\teval-mlogloss:0.39271\n",
      "[169]\ttrain-mlogloss:0.42472\teval-mlogloss:0.39049\n",
      "[170]\ttrain-mlogloss:0.42267\teval-mlogloss:0.38846\n",
      "[171]\ttrain-mlogloss:0.42118\teval-mlogloss:0.38688\n",
      "[172]\ttrain-mlogloss:0.41918\teval-mlogloss:0.38473\n",
      "[173]\ttrain-mlogloss:0.41775\teval-mlogloss:0.38321\n",
      "[174]\ttrain-mlogloss:0.41610\teval-mlogloss:0.38120\n",
      "[175]\ttrain-mlogloss:0.41465\teval-mlogloss:0.37949\n",
      "[176]\ttrain-mlogloss:0.41356\teval-mlogloss:0.37846\n",
      "[177]\ttrain-mlogloss:0.41278\teval-mlogloss:0.37756\n",
      "[178]\ttrain-mlogloss:0.41050\teval-mlogloss:0.37504\n",
      "[179]\ttrain-mlogloss:0.40847\teval-mlogloss:0.37278\n",
      "[180]\ttrain-mlogloss:0.40602\teval-mlogloss:0.37004\n",
      "[181]\ttrain-mlogloss:0.40439\teval-mlogloss:0.36834\n",
      "[182]\ttrain-mlogloss:0.40284\teval-mlogloss:0.36671\n",
      "[183]\ttrain-mlogloss:0.40110\teval-mlogloss:0.36496\n",
      "[184]\ttrain-mlogloss:0.39929\teval-mlogloss:0.36316\n",
      "[185]\ttrain-mlogloss:0.39794\teval-mlogloss:0.36168\n",
      "[186]\ttrain-mlogloss:0.39738\teval-mlogloss:0.36117\n",
      "[187]\ttrain-mlogloss:0.39621\teval-mlogloss:0.35997\n",
      "[188]\ttrain-mlogloss:0.39527\teval-mlogloss:0.35900\n",
      "[189]\ttrain-mlogloss:0.39407\teval-mlogloss:0.35770\n",
      "[190]\ttrain-mlogloss:0.39269\teval-mlogloss:0.35617\n",
      "[191]\ttrain-mlogloss:0.39204\teval-mlogloss:0.35542\n",
      "[192]\ttrain-mlogloss:0.39035\teval-mlogloss:0.35366\n",
      "[193]\ttrain-mlogloss:0.38907\teval-mlogloss:0.35235\n",
      "[194]\ttrain-mlogloss:0.38807\teval-mlogloss:0.35124\n",
      "[195]\ttrain-mlogloss:0.38612\teval-mlogloss:0.34895\n",
      "[196]\ttrain-mlogloss:0.38400\teval-mlogloss:0.34644\n",
      "[197]\ttrain-mlogloss:0.38303\teval-mlogloss:0.34536\n",
      "[198]\ttrain-mlogloss:0.38167\teval-mlogloss:0.34387\n",
      "[199]\ttrain-mlogloss:0.38003\teval-mlogloss:0.34201\n",
      "[200]\ttrain-mlogloss:0.37910\teval-mlogloss:0.34117\n",
      "[201]\ttrain-mlogloss:0.37860\teval-mlogloss:0.34054\n",
      "[202]\ttrain-mlogloss:0.37763\teval-mlogloss:0.33958\n",
      "[203]\ttrain-mlogloss:0.37549\teval-mlogloss:0.33708\n",
      "[204]\ttrain-mlogloss:0.37449\teval-mlogloss:0.33597\n",
      "[205]\ttrain-mlogloss:0.37225\teval-mlogloss:0.33343\n",
      "[206]\ttrain-mlogloss:0.37064\teval-mlogloss:0.33173\n",
      "[207]\ttrain-mlogloss:0.36959\teval-mlogloss:0.33063\n",
      "[208]\ttrain-mlogloss:0.36865\teval-mlogloss:0.32966\n",
      "[209]\ttrain-mlogloss:0.36746\teval-mlogloss:0.32848\n",
      "[210]\ttrain-mlogloss:0.36724\teval-mlogloss:0.32835\n",
      "[211]\ttrain-mlogloss:0.36584\teval-mlogloss:0.32691\n",
      "[212]\ttrain-mlogloss:0.36441\teval-mlogloss:0.32520\n",
      "[213]\ttrain-mlogloss:0.36274\teval-mlogloss:0.32338\n",
      "[214]\ttrain-mlogloss:0.36088\teval-mlogloss:0.32144\n",
      "[215]\ttrain-mlogloss:0.35987\teval-mlogloss:0.32051\n",
      "[216]\ttrain-mlogloss:0.35815\teval-mlogloss:0.31869\n",
      "[217]\ttrain-mlogloss:0.35663\teval-mlogloss:0.31694\n",
      "[218]\ttrain-mlogloss:0.35544\teval-mlogloss:0.31555\n",
      "[219]\ttrain-mlogloss:0.35398\teval-mlogloss:0.31390\n",
      "[220]\ttrain-mlogloss:0.35228\teval-mlogloss:0.31187\n",
      "[221]\ttrain-mlogloss:0.35177\teval-mlogloss:0.31133\n",
      "[222]\ttrain-mlogloss:0.34966\teval-mlogloss:0.30887\n",
      "[223]\ttrain-mlogloss:0.34873\teval-mlogloss:0.30773\n",
      "[224]\ttrain-mlogloss:0.34668\teval-mlogloss:0.30547\n",
      "[225]\ttrain-mlogloss:0.34581\teval-mlogloss:0.30461\n",
      "[226]\ttrain-mlogloss:0.34515\teval-mlogloss:0.30391\n",
      "[227]\ttrain-mlogloss:0.34368\teval-mlogloss:0.30230\n",
      "[228]\ttrain-mlogloss:0.34305\teval-mlogloss:0.30159\n",
      "[229]\ttrain-mlogloss:0.34271\teval-mlogloss:0.30124\n",
      "[230]\ttrain-mlogloss:0.34158\teval-mlogloss:0.30006\n",
      "[231]\ttrain-mlogloss:0.34037\teval-mlogloss:0.29870\n",
      "[232]\ttrain-mlogloss:0.33969\teval-mlogloss:0.29798\n",
      "[233]\ttrain-mlogloss:0.33847\teval-mlogloss:0.29662\n",
      "[234]\ttrain-mlogloss:0.33665\teval-mlogloss:0.29458\n",
      "[235]\ttrain-mlogloss:0.33581\teval-mlogloss:0.29369\n",
      "[236]\ttrain-mlogloss:0.33453\teval-mlogloss:0.29251\n",
      "[237]\ttrain-mlogloss:0.33330\teval-mlogloss:0.29131\n",
      "[238]\ttrain-mlogloss:0.33290\teval-mlogloss:0.29086\n",
      "[239]\ttrain-mlogloss:0.33186\teval-mlogloss:0.28972\n",
      "[240]\ttrain-mlogloss:0.33064\teval-mlogloss:0.28837\n",
      "[241]\ttrain-mlogloss:0.32999\teval-mlogloss:0.28773\n",
      "[242]\ttrain-mlogloss:0.32957\teval-mlogloss:0.28740\n",
      "[243]\ttrain-mlogloss:0.32881\teval-mlogloss:0.28649\n",
      "[244]\ttrain-mlogloss:0.32806\teval-mlogloss:0.28579\n",
      "[245]\ttrain-mlogloss:0.32681\teval-mlogloss:0.28432\n",
      "[246]\ttrain-mlogloss:0.32663\teval-mlogloss:0.28423\n",
      "[247]\ttrain-mlogloss:0.32564\teval-mlogloss:0.28301\n",
      "[248]\ttrain-mlogloss:0.32418\teval-mlogloss:0.28125\n",
      "[249]\ttrain-mlogloss:0.32261\teval-mlogloss:0.27937\n",
      "[250]\ttrain-mlogloss:0.32182\teval-mlogloss:0.27845\n",
      "[251]\ttrain-mlogloss:0.32135\teval-mlogloss:0.27783\n",
      "[252]\ttrain-mlogloss:0.32056\teval-mlogloss:0.27703\n",
      "[253]\ttrain-mlogloss:0.32030\teval-mlogloss:0.27677\n",
      "[254]\ttrain-mlogloss:0.31907\teval-mlogloss:0.27539\n",
      "[255]\ttrain-mlogloss:0.31861\teval-mlogloss:0.27488\n",
      "[256]\ttrain-mlogloss:0.31786\teval-mlogloss:0.27403\n",
      "[257]\ttrain-mlogloss:0.31628\teval-mlogloss:0.27223\n",
      "[258]\ttrain-mlogloss:0.31504\teval-mlogloss:0.27091\n",
      "[259]\ttrain-mlogloss:0.31338\teval-mlogloss:0.26917\n",
      "[260]\ttrain-mlogloss:0.31226\teval-mlogloss:0.26794\n",
      "[261]\ttrain-mlogloss:0.31057\teval-mlogloss:0.26609\n",
      "[262]\ttrain-mlogloss:0.31043\teval-mlogloss:0.26601\n",
      "[263]\ttrain-mlogloss:0.30897\teval-mlogloss:0.26443\n",
      "[264]\ttrain-mlogloss:0.30786\teval-mlogloss:0.26301\n",
      "[265]\ttrain-mlogloss:0.30706\teval-mlogloss:0.26222\n",
      "[266]\ttrain-mlogloss:0.30641\teval-mlogloss:0.26139\n",
      "[267]\ttrain-mlogloss:0.30610\teval-mlogloss:0.26106\n",
      "[268]\ttrain-mlogloss:0.30472\teval-mlogloss:0.25963\n",
      "[269]\ttrain-mlogloss:0.30398\teval-mlogloss:0.25882\n",
      "[270]\ttrain-mlogloss:0.30304\teval-mlogloss:0.25763\n",
      "[271]\ttrain-mlogloss:0.30172\teval-mlogloss:0.25597\n",
      "[272]\ttrain-mlogloss:0.30082\teval-mlogloss:0.25501\n",
      "[273]\ttrain-mlogloss:0.29993\teval-mlogloss:0.25406\n",
      "[274]\ttrain-mlogloss:0.29885\teval-mlogloss:0.25291\n",
      "[275]\ttrain-mlogloss:0.29747\teval-mlogloss:0.25140\n",
      "[276]\ttrain-mlogloss:0.29699\teval-mlogloss:0.25088\n",
      "[277]\ttrain-mlogloss:0.29606\teval-mlogloss:0.24983\n",
      "[278]\ttrain-mlogloss:0.29554\teval-mlogloss:0.24935\n",
      "[279]\ttrain-mlogloss:0.29479\teval-mlogloss:0.24857\n",
      "[280]\ttrain-mlogloss:0.29386\teval-mlogloss:0.24751\n",
      "[281]\ttrain-mlogloss:0.29335\teval-mlogloss:0.24700\n",
      "[282]\ttrain-mlogloss:0.29238\teval-mlogloss:0.24587\n",
      "[283]\ttrain-mlogloss:0.29153\teval-mlogloss:0.24490\n",
      "[284]\ttrain-mlogloss:0.29055\teval-mlogloss:0.24390\n",
      "[285]\ttrain-mlogloss:0.29045\teval-mlogloss:0.24380\n",
      "[286]\ttrain-mlogloss:0.28907\teval-mlogloss:0.24210\n",
      "[287]\ttrain-mlogloss:0.28883\teval-mlogloss:0.24187\n",
      "[288]\ttrain-mlogloss:0.28769\teval-mlogloss:0.24071\n",
      "[289]\ttrain-mlogloss:0.28680\teval-mlogloss:0.23977\n",
      "[290]\ttrain-mlogloss:0.28595\teval-mlogloss:0.23883\n",
      "[291]\ttrain-mlogloss:0.28479\teval-mlogloss:0.23746\n",
      "[292]\ttrain-mlogloss:0.28404\teval-mlogloss:0.23668\n",
      "[293]\ttrain-mlogloss:0.28349\teval-mlogloss:0.23604\n",
      "[294]\ttrain-mlogloss:0.28298\teval-mlogloss:0.23556\n",
      "[295]\ttrain-mlogloss:0.28202\teval-mlogloss:0.23454\n",
      "[296]\ttrain-mlogloss:0.28154\teval-mlogloss:0.23400\n",
      "[297]\ttrain-mlogloss:0.28050\teval-mlogloss:0.23286\n",
      "[298]\ttrain-mlogloss:0.27917\teval-mlogloss:0.23136\n",
      "[299]\ttrain-mlogloss:0.27863\teval-mlogloss:0.23082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:58,511] Trial 42 finished with value: 1.0 and parameters: {'lambda': 6.984537770675732e-05, 'alpha': 0.9937803811103514, 'eta': 0.010344558057273932, 'gamma': 0.021812644314303485, 'max_depth': 5, 'min_child_weight': 4, 'subsample': 0.48202028505740613, 'colsample_bytree': 0.40113079190832585}. Best is trial 0 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.04937\teval-mlogloss:1.04689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:58,614] Trial 43 pruned. Trial was pruned at iteration 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.05219\teval-mlogloss:1.05067\n",
      "[1]\ttrain-mlogloss:1.02941\teval-mlogloss:1.02499\n",
      "[2]\ttrain-mlogloss:0.99059\teval-mlogloss:0.98418\n",
      "[3]\ttrain-mlogloss:0.94931\teval-mlogloss:0.93889\n",
      "[4]\ttrain-mlogloss:0.91391\teval-mlogloss:0.90145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:58,730] Trial 44 pruned. Trial was pruned at iteration 4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.91317\teval-mlogloss:0.90945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:58,826] Trial 45 pruned. Trial was pruned at iteration 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.01928\teval-mlogloss:1.01946\n",
      "[1]\ttrain-mlogloss:0.95615\teval-mlogloss:0.94944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:58,982] Trial 46 pruned. Trial was pruned at iteration 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.82668\teval-mlogloss:0.83015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:59,080] Trial 47 pruned. Trial was pruned at iteration 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.06862\teval-mlogloss:1.06771\n",
      "[1]\ttrain-mlogloss:1.04039\teval-mlogloss:1.03764\n",
      "[2]\ttrain-mlogloss:1.01185\teval-mlogloss:1.00783\n",
      "[3]\ttrain-mlogloss:0.98483\teval-mlogloss:0.98026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:59,202] Trial 48 pruned. Trial was pruned at iteration 4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.94305\teval-mlogloss:0.93614\n",
      "[1]\ttrain-mlogloss:0.86634\teval-mlogloss:0.85893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 22:07:59,308] Trial 49 pruned. Trial was pruned at iteration 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial: {'lambda': 0.00014405154340233053, 'alpha': 1.0028087182541891e-05, 'eta': 0.24399632761008344, 'gamma': 3.146022815330369e-05, 'max_depth': 6, 'min_child_weight': 5, 'subsample': 0.5976578373605221, 'colsample_bytree': 0.6061634604717767}\n",
      "Best accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Load the Iris dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the objective function for XGBoost\n",
    "def objective(trial):\n",
    "    # Hyperparameter search space\n",
    "    param = {\n",
    "        'verbosity': 0,\n",
    "        'objective': 'multi:softprob',\n",
    "        'num_class': 3,\n",
    "        'eval_metric': 'mlogloss',  # Ensure that the eval_metric is specified here\n",
    "        'booster': 'gbtree',\n",
    "        'lambda': trial.suggest_float('lambda', 1e-8, 1.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-8, 1.0, log=True),\n",
    "        'eta': trial.suggest_float('eta', 0.01, 0.3),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 9),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.4, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.4, 1.0),\n",
    "        'n_estimators': 300,\n",
    "    }\n",
    "\n",
    "    # Create DMatrix for XGBoost\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "    # Define a pruning callback based on evaluation metrics\n",
    "    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"eval-mlogloss\")  # Match the metric name in the evals list\n",
    "\n",
    "    # Train the model\n",
    "    bst = xgb.train(\n",
    "        param,\n",
    "        dtrain,\n",
    "        num_boost_round=300,\n",
    "        evals=[(dtrain, \"train\"), (dtest, \"eval\")],  # Ensure the eval datasets and names are specified\n",
    "        early_stopping_rounds=30,\n",
    "        callbacks=[pruning_callback]\n",
    "    )\n",
    "\n",
    "    # Predict on the test set\n",
    "    preds = bst.predict(dtest)\n",
    "    best_preds = [int(np.argmax(line)) for line in preds]\n",
    "\n",
    "    # Return accuracy as the objective value\n",
    "    accuracy = accuracy_score(y_test, best_preds)\n",
    "    return accuracy\n",
    "\n",
    "# Create a study with pruning\n",
    "study = optuna.create_study(direction='maximize', pruner=optuna.pruners.SuccessiveHalvingPruner())\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Output the best trial\n",
    "print(f\"Best trial: {study.best_trial.params}\")\n",
    "print(f\"Best accuracy: {study.best_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "876be74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install optuna-integration[xgboost]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a0d4ba9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Optuna\\env\\Lib\\site-packages\\optuna\\visualization\\_plotly_imports.py:7\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m try_import() \u001b[38;5;28;01mas\u001b[39;00m _imports:\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m plotly_version\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'plotly'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptuna\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvisualization\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m plot_intermediate_values\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# 1. Plot intermediate values during the trials\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mplot_intermediate_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m)\u001b[49m.show()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Optuna\\env\\Lib\\site-packages\\optuna\\visualization\\_intermediate_values.py:68\u001b[39m, in \u001b[36mplot_intermediate_values\u001b[39m\u001b[34m(study)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplot_intermediate_values\u001b[39m(study: Study) -> \u001b[33m\"\u001b[39m\u001b[33mgo.Figure\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     57\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Plot intermediate values of all trials in a study.\u001b[39;00m\n\u001b[32m     58\u001b[39m \n\u001b[32m     59\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     65\u001b[39m \u001b[33;03m        A :class:`plotly.graph_objects.Figure` object.\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     \u001b[43m_imports\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _get_intermediate_plot(_get_intermediate_plot_info(study))\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Optuna\\env\\Lib\\site-packages\\optuna\\_imports.py:94\u001b[39m, in \u001b[36m_DeferredImportExceptionContextManager.check\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._deferred \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     93\u001b[39m     exc_value, message = \u001b[38;5;28mself\u001b[39m._deferred\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc_value\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'."
     ]
    }
   ],
   "source": [
    "from optuna.visualization import plot_intermediate_values\n",
    "\n",
    "# 1. Plot intermediate values during the trials\n",
    "plot_intermediate_values(study).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dbdf69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2e0ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a6156a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
